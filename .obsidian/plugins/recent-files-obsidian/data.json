{
  "recentFiles": [
    {
      "basename": "202602061227 - ppo-ptx",
      "path": "002 - research/202602061227 - ppo-ptx.md"
    },
    {
      "basename": "202602200001 - policy optimization",
      "path": "002 - research/202602200001 - policy optimization.md"
    },
    {
      "basename": "202602200013 - transition",
      "path": "002 - research/202602200013 - transition.md"
    },
    {
      "basename": "202602200129 - policy gradient methods",
      "path": "002 - research/202602200129 - policy gradient methods.md"
    },
    {
      "basename": "202602200134 - reinforce",
      "path": "002 - research/202602200134 - reinforce.md"
    },
    {
      "basename": "202602201253 - model-based",
      "path": "002 - research/202602201253 - model-based.md"
    },
    {
      "basename": "202602201254 - actor-critic methods",
      "path": "002 - research/202602201254 - actor-critic methods.md"
    },
    {
      "basename": "202602201253 - model-free",
      "path": "002 - research/202602201253 - model-free.md"
    },
    {
      "basename": "202602192359 - value estimation",
      "path": "002 - research/202602192359 - value estimation.md"
    },
    {
      "basename": "reinforcement learning",
      "path": "002 - research/reinforcement learning.md"
    },
    {
      "basename": "neural networks",
      "path": "002 - research/neural networks.md"
    },
    {
      "basename": "202602201850 - bias-variance tradeoff",
      "path": "002 - research/202602201850 - bias-variance tradeoff.md"
    },
    {
      "basename": "202602111639 - underfitting",
      "path": "002 - research/202602111639 - underfitting.md"
    },
    {
      "basename": "202602111358 - weights and biases",
      "path": "002 - research/202602111358 - weights and biases.md"
    },
    {
      "basename": "202602201334 - reward model",
      "path": "002 - research/202602201334 - reward model.md"
    },
    {
      "basename": "202602111636 - regularization",
      "path": "002 - research/202602111636 - regularization.md"
    },
    {
      "basename": "202602201335 - bootstrapping",
      "path": "002 - research/202602201335 - bootstrapping.md"
    },
    {
      "basename": "202602201241 - proximal policy optimization",
      "path": "002 - research/202602201241 - proximal policy optimization.md"
    },
    {
      "basename": "202602201255 - value-based methods",
      "path": "002 - research/202602201255 - value-based methods.md"
    },
    {
      "basename": "202602200020 - value",
      "path": "002 - research/202602200020 - value.md"
    },
    {
      "basename": "202602200148 - advantage",
      "path": "002 - research/202602200148 - advantage.md"
    },
    {
      "basename": "202602201248 - on-policy",
      "path": "002 - research/202602201248 - on-policy.md"
    },
    {
      "basename": "202602201303 - q-learning",
      "path": "002 - research/202602201303 - q-learning.md"
    },
    {
      "basename": "202602201246 - off-policy",
      "path": "002 - research/202602201246 - off-policy.md"
    },
    {
      "basename": "202602201305 - temporal-difference algorithm",
      "path": "002 - research/202602201305 - temporal-difference algorithm.md"
    },
    {
      "basename": "202602010053 - on-line learning",
      "path": "002 - research/202602010053 - on-line learning.md"
    },
    {
      "basename": "202602192245 - policy",
      "path": "002 - research/202602192245 - policy.md"
    },
    {
      "basename": "202602192236 - markov decision process",
      "path": "002 - research/202602192236 - markov decision process.md"
    },
    {
      "basename": "202602192221 - reinforcement learning",
      "path": "002 - research/202602192221 - reinforcement learning.md"
    },
    {
      "basename": "202602192343 - agent",
      "path": "002 - research/202602192343 - agent.md"
    },
    {
      "basename": "202602192345 - environment",
      "path": "002 - research/202602192345 - environment.md"
    },
    {
      "basename": "202602192356 - episode",
      "path": "002 - research/202602192356 - episode.md"
    },
    {
      "basename": "202602192355 - return",
      "path": "002 - research/202602192355 - return.md"
    },
    {
      "basename": "202602192352 - reward",
      "path": "002 - research/202602192352 - reward.md"
    },
    {
      "basename": "202602192350 - action",
      "path": "002 - research/202602192350 - action.md"
    },
    {
      "basename": "202602192351 - state",
      "path": "002 - research/202602192351 - state.md"
    },
    {
      "basename": "202602111400 - activation",
      "path": "002 - research/202602111400 - activation.md"
    },
    {
      "basename": "202602192217 - reinforcement learning human feedback",
      "path": "002 - research/202602192217 - reinforcement learning human feedback.md"
    },
    {
      "basename": "202602191644 - post-training",
      "path": "002 - research/202602191644 - post-training.md"
    },
    {
      "basename": "202602191524 - large language model",
      "path": "002 - research/202602191524 - large language model.md"
    },
    {
      "basename": "202601282201 - supervised fine-tuning",
      "path": "002 - research/202601282201 - supervised fine-tuning.md"
    },
    {
      "basename": "202602030238 - cross-entropy loss",
      "path": "002 - research/202602030238 - cross-entropy loss.md"
    },
    {
      "basename": "build_a_large_language_model_from_scratch",
      "path": "007 - attachments/build_a_large_language_model_from_scratch.pdf"
    },
    {
      "basename": "202602181827 - kl divergence",
      "path": "002 - research/202602181827 - kl divergence.md"
    },
    {
      "basename": "202602120102 - cross-entropy",
      "path": "002 - research/202602120102 - cross-entropy.md"
    },
    {
      "basename": "202601282131 - training language models to follow instructions with human feedback",
      "path": "002 - research/202601282131 - training language models to follow instructions with human feedback.md"
    },
    {
      "basename": "deep learning",
      "path": "002 - research/deep learning.md"
    },
    {
      "basename": "202602191533 - attention is all you need",
      "path": "002 - research/202602191533 - attention is all you need.md"
    },
    {
      "basename": "natural language processing",
      "path": "002 - research/natural language processing.md"
    },
    {
      "basename": "definitions",
      "path": "002 - research/definitions.md"
    }
  ],
  "omittedPaths": [],
  "omittedTags": [],
  "updateOn": "file-open",
  "omitBookmarks": false
}