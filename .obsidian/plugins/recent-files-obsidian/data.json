{
  "recentFiles": [
    {
      "basename": "202602181827 - kl divergence",
      "path": "002 - research/202602181827 - kl divergence.md"
    },
    {
      "basename": "202602120102 - cross-entropy",
      "path": "002 - research/202602120102 - cross-entropy.md"
    },
    {
      "basename": "202602221914 - deepseek-r1 - incentivizing reasoning capability in llms via reinforcement learning",
      "path": "002 - research/202602221914 - deepseek-r1 - incentivizing reasoning capability in llms via reinforcement learning.md"
    },
    {
      "basename": "deepseek",
      "path": "002 - research/deepseek.md"
    },
    {
      "basename": "202602191533 - attention is all you need",
      "path": "002 - research/202602191533 - attention is all you need.md"
    },
    {
      "basename": "202602211411 - generalized advantage estimation",
      "path": "002 - research/202602211411 - generalized advantage estimation.md"
    },
    {
      "basename": "202602201241 - proximal policy optimization",
      "path": "002 - research/202602201241 - proximal policy optimization.md"
    },
    {
      "basename": "202602200134 - reinforce",
      "path": "002 - research/202602200134 - reinforce.md"
    },
    {
      "basename": "202602232025 - reinforcement learning verifiable rewards",
      "path": "002 - research/202602232025 - reinforcement learning verifiable rewards.md"
    },
    {
      "basename": "202602232031 - rule-based rewards",
      "path": "002 - research/202602232031 - rule-based rewards.md"
    },
    {
      "basename": "202602232032 - model-based rewards",
      "path": "002 - research/202602232032 - model-based rewards.md"
    },
    {
      "basename": "202602232038 - adversarial evaluation",
      "path": "002 - research/202602232038 - adversarial evaluation.md"
    },
    {
      "basename": "reinforcement learning",
      "path": "002 - research/reinforcement learning.md"
    },
    {
      "basename": "202602201334 - reward model",
      "path": "002 - research/202602201334 - reward model.md"
    },
    {
      "basename": "202602020022 - non-verifiable domains",
      "path": "002 - research/202602020022 - non-verifiable domains.md"
    },
    {
      "basename": "202602020018 - verifiable domains",
      "path": "002 - research/202602020018 - verifiable domains.md"
    },
    {
      "basename": "202602232008 - cold-start sft",
      "path": "002 - research/202602232008 - cold-start sft.md"
    },
    {
      "basename": "large language models",
      "path": "002 - research/large language models.md"
    },
    {
      "basename": "202602231948 - distillation",
      "path": "002 - research/202602231948 - distillation.md"
    },
    {
      "basename": "202602231804 - rejection sampling",
      "path": "002 - research/202602231804 - rejection sampling.md"
    },
    {
      "basename": "202601282201 - supervised fine-tuning",
      "path": "002 - research/202601282201 - supervised fine-tuning.md"
    },
    {
      "basename": "202602191524 - large language model",
      "path": "002 - research/202602191524 - large language model.md"
    },
    {
      "basename": "neural networks",
      "path": "002 - research/neural networks.md"
    },
    {
      "basename": "202602201939 - group relative policy optimization",
      "path": "002 - research/202602201939 - group relative policy optimization.md"
    },
    {
      "basename": "activation analysis",
      "path": "002 - research/activation analysis.md"
    },
    {
      "basename": "202601282131 - training language models to follow instructions with human feedback",
      "path": "002 - research/202601282131 - training language models to follow instructions with human feedback.md"
    },
    {
      "basename": "node - zettel",
      "path": "008 - templates/node - zettel.md"
    },
    {
      "basename": "node - inbox",
      "path": "008 - templates/node - inbox.md"
    },
    {
      "basename": "node - daily notes",
      "path": "008 - templates/node - daily notes.md"
    },
    {
      "basename": "202602062040 - probability",
      "path": "002 - research/202602062040 - probability.md"
    },
    {
      "basename": "202602231757 - reasoning",
      "path": "002 - research/202602231757 - reasoning.md"
    },
    {
      "basename": "openai",
      "path": "002 - research/openai.md"
    },
    {
      "basename": "202602221541 - deep reinforcement learning doesn't work yet (2018)",
      "path": "002 - research/202602221541 - deep reinforcement learning doesn't work yet (2018).md"
    },
    {
      "basename": "202602192221 - reinforcement learning",
      "path": "002 - research/202602192221 - reinforcement learning.md"
    },
    {
      "basename": "202602201303 - q-learning",
      "path": "002 - research/202602201303 - q-learning.md"
    },
    {
      "basename": "202602111640 - dropout",
      "path": "002 - research/202602111640 - dropout.md"
    },
    {
      "basename": "202602220131 - residual dropouts",
      "path": "002 - research/202602220131 - residual dropouts.md"
    },
    {
      "basename": "post-training",
      "path": "002 - research/post-training.md"
    },
    {
      "basename": "202602061227 - ppo-ptx",
      "path": "002 - research/202602061227 - ppo-ptx.md"
    },
    {
      "basename": "202602192217 - reinforcement learning human feedback",
      "path": "002 - research/202602192217 - reinforcement learning human feedback.md"
    },
    {
      "basename": "202602111600 - learning rate",
      "path": "002 - research/202602111600 - learning rate.md"
    },
    {
      "basename": "202602111617 - learning rate schedule",
      "path": "002 - research/202602111617 - learning rate schedule.md"
    },
    {
      "basename": "202602220121 - cosine decay",
      "path": "002 - research/202602220121 - cosine decay.md"
    },
    {
      "basename": "202602011426 - warmup",
      "path": "002 - research/202602011426 - warmup.md"
    },
    {
      "basename": "202602111256 - hyperparameters",
      "path": "002 - research/202602111256 - hyperparameters.md"
    },
    {
      "basename": "202602111634 - weight decay",
      "path": "002 - research/202602111634 - weight decay.md"
    },
    {
      "basename": "202602200001 - policy optimization",
      "path": "002 - research/202602200001 - policy optimization.md"
    },
    {
      "basename": "proximal policy optimisation",
      "path": "002 - research/proximal policy optimisation.md"
    },
    {
      "basename": "202602200129 - policy gradient methods",
      "path": "002 - research/202602200129 - policy gradient methods.md"
    },
    {
      "basename": "202602201335 - bootstrapping",
      "path": "002 - research/202602201335 - bootstrapping.md"
    }
  ],
  "omittedPaths": [],
  "omittedTags": [],
  "updateOn": "file-open",
  "omitBookmarks": false
}