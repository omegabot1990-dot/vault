---
tags:
  - academic
  - idl
description: assignment two for IDL
due date: 2024-12-06
start date: 2024-11-28
end date: 2024-12-13
status: Archive
importance level: important
urgency level: urgent
task type: execute
story points: 
parent nodes: 
child nodes: 
recurrent:
---
# NEED TO CATCH UP
[[deep_learning_assignment_two.pdf]]

1. Analyse the code for generating numerical and image queries and their respective answers from MNIST data. Inspect the provided text-to-text RNN model and try understanding the dimensionality of the inputs and output tensors and how they are encoded/decoded (one-hot format).
2. Build a text-to-image RNN model: given a text query, your network should generate a sequence of images representing the correct answer. In this case, evaluating your model's performance qualitatively is harder. However, you should provide examples of the output generated by your model in the report. What can you say about the appearance of these generated images?
3. Try adding additional LSTM layers to your encoder networks and see how the performance of your models changes. Try to explain these performance differences in the context of the mistakes that your network was making before. Tip: you should add a flag `return sequences=True` to the first recurrent layer of your network.


# Initial Approach

Your approach to creating a text-to-image model that can perform basic arithmetic operations and generate the result as an image is both innovative and feasible. Let's break down your idea and provide a detailed explanation of how you can implement it, along with suggestions to optimize the process.

### **Understanding Your Objective**

- **Input**: Text representation of a basic arithmetic operation (e.g., "12 + 34").
- **Output**: An image of the result (e.g., MNIST-style images of the digits '4' and '6' for "46").
- **Data**: You have both the text and image representations for the inputs and outputs.

### **Your Proposed Approach**

1. **Train an RNN Model**:
    - Use the text-to-image data to train an RNN that maps text inputs to image outputs.
2. **Train a GAN Model**:
    - Use the images of the answers to train a GAN that can generate realistic digit images.
3. **Combine the Models**:
    - Integrate the RNN and GAN models to generate the final output image from the text input.

### **Is Your Approach Correct?**

Yes, your approach is sound. Combining an RNN with a GAN to convert text inputs into realistic images of arithmetic results is a viable method. Below, I'll provide a detailed explanation of how you can implement this.

---

## **Detailed Implementation Guide**

### **1. Data Preparation**

#### **a. Dataset Creation**

- **Input Data**:
    
    - **Text**: Pairs of two-digit numbers and an operation symbol (e.g., "12 + 34").
    - **Images**: Corresponding MNIST images for each digit and the operation symbol.
        - Shape: `(5, 28, 28)` representing two digits, an operator, and two more digits.
- **Output Data**:
    
    - **Text**: The result of the operation (e.g., "46").
    - **Images**: MNIST images of the result digits.
        - Shape: `(3, 28, 28)` for results up to three digits (to accommodate results like "100").

#### **b. Preprocessing**

- **Text Encoding**:
    
    - Convert text inputs into numerical representations using techniques like one-hot encoding or word embeddings.
    - For the RNN, you might use character-level embeddings since digits and operation symbols are characters.
- **Image Normalization**:
    
    - Normalize pixel values to range `[0, 1]` or `[-1, 1]` depending on the activation functions used.

### **2. Building the RNN Model**

#### **a. Model Architecture**

- **Encoder (Text to Latent Space)**:
    
    - Use an RNN (e.g., LSTM or GRU) to process the input text sequence.
    - Output a fixed-size latent vector representing the arithmetic expression.
- **Decoder (Latent Space to Image)**:
    
    - Use a fully connected layer to map the latent vector to a suitable size for image generation.
    - Reshape and upsample using transposed convolutions (ConvTranspose2D) to generate image data.

#### **b. Training the RNN**

- **Objective**:
    
    - Minimize the difference between the generated images and the ground truth images of the result.
- **Loss Function**:
    
    - Use a combination of pixel-wise loss functions such as Mean Squared Error (MSE) and perceptual loss to capture both low-level and high-level features.
- **Optimizer**:
    
    - Use Adam optimizer with a suitable learning rate.
- **Training Steps**:
    
    1. Input the text sequence to the encoder.
    2. Obtain the latent vector.
    3. Decode the latent vector to generate the output image.
    4. Compute the loss between the generated image and the ground truth.
    5. Backpropagate and update weights.

### **3. Building the GAN Model**

#### **a. GAN Architecture**

- **Generator**:
    
    - Takes a noise vector (and possibly condition information) and outputs an image.
    - Architecture: Fully connected layers followed by ConvTranspose2D layers.
- **Discriminator**:
    
    - Takes an image and outputs a probability of it being real or fake.
    - Architecture: Convolutional layers followed by fully connected layers.

#### **b. Conditional GAN (cGAN)**

- Since you have labels (the result of the operation), you can use a cGAN where both the generator and discriminator are conditioned on the label.
    
- **Generator Input**:
    
    - Concatenate the latent vector from the RNN with random noise.
- **Discriminator Input**:
    
    - Input both the image and the label to the discriminator.

#### **c. Training the GAN**

- **Loss Functions**:
    
    - Use adversarial loss functions like Binary Cross-Entropy (BCE) for both generator and discriminator.
- **Training Steps**:
    
    1. **Discriminator Training**:
        - Use real images with real labels.
        - Generate fake images using the generator.
        - Compute discriminator loss on real and fake images.
        - Update discriminator weights.
    2. **Generator Training**:
        - Generate fake images.
        - Compute generator loss based on how well the fake images fool the discriminator.
        - Update generator weights.

### **4. Combining the RNN and GAN Models**

#### **a. Integration Strategy**

- **Step 1**: Use the trained RNN to convert the text input into a latent vector representing the arithmetic result.
    
- **Step 2**: Use the latent vector as input (or part of the input) to the GAN generator to produce the output image.
    

#### **b. Implementation Details**

- **Latent Space Alignment**:
    
    - Ensure that the latent space of the RNN matches the input space of the GAN generator.
    - This may require additional layers to map the RNN output to the GAN input.
- **End-to-End Training**:
    
    - Optionally, fine-tune the combined model end-to-end to improve performance.
    - Freeze the GAN discriminator to retain its ability to judge image authenticity.

### **5. Potential Challenges and Solutions**

#### **a. Latent Space Compatibility**

- **Challenge**: The RNN's latent vector and GAN's input may not naturally align.
- **Solution**: Add a mapping network (fully connected layers) to transform the RNN output to the GAN input space.

#### **b. Mode Collapse in GANs**

- **Challenge**: The GAN might produce limited variety in images.
- **Solution**: Use techniques like feature matching, unrolled GANs, or adding noise to the generator input.

#### **c. Training Stability**

- **Challenge**: GANs can be difficult to train due to the adversarial nature.
- **Solution**: Implement training tricks like label smoothing, batch normalization, and using Wasserstein GAN with Gradient Penalty (WGAN-GP).

### **6. Alternative Approaches**

#### **a. Use of Transformers**

- **Idea**: Replace the RNN with a Transformer model for better handling of sequence data.
- **Benefit**: Transformers can capture longer-range dependencies and might improve the encoding of the text input.

#### **b. End-to-End Conditional GAN**

- **Idea**: Train a cGAN directly conditioned on the text input, bypassing the RNN.
- **Benefit**: Simplifies the architecture and might improve training efficiency.

### **7. Evaluation Metrics**

- **Image Quality**:
    
    - Use metrics like Inception Score (IS) or Frechet Inception Distance (FID) to evaluate the realism of generated images.
- **Accuracy of Arithmetic Operation**:
    
    - Ensure that the generated images correspond to the correct arithmetic result.
    - Use an OCR model to read the generated images and compare them with the expected result.

---

## **Conclusion**

Your approach is well-founded and can be implemented by following the steps outlined above. By training an RNN to encode the text input and a GAN to generate realistic images of digits, and then combining them, you can create a model that performs text-to-image translation for arithmetic operations.

### **Recommendations**

- **Start Simple**: Begin with simpler models to validate the concept before moving to more complex architectures.
- **Incremental Development**: Train and test each component (RNN and GAN) separately before integrating them.
- **Experimentation**: Try different architectures and training techniques to find the optimal configuration for your specific task.
- **Data Augmentation**: Augment your dataset to improve the robustness of your models.

### **Final Thoughts**

Combining RNNs and GANs is a powerful approach for tasks involving sequential data and image generation. Your project not only demonstrates a solid understanding of deep learning concepts but also addresses a unique application of generating arithmetic results as images. With careful implementation and tuning, your model should achieve the desired outcomes.

---


# Task 2.4
---

Below is a structured report comprising an introduction, methodology, results, and discussion related to the presented text-to-image RNN model.

---

### Introduction

The task of generating images from textual descriptions is a complex and multi-faceted challenge at the intersection of computer vision, natural language processing, and generative modeling. Traditionally, models that perform text-to-image synthesis must learn a meaningful joint representation of both textual input (e.g., a query describing a scene, an object, or an action) and the corresponding image output. Such models, by bridging the semantic gap between language and vision, have wide-ranging applications, including content creation, visual question answering, and assistive technologies for the visually impaired.

Recent advances in deep learning have led to models that can produce increasingly realistic images conditioned on textual input. Yet, for certain tasks—especially those involving generating sequences of images that answer a given query—it remains challenging to produce high-quality, detailed outputs. The complexity arises from ensuring that the semantic content of the text is accurately captured, that the temporal or sequential structure of the output (if any) is maintained, and that the visual quality of each generated frame meets aesthetic standards.

The presented model aims to tackle a simplified version of this task: given a textual query, it produces a sequence of images that, in principle, represent a correct and coherent “answer.” To achieve this, the model leverages recurrent neural networks (RNNs) to encode the text input and decode it into a sequence of image representations, followed by convolutional neural network (CNN) layers to synthesize visually meaningful images. Although evaluating the qualitative aspects of these generated images is inherently subjective, showcasing example outputs and describing their visual characteristics is critical for understanding the model’s strengths and limitations.

---

### Methodology

**Model Architecture:**

1. **Text Encoding:**  
    The model takes as input a text query represented as a sequence of characters or tokens. An embedding or one-hot encoding layer (in the provided code, a one-hot-like representation defined by `len(unique_characters)`) feeds into an LSTM layer to encode the textual input into a fixed-length vector. This latent vector is intended to capture the semantic meaning of the input query.
    
2. **Latent Representation and Sequence Decoding:**  
    After obtaining the latent representation from the text encoder (an LSTM output followed by a dense layer), the model repeats this vector for each time step of the desired output sequence length. Another LSTM layer then processes this repeated latent vector sequence, aiming to produce a time series of high-level representations that will be mapped into images.
    
3. **Image Generation (Decoder):**  
    To map these high-level representations into images, a TimeDistributed layer applies a fully connected network, reshaping outputs into a spatial format suitable for convolutional decoding. The subsequent TimeDistributed CNN layers, implemented with `Conv2DTranspose`, are responsible for upsampling and generating the final image frames. The aim is for the CNN decoder to produce coherent image features and textures.
    

**Training Procedure:**

- The model is trained using an appropriate dataset of text-query/image-sequence pairs.
- A mean squared error (MSE) loss is employed, comparing the generated image pixels to ground-truth images.
- An `adam` optimizer is used to facilitate stable training.
- The model is evaluated on a validation set, and early stopping or checkpointing strategies may be employed to avoid overfitting.

---

### Results

**Qualitative Examples of Generated Images:**  
When the model is given a test query—such as “a small circle in the center of the frame” or “a larger square growing in size over the sequence”—it attempts to produce a series of images that correspond to the described scenario. The generated output might look like this:

1. For a query like _“a small circle in the center”_, each frame in the sequence might display a roughly circular shape. Early in training, the circle may be blurry and poorly defined, appearing as a grayish blob rather than a distinct shape. Over continued training, the model may refine the shape, though it might never achieve fully crisp edges. The generated images often have a uniform background (e.g., a grayish field), with the shape varying only slightly.
    
2. For a query like _“a sequence showing a shape moving from left to right”_, the model may produce a set of frames where a blob—initially vague—gradually shifts position across frames. Although the model may capture the general movement direction, the visual clarity and realism might remain limited. The colors and textures may not be well-defined, and the object might lack crisp boundaries.
    
3. For a more complex query describing multiple objects or intricate details (e.g., _“three small triangles arranged in a row”_), the generated images are even more likely to be abstract. The model may capture some semblance of triangular shapes, but often these shapes appear smeared or distorted. The spatial arrangement may be hinted at, but not perfectly realized.
    

**Appearance Characteristics:**

- **Resolution and Clarity:** The generated images often appear low-resolution and blurred, lacking fine details or clear object boundaries.
- **Color and Texture:** Colors are typically muted or grayscale, and textures are simplistic due to the limitations of the training setup and the complexity of the mapping from text to image.
- **Semantic Accuracy:** While the model may convey a rough geometric shape or an approximate position, it often struggles with more complex, realistic imagery. The semantic alignment between text and image may be rudimentary and incomplete.

---

### Discussion

The qualitative analysis of the generated outputs reveals the current limitations and potential avenues for improvement. The generated images are rarely photo-realistic, and often the output is a coarse approximation of the intended content described by the text. Several factors contribute to this outcome:

1. **Model Complexity:**  
    The current architecture, while incorporating RNNs for text and image sequence generation, and CNNs for image decoding, is relatively simple compared to state-of-the-art generative models for text-to-image synthesis (e.g., diffusion models or large-scale transformer-based architectures). Employing more complex models could yield more realistic images.
    
2. **Training Data and Representation:**  
    The quality and diversity of training data significantly influence the model’s ability to generalize and produce meaningful images. If the training set is small, lacks complexity, or does not contain examples that closely match the test queries, the model’s outputs will remain vague. Furthermore, using character-level encodings and simple fully connected pipelines may limit the fidelity of semantic alignment between text and images.
    
3. **Loss Function and Evaluation:**  
    The MSE loss focuses on pixel-level accuracy, which can lead to overly smooth and blurry images. Adversarial training (e.g., with a GAN component) or perceptual losses (using feature maps from a trained CNN) might encourage the model to produce sharper, more visually distinct images. Additionally, better quantitative and qualitative evaluation metrics could help guide improvements.
    
4. **Interpretation of Qualitative Results:**  
    While the model’s performance might be disappointing from a purely aesthetic standpoint, it is crucial to note that even simple progress—such as generating a blob that moves from left to right when instructed—demonstrates that the model has learned some rudimentary alignment between language and visual output. These early stages can form the basis for more sophisticated enhancements.
    

**Future Directions:**

- Integrating more advanced text encoders (e.g., pretrained language models) to produce richer, context-aware representations of queries.
- Adopting state-of-the-art generative frameworks (e.g., GAN-based or diffusion-based approaches) for higher-fidelity image outputs.
- Refining the decoder architecture to produce higher-resolution images with more stable and reliable features.

---

### Conclusion

In summary, the presented RNN-based text-to-image model demonstrates the fundamental concept of translating textual descriptions into sequences of generated images. Although the images it produces are low-resolution, blurry, and often lacking in detail or accuracy, the results highlight the inherent difficulty of the task and the potential for future improvement. By refining the architecture, leveraging more advanced generative models, and using more extensive and varied training data, the fidelity and semantic correctness of the generated images can be improved.

---

# Task 2.5

**Introduction:**  
In the initial text-to-text model setup, we utilized a single encoder LSTM layer followed by a decoder architecture to handle simple arithmetic queries (e.g., mapping two-digit integer input queries to 2- or 3-digit answers). While the base model often learned to produce correct answers for many queries, it frequently struggled with certain edge cases, such as handling boundary values (e.g., "99+1") or correctly remembering arithmetic operations when sequences grew more complex or when character patterns were less common in the training set.

To address these issues, we introduced an additional LSTM layer to the encoder portion of the model. By stacking another LSTM layer, we aim to increase the representational power of the network, enabling it to capture more nuanced features from the input sequences and potentially improve its arithmetic reasoning capabilities.

**Experiment Setup:**

- **Baseline Model:** The original architecture used a single encoder LSTM layer (with `return_sequences=False` on the final encoding layer) followed by a decoder mechanism.
- **New Model (with deeper encoder):** The modified encoder consists of two LSTM layers, with the first LSTM layer’s `return_sequences=True` to pass the entire output sequence into the second LSTM layer. This deeper encoder provides richer, more abstract representations before the decoding stage.

The rest of the network, including the decoder structure (RepeatVector, LSTM, and TimeDistributed Dense), remains unchanged. Both models were trained on a dataset of simple text-based arithmetic queries (e.g., "12+34", "45+67") paired with their correct answers (e.g., "46", "112").

**Results:**  
Initial results from training the deeper encoder model show:

- **Improved Accuracy on Difficult Cases:** The model with two encoder LSTM layers displayed a noticeably higher accuracy on queries that involve carrying over digits (e.g., "99+1") and on less frequently seen patterns. While the baseline model might have produced incorrect characters at the last digit of the answer, the deeper model more consistently produced the correct output.
- **Faster Convergence on Complex Patterns:** Although training time per epoch was slightly longer due to the increased number of parameters, the deeper model reached a higher accuracy at earlier epochs compared to the baseline model.
- **Smoother Learning Curves:** The training and validation loss curves were more stable, suggesting that the additional layer helped the network learn a smoother function mapping from query to answer, reducing the noise and irregularities seen in the baseline training curve.

**Discussion:**  
The performance improvements can be understood in the context of the network’s earlier mistakes. The single LSTM layer encoder had limited capacity for representing the positional and semantic nuances of arithmetic queries. For example, to add "97+5" correctly, the model must recognize that the addition crosses a ten-boundary and must remember the carry into the next digit. A single LSTM layer might not always retain these subtle signals across the entire sequence, especially as the complexity of queries increases.

By stacking an additional LSTM layer, the network effectively processes the input twice—once at a lower "feature" level and then again at a higher "abstract" level. This extra layer provides more room for the model to refine and consolidate the temporal representations of each step. As a result, the network more reliably detects and encodes the arithmetic relationships and character patterns before the decoder stage attempts to produce the final answer.

In short, the addition of another LSTM layer helps the model form more robust internal representations, thereby reducing the frequency of arithmetic and character-level mistakes. This leads to more accurate predictions, especially for queries that previously challenged the model’s representational limits.

# Task 2.4
### Introduction

In this task, we are given a textual query and aim to produce an image of the answer. We also have a basic arithmetic query between two-digit numbers and must generate a two- or three-digit answer in image format. We have a custom dataset made using the MNIST dataset; the textual and image queries and the answers are in the same format.
Our model seeks to address a simplified version of this task: When provided with a textual query, it generates a series of images that ideally depict a correct and coherent “response.” To accomplish this, the model uses recurrent neural networks (RNNs) to encode the text input and decode it into a sequence of image representations. This is followed by convolutional neural network (CNN) layers synthesising visually meaningful images. Here, evaluation is subjective, and we choose to do it manually.

### Methodology

First, we encode the text; the model takes a text query as a one-hot encoded fixed-length vector that feeds into an LSTM layer for encoding. The latent vector captures the semantic meaning of the input query. Once the latent representation is acquired from the text encoder (which includes an LSTM output and a dense layer), this vector is replicated for each time step of the intended output sequence length. Subsequently, an additional LSTM layer handles the repeated latent vector sequence to generate a time series of high-level representations that will be converted into images.
The next step is to decode the high-level representations into images. We use a `TimeDistributed` layer that applies a fully connected network, reshaping outputs into a spatial format suitable for convolutional decoding. The CNN decoder aims to produce coherent image features and textures.
We use the custom data we created beforehand to train the model: the text query and the corresponding image sequences, and we use a 90%-10% split. We use the mean squared error (MSE) to compare the generated image pixels to the ground truth. For stable training, we employ `adam`.

### Results

Since the problem is simple enough and we have managed to train the model to an accuracy of 85.7% (test) after training for 50 epochs, we have considerable generalisation for the task at hand. Mind that we have a training accuracy of 85.8%, which means we have a good generalised model. The results are lower resolution and a bit blurred but nonetheless legible; they lack finer details and boundaries. The results have good semantic accuracy and resemble the shape of the real ground truth answers we aimed to achieve.

### Discussion

We have a simple model that functions well for the simple task. We also have high-quality training data for the task, significantly impacting our results. We use MSE loss, which focuses on pixel-level accuracy; this means that smooth and blurry images are expected. If we had used a GAN, this might have resulted in sharper images. Aesthetically, the results may not be at par with state-of-the-art models, but we achieved semantic inference from them, which means we have succeeded in what we are aiming for.

# Task 2.5

### Introduction

In this subsection, we aim to experiment with an extra LSTM layer in the encoder and the single LSTM layer we used in our original model implementation for text-to-text, image-to-text, and text-to-image models. Here, our assumption is that an additional LSTM layer in the encoder will increase the representational power of the network, help it capture more intricate details, and potentially improve the reasoning capabilities of the models. 

### Methodology

The original architecture used a single encoder LSTM layer followed by a decoder mechanism; here, we add a new LSTM layer with the first LSTM layer’s `return_sequences=True` to pass the entire output sequence into the second LSTM layer, making the network deeper. This should enable the encoder to provide richer, more abstract representations before decoding. 
The rest of the network is left intact, and the training for the original and updated model is kept the same.

### Results

text-to-text 
In the original model, we trained for 40 epochs to achieve a test accuracy of 98.9%. We also checked the answers manually and saw that the answers always had errors. It either had an error within +1/-1 or +10/-10. One digit kept being wrong across our test cases, which meant that our model was generalising well enough but was getting errors with the arithmetic operations. This also means that accuracy does not indicate how well the generalisation happens. 
The updated model's test accuracy is 99.7%, but we see mixed results when manually checking the values. Some answers have +1/-1 errors, but others have more than expected. Even though we had better accuracy, we did not see a considerable improvement in generalising all cases.

image-to-text
In the original model, we trained for 25 epochs, and we noticed that the model seemed to reach a training accuracy of 95% after 20 epochs, as in the previous case of test-to-text. Here, we achieve a training accuracy of 98.7%. Still, the model does not generalise well, as the test accuracy is merely 57.7%, which is expected as we have a more complicated problem than the previous one. Manually testing the answers showed that we did get some answers in an error range of +1/-1 percentage in some cases. We also noticed that the validation loss took a dip initially and then continued to increase. 
This time, the updated model showed considerable improvement. The training accuracy is only 82.5%, but the test accuracy is 83.2%, which means our additional LSTM layer has improved the model's generalisation. In manual testing, we found more accurate examples and fewer errors, and the percentage of deviation from the ground truth answer was better.

text-to-image
The original model had a test accuracy of 85.7%, and the updated model had an accuracy of 86%. Adding a new LSTM layer does not significantly increase the model's performance. Manual testing shows that the original model has fewer blurry images than the updated model. 

### Discussion

text-to-text 
The intuition behind adding a new LSTM layer to the encoder was that we could have a more representative latent space. However, we see that even though there was an increase in accuracy, this did not result in better generalisation or any considerable improvement in arithmetic reasoning. This could be because we are overcomplicating a simple problem and overfitting the problem as we add more memory. We also see that the updated model had reached an accuracy of around 70% before the original model, indicating that the model could learn faster. Then, there was a steeper climb above 95% in the next five epochs for the updated model, and it saturates and shows a very slow improvement. In contrast, the original model has a smoother curve, showing it learned the nuances more moderately. More epochs, even though it increased the accuracy, might have caused overfitting and learning more noise information.

image-to-text
Using an LSTM layer to create a more representative latent space has worked out, per our reasoning. The fact that we have a more complicated problem where we have an image-based query rather than a textual query helps us understand how we needed more abstraction for the latent space. We see a considerable improvement in the model's generalisation by adding this LSTM layer to the encoder to achieve this. This one change showed a jump of around 25% in test performance. We also see how the plots are so different; the original model did very badly at generalising the problem, and we see that validation loss plateaus and then rises again. It goes beyond the starting point by the end. The updated network has a much clearer plot that represents the improvement in performance that we could verify manually, too. Here, we can see a clear example of how the problem's complexity benefits directly from adding an LSTM layer.  

text-to-image
Here, an additional LSTM layer does not do much in accuracy. However, manual testing shows that the updated model has more blurry images. This could be because we are converting the text sequence at the beginning into the latent space, which is much simpler than an image query, as in the previous example. We have 5 x MNIST images as the query in the previous example of image-to-text vs the character string query in this example. The LSTM layers help generalise more complex examples with additional abstraction. At the same time, it overgeneralises the case when the task is too simple, as is the case here. 