---
tags:
  - inbox
  - work
  - reference
description:
due date: 2025-11-05
start date: 2025-11-05
end date:
status: Archive
importance level: important
urgency level: urgent
task type: capture
story points: 3
parent nodes:
child nodes:
recurrent:
---

[5 W's and 1 H - Critical Thinking](https://www.reddit.com/r/coolguides/comments/1nrukis/a_cool_guide_to_sharpening_your_critical_thinking/?share_id=lbCUFnxcBh8TwgLEeMV_1&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1)

## Build LLM: Step-by-Step Guide

- [Reddit: ðð®ð¢ð¥ð ð‹ð‹ðŒð¬ ðŸð«ð¨ð¦ ð¬ðœð«ðšð­ðœð¡](https://www.reddit.com/r/learnmachinelearning/comments/1gq6jsr/%F0%9D%90%81%F0%9D%90%AE%F0%9D%90%A2%F0%9D%90%A5%F0%9D%90%9D_%F0%9D%90%8B%F0%9D%90%8B%F0%9D%90%8C%F0%9D%90%AC_%F0%9D%90%9F%F0%9D%90%AB%F0%9D%90%A8%F0%9D%90%A6_%F0%9D%90%AC%F0%9D%90%9C%F0%9D%90%AB%F0%9D%90%9A%F0%9D%90%AD%F0%9D%90%9C%F0%9D%90%A1/)
- [GitHub: LLMs-from-Scratch](https://github.com/rasbt/LLMs-from-scratch) by [Sebastian Raschka](https://www.linkedin.com/in/sebastianraschka/)
- [YouTube: Building LLMs from Scratch](https://www.youtube.com/playlist?list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu)

[[build_a_large_language_model_from_scratch.pdf|Build an LLM from Scratch]] - TODO

## Hierarchical Reasoning Models

- [New AI architecture delivers 100x faster reasoning than LLMs with just 1,000 training examples](https://venturebeat.com/ai/new-ai-architecture-delivers-100x-faster-reasoning-than-llms-with-just-1000-training-examples)

[Hierarchical Reasoning Model](https://arxiv.org/abs/2506.21734) - TODO

## IBM Granite 4.0 Micro

- [Reddit: Granite 4.0 Micro (3.4B) running 100% locally in your browser w/ WebGPU acceleration](https://www.reddit.com/r/LocalLLaMA/comments/1nw8c6y/granite_40_micro_34b_running_100_locally_in_your/?share_id=dvSAQX4Rr8nvInywXNjq4&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1)

## ColBERT Nano Series

- [Reddit: Introducing the ColBERT Nano series of models. All 3 of these models come in at less than 1 million parameters (250K, 450K, 950K)](https://www.reddit.com/r/LocalLLaMA/comments/1o1mpt5/introducing_the_colbert_nano_series_of_models_all/?share_id=8nxjySTdjvq1qeO_h8C_n&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1)
	- Domain-specific models for RETRIEVAL
- [ColBERT Collection HF](https://huggingface.co/collections/NeuML/colbert)

## NanoChat from Karpathy

- [Reddit: It has been 4 hours since the release of Nanochat from Karpathy, and no sign of it here! A new full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase](https://www.reddit.com/r/LocalLLaMA/comments/1o5qo0r/it_has_been_4_hrs_since_the_release_of_nanochat/?share_id=duHbk2grWv70-GYsaQOA9&utm_content=2&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1)
- [GitHub: nanochat](https://github.com/karpathy/nanochat)
- [YouTube: Neural Networks: Zero to Hero](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)

## Hugging Face SMOL Training Playbook

- [Reddit: 200+ pages of Hugging Face secrets on how to train an LLM](http://reddit.com/r/LocalLLaMA/comments/1ok3xie/200_pages_of_hugging_face_secrets_on_how_to_train/?share_id=yP5ehYMRGIbeW-kno40pz&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1)
- [The Smol Training Playbook: The Secrets to Building World-Class LLMs](https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook#introduction) - TODO

## Pruning

- [Reddit: Users of REAP Pruned models, So far how's your experience?](https://www.reddit.com/r/LocalLLaMA/comments/1ok1tkh/users_of_reap_pruned_models_so_far_hows_your/?share_id=cInnM_H1pQZAiip5AbDh1&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1)

## Linear Attention - Kimi

- [Reddit: The first linear attention mechanism O(n) that outperforms modern attention O(n^2). 6Ã— Faster 1M-Token Decoding and Superior Accuracy](https://www.reddit.com/r/singularity/comments/1on25fn/the_first_linear_attention_mechanism_on_that/?share_id=fx0COthw3YbOOw0YtZw4P&utm_content=2&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1)
- [KIMI LINEAR: AN EXPRESSIVE, EFFICIENT ATTENTION ARCHITECTURE](https://arxiv.org/pdf/2510.26692) - TODO

## GPT-OSS

- [Reddit: I implemented GPT-OSS from scratch in pure Python, without PyTorch or a GPU](https://www.reddit.com/r/LocalLLaMA/comments/1oogvcw/i_implemented_gptoss_from_scratch_in_pure_python/?share_id=kyiS1n67a15fD0TyyLGXW&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1)
- [GPTOSS: From-Scratch Implementation](https://www.projektjoe.com/blog/gptoss) - TODO
- [GitHub: GPT-OSS: Implementation from Scratch in Python](https://github.com/projektjoe/gpt-oss) - TODO

## DeepSeek

- [Reddit: Build a DeepSeek Model from Scratch: A Book](https://www.reddit.com/r/LocalLLaMA/comments/1op0yep/build_a_deepseek_model_from_scratch_a_book/?share_id=1aI-4oPraaNNtkj6lEre-&utm_content=2&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1)
- GET PDF ONCE IT'S OUT


---
%% 
(resources:: Resources - 202511052214)
%%
> [!info] 2025-11-05
> > [!example] **Resources:**
> > - [I trained an LLM from scratch AMA!](https://www.reddit.com/r/LocalLLaMA/comments/1nqkayx/i_trained_an_llm_from_scratch_ama/?share_id=PEsNJ3bv4bSvbXD5ZclDV&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1)
> > 	- [Code an LLM From Scratch â€“ Theory to RLHF](https://www.freecodecamp.org/news/code-an-llm-from-scratch-theory-to-rlhf/)

---
%% 
(action-items:: Action Items - 202511071822) 
%%
> [!info] 2025-11-07
> > [!todo] **Action Items:**
> > - [ ] Make Tasks for each TODO

