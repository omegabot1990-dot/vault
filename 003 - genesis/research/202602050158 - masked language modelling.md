---
tags:
  - note
aliases:
  - Masked language modelling
title: masked language modelling
description:
parent nodes:
  - "[[202602050153 - self-supervised learning|Self-supervised learning]]"
child nodes:
annotation-target:
published on:
---

- [ ] Update BERT link

---
- Masked language modelling involves hiding (masking) random tokens within a sentence and tasking the model with <mark style="background: #BBFABBA6;">predicting those missing words using the surrounding context</mark>
- The model can look at both the words to the left and the words to the right of the mask to make its prediction
- Self-supervised learning paradigm
- Example: Natural Language Understanding ([[encoder transformer|BERT]])