---
tags:
  - work
  - academic
aliases:
  - Quantisation
title: quantisation
description: introduction to quantisation
parent nodes:
  - "[[definitions]]"
child nodes:
annotation-target:
---

# üß† **What is Quantisation?**

**Quantisation** is a model-compression technique where you reduce the precision of numbers used to store neural network weights and activations.

Instead of storing values as **32-bit floating-point (FP32)**, we convert them to **lower-precision formats** such as FP16, INT8, INT4, or even INT2.

### ‚úÖ Why quantise?

- **Smaller model size**  
    ‚Üí Fits on laptops, phones, or edge devices (Jetson/Raspberry Pi).
    
- **Faster inference**  
    ‚Üí Lower precision ‚Üí fewer bits ‚Üí faster matrix multiplications.
    
- **Lower energy usage**  
    ‚Üí Important for sustainable mental-health assistants.
    
- **Minimal accuracy drop** (if done correctly)
    

---

# ‚öôÔ∏è **Major Quantisation Methods**

Below are the most relevant techniques for LLMs today:

---

## **1. Post-Training Quantisation (PTQ)**

The simplest and most common.

You take a trained FP32 model and convert it to lower precision **without retraining**.

### Variants:

### üîπ **8-bit (INT8) Quantisation**

- Weights stored as 8-bit integers
    
- Activations often remain FP16 or FP32
    
- ~2√ó size reduction
    
- Good accuracy retention
    
- Supported by many inference engines (TensorRT, ONNX Runtime, llama.cpp)
    

### üîπ **4-bit Quantisation (INT4 / Q4)**

- Weights in 4 bits
    
- ~4√ó smaller than FP32
    
- Great for LLMs on laptops & phones
    
- Slight accuracy drop but usually acceptable
    
- Common in GGUF models
    

### üîπ **3-bit / 2-bit Quantisation**

- More aggressive
    
- Used for extremely low-power devices
    
- Bigger accuracy loss ‚Üí requires tuning
    

---

## **2. Quantisation-Aware Training (QAT)**

You simulate quantisation _during training_, so the model learns to compensate.  
This gives the **highest accuracy** for low-bit models (INT4/INT2).

- Costly because you have to retrain
    
- Used for production-grade low-bit models (e.g., mobile NLP, edge vision)
    

---

## **3. GPTQ (Generalized Post-Training Quantization)**

A PTQ method specialized for **transformers**.

- Quantises weights column-wise
    
- Uses a small calibration dataset
    
- Very good accuracy even at **3‚Äì4 bits**
    
- Early SOTA for quantising LLaMA models
    

---

## **4. AWQ (Activation-Aware Weight Quantization)**

A newer PTQ method.

- Focuses on ‚Äúimportant‚Äù weights
    
- Quantises only the less-sensitive areas
    
- Better accuracy than GPTQ for many models
    
- Tends to outperform GPTQ for LLaMA-2, Mistral, etc.
    

---

## **5. KV-Cache Quantisation**

LLMs store past tokens in a **KV cache**.  
Quantising the KV cache (e.g., to FP8 or INT4):

- Reduces memory usage during long conversations
    
- Speeds up inference
    
- Used by R1, DeepSeek, and Gemini inference optimisations
    

---

