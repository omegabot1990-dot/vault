---
tags:
  - training
aliases:
  - Cosine Decay
title: cosine decay
description: ""
bot: false
parent nodes:
  - "[[202602111617 - learning rate schedule|Learning Rate Schedule]]"
published on:
---

- Cosine decay is a [[202602111617 - learning rate schedule|learning-rate schedule]] that smoothly decreases the step size following a cosine curve over [[202602111335 - training|training]]
- A common form is:

> [!MATH] Cosine Decay
  > $$\eta_t = \eta_{\min} + \tfrac{1}{2}(\eta_{\max}-\eta_{\min})(1+\cos(\pi t/T))$$

- Early in training it keeps relatively larger updates, then gradually anneals toward smaller updates
- This can improve optimization stability and final convergence compared with abrupt step drops
- Cosine schedules are often combined with [[202602011426 - warmup|warmup]] and optional restarts (SGDR)
- Schedule quality depends on total training horizon $T$, warmup length, and minimum learning rate choice