---
tags:
- note
aliases:
- Cosine Decay
title: cosine decay
description: ''
bot: true
parent nodes:
- '[[202602111256 - hyperparameters|Hyperparameters]]'
published on: null
---

- Cosine decay is a learning-rate schedule that smoothly decreases the step size following a cosine curve over training
- A common form is:
  - $$\eta_t = \eta_{\min} + \tfrac{1}{2}(\eta_{\max}-\eta_{\min})(1+\cos(\pi t/T))$$
- Early in training it keeps relatively larger updates, then gradually anneals toward smaller updates
- This can improve optimization stability and final convergence compared with abrupt step drops
- Cosine schedules are often combined with warmup and optional restarts (SGDR)
- Schedule quality depends on total training horizon $T$, warmup length, and minimum learning rate choice