---
tags:
  - note
aliases:
  - Precision
title: precision
description:
parent nodes:
  - "[[evaluation]]"
published on:
---

- The proportion of positive predictions that were actually correct
- $\text{Precision} = \frac{\text{True Positives}}{\text{True Positives} + \text{False Positives}}$
- Precision and recall often have an inverse relationship; increasing one often decreases the other

| Metric        | Focus Question                                   |                 Formula                 | Best Used When...               |
|:------------- |:------------------------------------------------ |:---------------------------------------:|:------------------------------- |
| **Accuracy**  | How often is the model correct overall?          | $\frac{(TP + TN)}{(TP + TN + FP + FN)}$ | Classes are balanced (50/50).   |
| **Precision** | Of all predicted positives, how many were right? |         $\frac{TP}{(TP + FP)}$          | You want to avoid False Alarms. |
| **Recall**    | Of all actual positives, how many were caught?   |         $\frac{TP}{(TP + FN)}$          | You want to avoid Missed Cases. |
#### Confusion Matrix
$$
\begin{array}{l|c|c}
 & \text{Predicted Positive} & \text{Predicted Negative} \\
\hline
\text{Actual Positive} & \text{TP} & \text{FN} \\
\hline
\text{Actual Negative} & \text{FP} & \text{TN} \\
\end{array}
$$