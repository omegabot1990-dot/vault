---
tags:
- math
aliases:
- Variance
title: variance
description: ''
parent nodes:
- '[[202602062059 - statistics|Statistics]]'
published on: null
---

- Variance measures <mark style="background: #BBFABBA6;">how "spread out" the data points are around the mean</mark>
- It is the average of the squared differences from the [[202602062110 - mean|mean]]
- Average squared deviation from the mean:

> [!MATH] Variance ($\sigma^2$)
> $$\sigma^2 = \frac{\sum (x_i - \mu)^2}{N}$$

- Squaring
	- We square the differences so that negative distances don't cancel out positive ones
- Scale
	- Because it is squared, the units are also squared (e.g., if data is in "meters," variance is in "meters squared")
- Variance of a [[202602061340 - random variable|random variable]] is as follows,

> [!MATH] Variance: $Var(X)$ or $\sigma^2$
> $$\sigma^2 = Var[X] = E[(X - \mu)^2] = E[X^2] - (E[X])^2$$

- Let $\mu = E[X]$ be the expected value (mean),

> [!MATH] The Step-by-Step Proof
> 
> **1. The Definition:**
> $$\sigma^2 = E[(X - \mu)^2]$$
> 
> **2. Expand the Square:**
> Inside the expectation, expand $(X - \mu)^2$ using algebra:
> $$\sigma^2 = E[X^2 - 2X\mu + \mu^2]$$
> 
> **3. Use Linearity of Expectation:**
> Distribute the $E[\cdot]$ across the terms:
> $$\sigma^2 = E[X^2] - E[2X\mu] + E[\mu^2]$$
> 
> **4. Simplify Constants:**
> Since $\mu$ is a constant number (the mean), we can pull it out of the expectation:
> $$\sigma^2 = E[X^2] - 2\mu E[X] + \mu^2$$
> 
> **5. Substitute $\mu$ back in:**
> Remember that $E[X] = \mu$. Replace $E[X]$ with $\mu$:
> $$\sigma^2 = E[X^2] - 2\mu(\mu) + \mu^2$$
> $$\sigma^2 = E[X^2] - 2\mu^2 + \mu^2$$
> 
> **6. Final Result:**
> $$\sigma^2 = E[X^2] - (E[X])^2$$


[^1]: [Calculating the Mean, Variance and Standard Deviation, Clearly Explained!!!](https://www.youtube.com/watch?v=SzZ6GpcfoQY)
[^2]: https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/random-variables.html#:~:text=This%20is%20a%20quantitative%20measure%20of%20how%20far%20a%20random%20variable%20deviates%20from%20the%20mean