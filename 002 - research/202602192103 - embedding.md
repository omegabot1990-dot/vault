---
tags:
  - nlp
aliases:
  - Embedding
title: embedding
description: ""
bot: false
parent nodes:
  - "[[202602061133 - natural language processing|Natural Language Processing]]"
  - "[[embeddings]]"
published on:
---

- An embedding maps discrete [[202602192100 - token|tokens]] to dense continuous vectors in a learned representation space
- Token embeddings allow [[202602061153 - neural network|neural networks]] to operate on <mark style="background: #BBFABBA6;">symbolic inputs with geometric structure</mark>
- In [[202602102143 - transformer|transformers]], embedding vectors are combined with [[202602192054 - positional encoding|positional information]] before [[202602191531 - attention|attention]] layers
- Similar tokens tend to occupy nearby regions when [[202602111335 - training|training]] induces semantic or syntactic structure
- <mark style="background: #FF5582A6;">Embedding dimension controls representational capacity and compute/memory cost</mark>
- <mark style="background: #FFF3A3A6;">Input and output embeddings are often tied to improve parameter efficiency and consistency</mark>