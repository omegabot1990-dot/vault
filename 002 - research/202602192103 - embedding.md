---
tags:
- note
aliases:
- Embedding
title: embedding
description: ''
bot: true
parent nodes:
- '[[transformers]]'
published on: null
---

- An embedding maps discrete tokens to dense continuous vectors in a learned representation space
- Token embeddings allow neural networks to operate on symbolic inputs with geometric structure
- In transformers, embedding vectors are combined with positional information before attention layers
- Similar tokens tend to occupy nearby regions when training induces semantic or syntactic structure
- Embedding dimension controls representational capacity and compute/memory cost
- Input and output embeddings are often tied to improve parameter efficiency and consistency