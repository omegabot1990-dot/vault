---
tags:
- math
aliases:
- Joint entropy
title: joint entropy
description: ''
bot: true
parent nodes:
- '[[math]]'
published on: null
---

- Joint entropy measures total uncertainty in a pair of random variables considered together
- It extends entropy from one variable to multivariate settings
- For discrete variables, it is computed from the joint distribution

> [!MATH] Joint entropy definition
> For discrete random variables $X,Y$ with joint distribution $p(x,y)$:
> $$H(X,Y)=-\sum_{x,y} p(x,y)\log p(x,y)$$

- Chain rule relation
  - $$H(X,Y)=H(X)+H(Y\mid X)=H(Y)+H(X\mid Y)$$

- If $X$ and $Y$ are independent, then $H(X,Y)=H(X)+H(Y)$
- Joint entropy is linked to mutual information via
  $$I(X;Y)=H(X)+H(Y)-H(X,Y)$$