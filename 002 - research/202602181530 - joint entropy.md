---
tags:
  - math
aliases:
  - Joint entropy
title: joint entropy
description: ""
bot: true
parent nodes:
  - "[[202602061804 - entropy|Entropy]]"
published on:
---

- Joint entropy measures total uncertainty in a pair of [[202602061340 - random variable|random variables]] considered together
- It extends [[202602061804 - entropy|entropy]] from one variable to multivariate settings
- For discrete variables, it is computed from the joint distribution
- For discrete random variables $X,Y$ with joint distribution $p(x,y)$:

> [!MATH] Joint Entropy
> $$H(X,Y)=-\sum_{x,y} p(x,y)\log p(x,y)$$

- Chain rule relation:

> [!MATH] Chain Rule
> $$H(X,Y)=H(X)+H(Y\mid X)=H(Y)+H(X\mid Y)$$
> - If $X$ and $Y$ are independent, then $H(X,Y)=H(X)+H(Y)$

- Joint entropy is linked to [[202602181338 - mutual information|mutual information]] via
  
> [!MATH] MI Relation
> $$I(X;Y)=H(X)+H(Y)-H(X,Y)$$