---
tags:
  - moc
description:
parent nodes:
  - "[[post-training]]"
child nodes:
---

- [ ] [YouTube - Knowledge Distillation: How LLMs train each other](https://www.youtube.com/watch?v=jrJKRYAdh7I&list=PLDSlxv8fpkvgNQTSCCvEaRRmEp5tk9gpC&index=9)
- [ ] [A Survey on Knowledge Distillation of Large Language Model](https://www.alphaxiv.org/abs/2402.13116)
- [ ] [Self-Distillation Enables Continual Learning](https://www.alphaxiv.org/abs/2601.19897)
	- [ ] <mark style="background: #FF5582A6;">IMPORTANT</mark>