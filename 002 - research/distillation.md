---
tags:
- moc
description: ''
parent nodes:
- '[[post-training]]'
---

## Topics

## Blogs

## Papers

- [ ] [A Survey on Knowledge Distillation of Large Language Model](https://www.alphaxiv.org/abs/2402.13116)

- [ ] [Self-Distillation Enables Continual Learning](https://www.alphaxiv.org/abs/2601.19897)
	- [ ] <mark style="background: #FF5582A6;">IMPORTANT</mark>

## Videos

- [ ] [YouTube - Knowledge Distillation: How LLMs train each other](https://www.youtube.com/watch?v=jrJKRYAdh7I&list=PLDSlxv8fpkvgNQTSCCvEaRRmEp5tk9gpC&index=9)

## Code
