---
tags:
- note
aliases:
- Reasoning
title: reasoning
description: ''
bot: true
parent nodes:
- '[[large language models]]'
published on: null
---

- In LLMs, reasoning is the modelâ€™s ability to perform multi-step inference to derive answers beyond direct recall
- It often requires decomposition, intermediate consistency checks, and error correction across steps
- Reasoning quality is usually evaluated on math, logic, coding, and planning-style benchmarks
- Improvements come from better training signals, inference-time scaling, and post-training methods like RL-based optimization
- Strong reasoning does not guarantee factuality, so grounding and verification remain necessary
- Practical systems combine reasoning with tools, retrieval, and constraint checks for reliability