---
tags:
  - training
aliases:
  - Post-Training
  - Post-training
title: post training
description: ""
bot: false
parent nodes:
  - "[[202602111335 - training|Training]]"
  - "[[post-training]]"
published on:
---

- [ ] What is RLAIF?
- [ ] What is DPO?

---
- Post-training is the phase after base [[202602030230 - pre-training|pre-training]], where a [[202602010044 - model|model]] is adapted for alignment and downstream behaviour
- Typical stages include [[202601282201 - supervised fine-tuning|supervised fine-tuning]], preference optimization, and safety tuning
- It improves:
	- Instruction following
	- Helpfulness
	- Harmlessness
	- Task-specific performance
- Common methods include [[202601282201 - supervised fine-tuning|SFT]], [[202602192217 - reinforcement learning human feedback|RLHF]]/RLAIF, DPO-family objectives, and rejection sampling pipelines
- Post-training can significantly change model behaviour without changing the core architecture
- Trade-offs include capability retention, alignment tax, compute cost, and evaluation complexity