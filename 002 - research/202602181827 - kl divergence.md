---
tags:
  - math
aliases:
  - KL Divergence
  - Kullback-Leibler Divergence
title: kl divergence
description: ""
bot: false
parent nodes:
  - "[[202602072325 - distribution|Distribution]]"
published on:
---

- KL divergence measures how one probability [[202602072325 - distribution|distribution]] differs from a reference distribution
- It is <mark style="background: #FF5582A6;">not symmetric</mark>, so in general $D_{KL}(P\|Q)\neq D_{KL}(Q\|P)$ [^2]
- It is always <mark style="background: #BBFABBA6;">nonnegative</mark> and equals zero if and only if the distributions are equal almost everywhere [^3]
- For discrete distributions $P,Q$ on support $\mathcal{X}$:

> [!MATH] Discrete KL Divergence
> $$D_{\mathrm{KL}}(P\|Q)=\sum_{x\in\mathcal{X}} P(x)\log\frac{P(x)}{Q(x)}$$

- For densities $p,q$:

> [!MATH] Continuous KL Divergence
> $$D_{\mathrm{KL}}(p\|q)=\int p(x)\log\frac{p(x)}{q(x)}\,dx$$

- KL divergence can be written as [[202602120102 - cross-entropy|cross-entropy]] minus [[202602061804 - entropy|entropy]]:

> [!MATH] In terms of Entropy and Cross-Entropy
> $$D_{\mathrm{KL}}(P\|Q)=H(P,Q)-H(P)$$

- In learning, minimising KL often appears in variational inference, distillation, and distribution matching


[^1]: [A Short Introduction to Entropy, Cross-Entropy and KL-Divergence](https://www.youtube.com/watch?v=ErfnhcEV1O8)
[^2]: https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html#:~:text=KL%20divergence%20is,(22.11.22)
[^3]: https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html#:~:text=KL%20divergence%20is%20non%2Dnegative,the%20equality%20holds%20only%20when