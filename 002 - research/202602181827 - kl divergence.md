---
tags:
- math
aliases:
- KL divergence
- Kullback-Leibler divergence
title: kl divergence
description: ''
bot: true
parent nodes:
- '[[math]]'
published on: null
---

- KL divergence measures how one probability distribution differs from a reference distribution
- It is not symmetric, so in general $D_{KL}(P\|Q)\neq D_{KL}(Q\|P)$
- It is always nonnegative and equals zero iff the distributions are equal almost everywhere

> [!MATH] Discrete KL divergence
> For discrete distributions $P,Q$ on support $\mathcal{X}$:
> $$D_{\mathrm{KL}}(P\|Q)=\sum_{x\in\mathcal{X}} P(x)\log\frac{P(x)}{Q(x)}$$

> [!MATH] Continuous KL divergence
> For densities $p,q$:
> $$D_{\mathrm{KL}}(p\|q)=\int p(x)\log\frac{p(x)}{q(x)}\,dx$$

- KL divergence can be written as cross-entropy minus entropy:
  $$D_{\mathrm{KL}}(P\|Q)=H(P,Q)-H(P)$$
- In learning, minimizing KL often appears in variational inference, distillation, and distribution matching