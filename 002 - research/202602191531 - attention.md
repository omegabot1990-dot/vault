---
tags:
- note
aliases:
- Attention
title: attention
description: ''
bot: true
parent nodes:
- '[[transformers]]'
published on: null
---

- Attention is a mechanism that lets each token weigh information from other tokens based on relevance
- It computes context-aware representations by comparing queries with keys and mixing values
- In transformers, self-attention enables direct token-to-token interaction without recurrence
- Multi-head attention runs several attention projections in parallel to capture different relation patterns
- Scaled dot-product attention uses softmax-normalised similarity scores to form weighted sums
- Attention improves long-range dependency modeling and parallel computation in sequence models