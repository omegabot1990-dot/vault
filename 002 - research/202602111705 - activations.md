---
tags:
- note
aliases:
- Activations
title: activations
description: ''
bot: true
parent nodes:
- '[[deep learning]]'
published on: null
---

- Activations are nonlinear functions applied to layer outputs in neural networks
- They let deep models represent complex nonlinear mappings instead of collapsing to a linear transform
- Common choices are ReLU, GELU, sigmoid, tanh, and softmax depending on layer role
- Hidden-layer activations affect gradient flow, sparsity, and convergence dynamics
- Output-layer activation is task dependent, such as sigmoid for binary classification or softmax for multiclass
- Activation choice interacts with initialization, normalization, and optimizer settings