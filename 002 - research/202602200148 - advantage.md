---
tags:
  - reinforcement_learning
aliases:
  - Advantage
title: advantage
description: ""
bot: false
parent nodes:
  - "[[202602200129 - policy gradient methods|Policy Gradient Methods]]"
published on:
---

- [ ] What are temporal-difference residuals?
- [ ] What is generalised advantage estimation (GAE)?

---
- Advantage quantifies how much better an [[202602192350 - action|action]] is than the [[202602192245 - policy|policyâ€™s]] average action at a [[202602192351 - state|state]]
- <mark style="background: #BBFABBA6;">It centres action values by subtracting the state value baseline</mark>
- Positive advantage means the action is better than expected under the current policy
- Advantage is widely used in [[202602200129 - policy gradient methods|policy-gradient]] updates to reduce [[202602062206 - variance|variance]] and improve the learning signal
- Definition:

> [!MATH] Advantage
> $$A^\pi(s,a)=Q^\pi(s,a)-V^\pi(s)$$

- Practical estimators include temporal-difference residuals and generalised advantage estimation (GAE)