---
tags:
  - reinforcement_learning
aliases:
  - Advantage
title: advantage
description: ""
bot: false
parent nodes:
  - "[[202602192359 - value estimation|Value Estimation]]"
published on:
---

- Advantage quantifies how much better an [[202602192350 - action|action]] is than the [[202602192245 - policy|policyâ€™s]] average action at a [[202602192351 - state|state]]
- <mark style="background: #BBFABBA6;">It centres action values by subtracting the state value baseline</mark>
- Positive advantage means the action is better than expected under the current policy
- Advantage is widely used in [[202602200129 - policy gradient methods|policy-gradient]] updates to reduce [[202602062206 - variance|variance]] and improve the learning signal
- Definition:

> [!MATH] Advantage
> $$A^\pi(s,a)=Q^\pi(s,a)-V^\pi(s)$$

- Practical estimators include [[202602201305 - temporal-difference algorithm|temporal-difference residuals]] and [[202602211411 - generalized advantage estimation|generalised advantage estimation (GAE)]]