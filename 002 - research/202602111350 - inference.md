---
tags:
  - deep_learning
  - inference
aliases:
  - Inference
title: inference
description: ""
bot: true
parent nodes:
  - "[[inference]]"
published on:
---

- [ ] What is decoding?
- [ ] What is greedy decoding?
- [ ] What is beam search?
- [ ] What is top-k sampling?
- [ ] What is top-p sampling or nucleus sampling?

---
- Inference is the stage where a trained model is used to make predictions on new inputs
- During inference, parameters are fixed, and no gradient updates are applied
- The main goals are prediction quality, low latency, and efficient resource use
- [[202602111625 - batch size|Batch Size]], precision, and hardware choice strongly affect inference throughput and cost
- For generative language models, the decoding strategy controls output quality and speed
- Common decoding methods are greedy decoding, beam search, top-k sampling, and nucleus sampling