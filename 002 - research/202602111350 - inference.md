---
tags:
- note
aliases:
- Inference
title: inference
description: ''
bot: true
parent nodes:
- '[[inference]]'
published on: null
---

- Inference is the stage where a trained model is used to make predictions on new inputs
- During inference, parameters are fixed and no gradient updates are applied
- The main goals are prediction quality, low latency, and efficient resource use
- Batch size, precision, and hardware choice strongly affect inference throughput and cost
- For generative language models, decoding strategy controls output quality and speed
- Common decoding methods are greedy decoding, beam search, top-k sampling, and nucleus sampling