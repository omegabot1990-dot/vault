---
tags:
  - inference
aliases:
  - Inference
title: inference
description: ""
bot: false
parent nodes:
  - "[[inference]]"
published on:
---

- [ ] What is decoding?
- [ ] What is greedy decoding?

---
- Inference is the stage where a [[202602111335 - training|trained]] [[202602010044 - model|model]] is used to make predictions on new inputs
- During inference, [[202602111314 - model parameters|parameters]] are fixed, and no gradient updates are applied
- The main goals are prediction quality, low latency, and efficient resource use
- [[202602111625 - batch size|Batch Size]], precision, and hardware choice strongly affect inference throughput and cost
- For generative language models, the decoding strategy controls output quality and speed
- Common decoding methods are greedy decoding, [[202602010147 - beam search|beam search]], [[202602011422 - top k|top-k sampling]], and [[202602011423 - top p|nucleus sampling]]