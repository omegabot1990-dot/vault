---
tags:
  - deep_learning
aliases:
  - Activation
title: activation
description: ""
bot: false
parent nodes:
  - "[[202602061153 - neural network|Neural Network]]"
published on:
---

- Activation is the <mark style="background: #BBFABBA6;">nonlinear transformation</mark> applied after an [[202602111417 - affine map|affine]] layer output
- Without activation functions, <mark style="background: #FF5582A6;">stacked layers collapse to a single linear map</mark>
- Nonlinearity enables [[202602061153 - neural network|neural networks]] to model complex functions and decision boundaries
- Common activations are:
	- [[202602111510 - sigmoid|Sigmoid]]
	- [[202602111518 - tanh|Tanh]]
	- [[202602111526 - relu|ReLU]]
	- [[202602111527 - gelu|GELU]]
	- [[202602111530 - softmax|Softmax]] (for output normalisation)
- Activation choice affects gradient flow, convergence speed, and final model quality
- Hidden layers usually use ReLU-family activations, while the output activation depends on the task type