---
tags:
- note
aliases:
- Activation
title: activation
description: ''
bot: true
parent nodes:
- '[[neural networks]]'
published on: null
---

- Activation is the nonlinear transformation applied after an affine layer output
- Without activation functions, stacked layers collapse to a single linear map
- Nonlinearity enables neural networks to model complex functions and decision boundaries
- Common activations are sigmoid, tanh, ReLU, GELU, and softmax (for output normalization)
- Activation choice affects gradient flow, convergence speed, and final model quality
- Hidden layers usually use ReLU-family activations, while output activation depends on task type