---
tags:
- note
aliases:
- Large Language Model
- LLM
title: large language model
description: ''
bot: true
parent nodes:
- '[[large language models]]'
published on: null
---

- A large language model is a neural network trained on massive text corpora to model token sequences
- Most modern LLMs use transformer architectures with self-attention and next-token prediction objectives
- Pre-training learns broad linguistic and world knowledge, while post-training aligns behavior for downstream use
- Capabilities include generation, summarization, translation, coding, and reasoning with context-dependent quality
- Performance depends on model scale, data quality, compute budget, and training recipe
- Common limitations include hallucinations, bias, context-window constraints, and sensitivity to prompting