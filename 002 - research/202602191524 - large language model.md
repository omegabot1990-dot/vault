---
tags:
  - transformers
aliases:
  - Large Language Model
  - LLM
title: large language model
description: ""
bot: false
parent nodes:
  - "[[large language models]]"
published on:
---

- A large language model is a [[202602061153 - neural network|neural network]] [[202602111335 - training|trained]] on massive text corpora to model token sequences
<<<<<<< HEAD
- Most modern LLMs use [[202602102143 - transformer|transformer]] architectures with [[202602191531 - attention|self-attention]] and [[202602050155 - causal language modelling|next-token prediction]] objectives
- [[202602030230 - pre-training|Pre-training]] learns broad linguistic and world knowledge, while post-training aligns behaviour for downstream use
- Capabilities include generation, summarisation, translation, coding, and reasoning with context-dependent quality
- Performance depends on [[202602111314 - model parameters|model scale]], data quality, compute budget, and [[202602111335 - training|training]] recipe
=======
- Most modern LLMs use [[202602102143 - transformer|transformer]] architectures with self-attention and next-token prediction objectives
- Pre-training learns broad linguistic and world knowledge, while post-training aligns behavior for downstream use
- Capabilities include generation, summarization, translation, coding, and reasoning with context-dependent quality
- Performance depends on model scale, data quality, compute budget, and training recipe
>>>>>>> db14ed6fea0c654ca337628fccb43d39d77d0c5c
- Common limitations include hallucinations, bias, context-window constraints, and sensitivity to prompting