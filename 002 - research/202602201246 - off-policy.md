---
tags:
  - reinforcement_learning
aliases:
  - Off-policy
title: off-policy
description: ""
bot: false
parent nodes:
  - "[[202602192245 - policy|Policy]]"
published on:
---

- Off-policy reinforcement learning learns a target [[202602192245 - policy|policy]] from data generated by a different behaviour policy
- <mark style="background: #BBFABBA6;">It enables the reuse of past experience and improves sample efficiency</mark> via replay buffers or logged trajectories
- [[202602201303 - q-learning|Q-learning]] and many [[202602201254 - actor-critic methods|actor-critic]] variants are off-policy methods
- [[202602072325 - distribution|Distribution]] mismatch between behaviour and target policies can introduce bias or instability
- [[202602211445 - importance sampling|Importance sampling]] and conservative update rules are common corrections for off-policy drift
- Off-policy learning is central when interaction is expensive or when large offline datasets are available


[^1]: [Monte Carlo And Off-Policy Methods | Reinforcement Learning Part 3](https://www.youtube.com/watch?v=bpUszPiWM7o&list=PLzvYlJMoZ02Dxtwe-MmH4nOB5jYlMGBjr&index=3)