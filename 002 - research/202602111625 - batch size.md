---
tags:
- note
aliases:
- Batch Size
title: batch size
description: ''
bot: true
parent nodes:
- '[[hyperparameters]]'
published on: null
---

- Batch size is the number of training examples used to compute one gradient update
- It is a key hyperparameter controlling memory use, update frequency, and gradient noise
- Small batch sizes give noisier gradients but can improve exploration and generalization
- Large batch sizes give smoother gradients and better hardware throughput
- Effective batch size depends on model scale, optimizer, and learning-rate settings
- One epoch contains approximately $\lceil N/B \rceil$ updates for dataset size $N$ and batch size $B$