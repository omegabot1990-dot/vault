---
tags:
  - training
aliases:
  - Batch Size
title: batch size
description: ""
bot: false
parent nodes:
  - "[[202602111256 - hyperparameters|Hyperparameters]]"
published on:
---

- Batch size is the number of [[202602111335 - training|training]] examples used to compute one gradient update
- It is a key [[202602111256 - hyperparameters|hyperparameter]] controlling memory use, update frequency, and gradient noise
- <mark style="background: #BBFABBA6;">Small batch sizes give noisier gradients but can improve exploration and generalisation</mark>
- <mark style="background: #ABF7F7A6;">Large batch sizes give smoother gradients and better hardware throughput</mark>
- Effective batch size depends on [[202602010044 - model|model]] scale, [[202602111605 - optimization algorithms|optimizer]], and [[202602111600 - learning rate|learning-rate]] settings
- One [[202602111623 - epochs|epoch]] contains approximately $\lceil N/B \rceil$ updates for dataset size $N$ and batch size $B$