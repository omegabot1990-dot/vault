---
tags:
- note
aliases:
- Evaluation
title: evaluation
description: ''
bot: true
parent nodes:
- '[[deep learning]]'
published on: null
---

- Evaluation measures how well a trained model performs on data not used for parameter updates
- It estimates generalization quality and supports model comparison and selection
- Metrics must match task type, such as accuracy/F1 for classification and MAE/RMSE for regression
- Robust evaluation uses held-out validation and test splits to reduce selection bias
- Error analysis by slices and failure cases helps diagnose weakness beyond a single scalar score
- Good evaluation also checks calibration, robustness, and cost-latency trade-offs when relevant