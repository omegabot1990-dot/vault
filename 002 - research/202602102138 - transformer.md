---
tags:
- note
description: ''
bot: true
parent nodes:
- '[[transformers]]'
aliases:
- Transformer
published on: null
---

# 202602102138 - transformer

## Summary

- Neural network architecture for sequence modelling built around attention
- Replaces recurrence with parallel token-to-token interactions via self-attention
- Common backbone for modern language models

## Notes

- Core building blocks
	- Token embeddings plus positional information
	- Self-attention to mix information across positions
	- Feed-forward network applied per token
	- Residual connections and layer normalization for stable training
- Encoder decoder vs decoder only
	- Encoder decoder maps an input sequence to an output sequence
	- Decoder only predicts next tokens autoregressively
- Multi-head attention
	- Runs several attention heads in parallel
	- Lets different heads focus on different relations
- Scaling behaviour
	- Larger models and more data tend to yield better generalization
	- Inference cost scales with sequence length due to attention

## References
