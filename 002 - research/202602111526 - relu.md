---
tags:
  - math
aliases:
  - ReLU
  - Rectified Linear Unit
title: relu
description: ""
bot: false
parent nodes:
  - "[[202602111400 - activation|Activation]]"
published on:
---

- Rectified Linear Unit or ReLU is a <mark style="background: #BBFABBA6;">piecewise linear</mark> activation that keeps positive inputs and clips negative inputs to zero
- It is widely used because it is simple and computationally efficient
- The rectified linear unit is:

> [!MATH] ReLU function
> $$\operatorname{ReLU}(x)=\max(0,x)$$

- Piecewise form
    - $\operatorname{ReLU}(x)=0$ for $x<0$
    - $\operatorname{ReLU}(x)=x$ for $x\ge 0$

![ReLU|700](https://docs.pytorch.org/docs/stable/_images/ReLU.png)

> [!MATH] Derivative of ReLU
> For $x\neq 0$:
> $$\frac{d}{dx}\operatorname{ReLU}(x)=\begin{cases}0,&x<0\\1,&x>0\end{cases}$$

- ReLU <mark style="background: #FF5582A6;">can create sparse activations</mark> because many negative inputs map to zero