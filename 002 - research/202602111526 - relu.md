---
tags:
- math
aliases:
- ReLU
- Rectified Linear Unit
title: relu
description: ''
bot: true
parent nodes:
- '[[math]]'
published on: null
---

- ReLU is a piecewise linear activation that keeps positive inputs and clips negative inputs to zero
- It is widely used because it is simple and computationally efficient

> [!MATH] ReLU function
> The rectified linear unit is
> $$\operatorname{ReLU}(x)=\max(0,x)$$

- Piecewise form
  - $\operatorname{ReLU}(x)=0$ for $x<0$
  - $\operatorname{ReLU}(x)=x$ for $x\ge 0$

> [!MATH] Derivative of ReLU
> For $x\neq 0$:
> $$\frac{d}{dx}\operatorname{ReLU}(x)=\begin{cases}0,&x<0\\1,&x>0\end{cases}$$

- ReLU can create sparse activations because many negative inputs map to zero