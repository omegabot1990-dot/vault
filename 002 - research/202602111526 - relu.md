---
tags:
- math
aliases:
- ReLU
- Rectified Linear Unit
title: relu
description: ''
bot: true
parent nodes:
- '[[math]]'
published on: null
---

- ReLU is a piecewise linear activation that keeps positive inputs and clips negative inputs to zero
- It is widely used because it is simple and computationally efficient
- The rectified linear unit is:

> [!MATH] ReLU function
> $$\operatorname{ReLU}(x)=\max(0,x)$$

- Piecewise form
    - $\operatorname{ReLU}(x)=0$ for $x<0$
    - $\operatorname{ReLU}(x)=x$ for $x\ge 0$

![ReLU|700](https://docs.pytorch.org/docs/stable/_images/ReLU.png)

> [!MATH] Derivative of ReLU
> For $x\neq 0$:
> $$\frac{d}{dx}\operatorname{ReLU}(x)=\begin{cases}0,&x<0\\1,&x>0\end{cases}$$

- ReLU can create sparse activations because many negative inputs map to zero