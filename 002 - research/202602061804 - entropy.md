---
tags:
- math
aliases:
- Entropy
title: entropy
description: ''
parent nodes:
- '[[202602061530 - self-information|Self-Information]]'
published on: null
---

- Entropy (denoted as $H$) is the mathematical <mark style="background: #BBFABBA6;">measure of uncertainty or unpredictability</mark> in a set of data
- While [[202602061530 - self-information|self-information]] measures the surprise of a <mark style="background: #FF5582A6;">single event</mark>, Entropy measures the average uncertainty of the <mark style="background: #ADCCFFA6;">entire system</mark>
- It is the expected value of the information content across all possible outcomes:

> [!MATH] Entropy $H(X)$
> $$H(X) = E[I(x)] = -\sum_{i=1}^{n} P(X=x_i) \log_2 P(X=x_i)$$
> 
> - **$P(X=x_i)$**: The probability of outcome $i$
> - **$\log P(X=x_i)$**: The surprise/information of that outcome

- Units
	- Measured in <mark style="background: #FF5582A6;">bits</mark> (when using $\log_2$), for discrete random variables, same as [[202602061530 - self-information|self-information]]

- For a continuous random variable $X$ with a probability density function (PDF) $f(x)$:

> [!MATH] Differential Entropy $h(X)$
> 
> $$h(X) = -\int_{-\infty}^{\infty} f(x) \ln f(x) \, dx$$
> 

- Units
	- Measured in <mark style="background: #FF5582A6;">nats</mark> (when using $\log_e$ or $\ln$), for continuous random variables, same as [[202602061530 - self-information|self-information]]
- Unlike discrete entropy, <mark style="background: #ADCCFFA6;">differential entropy can be negative</mark>
	- If a distribution is extremely "peaked" (very certain), the value drops below zero

1. Non-negativity
	- Entropy is always greater than or equal to zero

> [!MATH] Non-negativity
> $$H(X) \ge 0$$
> - $H(X) = 0$ only if the outcome is certain ($P(x)=1$)

2. Maximum Entropy
	- For a discrete variable with $n$ outcomes, entropy is maximised when the distribution is **Uniform** (all outcomes equally likely)

> [!MATH] Maximum Entropy
> $$H(X) \le \log_2(n)$$

3. Additivity
	- For two **independent** random variables $X$ and $Y$, the entropy of the pair is the sum of their individual entropies

> [!MATH] Mathematical Properties
> $$H(X, Y) = H(X) + H(Y)$$


4. Conditioning Reduces Entropy
	- On average, knowing extra information ($Y$) can only decrease (or keep the same) the uncertainty of $X$

> [!MATH] Conditioning Reduces Entropy
> $$H(X | Y) \le H(X)$$

5. Symmetry 
	- The order of the outcomes doesn't change the entropy

> [!MATH] Symmetry
> $$H(p_1, p_2, \dots, p_n) = H(p_{\text{permuted}})$$


[^1]: [Entropy (for data science) Clearly Explained!!!](https://www.youtube.com/watch?v=YtebGVx-Fxw&t=1s)

