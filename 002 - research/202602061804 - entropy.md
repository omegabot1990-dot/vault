---
tags:
  - math
aliases:
  - Entropy
title: entropy
description:
parent nodes:
  - "[[202602061530 - self-information|Self-Information]]"
annotation-target:
published on:
---

- Entropy (denoted as $H$) is the mathematical <mark style="background: #BBFABBA6;">measure of uncertainty or unpredictability</mark> in a set of data
- While [[202602061530 - self-information|self-information]] measures the surprise of a <mark style="background: #FF5582A6;">single event</mark>, Entropy measures the average uncertainty of the <mark style="background: #ADCCFFA6;">entire system</mark>
- It is the expected value of the information content across all possible outcomes:

> [!MATH] Entropy $H(X)$
> $$H(X) = E[I(x)] = -\sum_{i=1}^{n} P(X=x_i) \log_2 P(X=x_i)$$
> 
> - **$P(X=x_i)$**: The probability of outcome $i$
> - **$\log P(X=x_i)$**: The surprise/information of that outcome

- Units
	- Measured in <mark style="background: #FF5582A6;">bits</mark> (when using $\log_2$), for discrete random variables, same as [[202602061530 - self-information|self-information]]

- For a continuous random variable $X$ with a probability density function (PDF) $f(x)$:

> [!MATH] Differential Entropy $h(X)$
> 
> $$h(X) = -\int_{-\infty}^{\infty} f(x) \ln f(x) \, dx$$
> 

- Units
	- Measured in <mark style="background: #FF5582A6;">nats</mark> (when using $\log_e$ or $\ln$), for continuous random variables, same as [[202602061530 - self-information|self-information]]
- Unlike discrete entropy, <mark style="background: #ADCCFFA6;">differential entropy can be negative</mark>
	- If a distribution is extremely "peaked" (very certain), the value drops below zero


[^1]: [Entropy (for data science) Clearly Explained!!!](https://www.youtube.com/watch?v=YtebGVx-Fxw&t=1s)

