---
tags:
  - moc
  - transformers
description: ""
parent nodes:
  - "[[202602061153 - neural network|Neural Network]]"
---

## Topics

<<<<<<< HEAD
=======
- [x] [[202602191531 - attention|Attention]]
- [x] [[202602191619 - multi-head self attention|Multi-Head Self Attention]]
- [x] [[202602191621 - residual connection|Residual Connection]]
- [x] [[202602191623 - layer normalization|Layer Normalization]]
- [x] [[202602191644 - post training|Post Training]]

>>>>>>> db14ed6fea0c654ca337628fccb43d39d77d0c5c
## Blogs

## Papers

- [x] [[202602191533 - attention is all you need|Attention Is All You Need]]
	- [ ] Holy Grail
	- [ ] June 12, 2017
	- [ ] https://www.youtube.com/watch?v=iDulhoQ2pro

## Videos

## Code
