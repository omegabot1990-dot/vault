---
tags:
  - reinforcement_learning
aliases:
  - Model-free
title: model-free
description: ""
bot: false
parent nodes:
  - "[[202602192221 - reinforcement learning|Reinforcement Learning]]"
published on:
---

- [ ] What is experience replay?
- [ ] What is the target network?

---
- Model-free [[202602192221 - reinforcement learning|reinforcement learning]] learns behaviour directly from experience without an explicit dynamics model
- It avoids learning $P(s'\mid s,a)$ and instead estimates [[202602192359 - value estimation|value functions]], [[202602192245 - policy|policies]], or both
- Classical model-free methods include:
	- [[202602201303 - q-learning|Q-learning]]
	- SARSA
	- [[202602200134 - reinforce|REINFORCE]]
	- [[202602201241 - proximal policy optimization|PPO]]-style [[202602201254 - actor-critic methods|actor-critic]] variants
- It is often simpler to deploy, but it may require more [[202602192345 - environment|environment]] interactions than [[202602201253 - model-based|model-based]] methods
- Experience replay, target networks, and [[202602211411 - generalized advantage estimation|advantage estimation]] are common stabilising techniques
- Model-free methods are widely used when accurate environment models are unavailable


[^1]: [Monte Carlo And Off-Policy Methods | Reinforcement Learning Part 3](https://www.youtube.com/watch?v=bpUszPiWM7o&list=PLzvYlJMoZ02Dxtwe-MmH4nOB5jYlMGBjr&index=3)