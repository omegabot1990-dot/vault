---
tags:
  - reinforcement_learning
aliases:
  - Q-learning
title: q-learning
description: ""
bot: false
parent nodes:
  - "[[202602201305 - temporal-difference algorithm|TD learning]]"
published on:
---

- [ ] What is epsilon-greedy?
- [ ] What are replay buffers?
- [ ] What is Bellman optimality?

---
- Q-learning is an [[202602201246 - off-policy|off-policy]] [[202602201305 - temporal-difference algorithm|temporal-difference algorithm]] for learning optimal [[202602192350 - action|action]] values
- It updates estimates toward a Bellman optimality target using sampled transitions
- The canonical update is:

> [!MATH] Q-learning
  $$Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha \left[r_{t+1} + \gamma \max_{a'} Q(s_{t+1},a') - Q(s_t,a_t)\right]$$
>
> | Symbol | Name | Description |
> | :--- | :--- | :--- |
> | $Q(s_t, a_t)$ | **Current Q-Value** | The agent's existing "guess" for the value of taking action $a$ in state $s$ |
> | $\alpha$ | **Learning Rate** | How much the agent "overwrites" old knowledge with new info (0 to 1) |
> | $r_{t+1}$ | **Immediate Reward** | The actual "payoff" received after taking the action |
> | $\gamma$ | **Discount Factor** | How much the agent values future rewards (near 1 = farsighted) |
> | $\max_{a'} Q(s_{t+1}, a')$ | **Max Future Q-Value** | The best possible score the agent thinks it can get from the *next* state |
> | $s_{t+1}$ | **Next State** | The state where the agent landed after performing the action |
  
  
- Action selection is often epsilon-greedy to balance exploration and exploitation
- Q-learning converges in tabular settings under standard assumptions and sufficient exploration
- Deep Q-learning extends the method with [[202602061153 - neural network|neural networks]], replay buffers, and target networks

