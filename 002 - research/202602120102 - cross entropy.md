---
tags:
- math
aliases:
- Cross entropy
- Cross-entropy
title: cross entropy
description: ''
bot: true
parent nodes:
- '[[math]]'
published on: null
---

- Cross entropy measures the expected negative log-probability assigned by a model distribution to true outcomes
- Lower cross entropy means the model assigns higher probability to correct outcomes

> [!MATH] Cross entropy for discrete distributions
> For true distribution $p$ and model distribution $q$ on classes $i$:
> $$H(p,q)=-\sum_i p(i)\log q(i)$$

- One-hot label case
  - If the true class is $k$, then $H(p,q)=-\log q(k)$

> [!MATH] Binary cross entropy
> For target $y\in\{0,1\}$ and predicted probability $\hat y\in(0,1)$:
> $$\ell(y,\hat y)= -\big(y\log \hat y + (1-y)\log(1-\hat y)\big)$$

- Cross entropy is commonly used with softmax (multiclass) and sigmoid (binary)
- Minimizing cross entropy is equivalent to maximizing log-likelihood under the model