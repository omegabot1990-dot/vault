---
tags:
  - math
aliases:
  - Cross entropy
  - Cross-entropy
title: cross-entropy
description: ""
bot: true
parent nodes:
  - "[[202602061804 - entropy|Entropy]]"
published on:
---

- Cross entropy measures the expected negative log-probability assigned by a model distribution to true outcomes
- Lower cross-entropy means the model assigns a higher probability to correct outcomes
- For true distribution $p$ and model distribution $q$ on classes $i$:

> [!MATH] Cross-Entropy for Discrete Distributions
> $$H(p,q)=-\sum_i p(i)\log q(i)$$

- One-hot label case
	- If the true class is $k$, then $H(p,q)=-\log q(k)$
- For target $y\in\{0,1\}$ and predicted probability $\hat y\in(0,1)$:

> [!MATH] Binary Cross Entropy
> $$\ell(y,\hat y)= -\big(y\log \hat y + (1-y)\log(1-\hat y)\big)$$

- Cross entropy is commonly used with softmax (multi-class) and sigmoid (binary)
- Minimising cross entropy is equivalent to maximising log-likelihood under the model