---
tags:
  - math
aliases:
  - Cross-Entropy
  - Cross-entropy
title: cross-entropy
description: ""
bot: false
parent nodes:
  - "[[202602061804 - entropy|Entropy]]"
published on:
---

- Cross entropy measures the expected negative log-[[202602062040 - probability|probability]] assigned by a model [[202602072325 - distribution|distribution]] to true outcomes
- Lower cross-entropy means the model assigns a higher probability to correct outcomes
- For true distribution $p$ and model distribution $q$ on classes $i$:

> [!MATH] Cross-Entropy for Discrete Distributions
> $$H(p,q)=-\sum_i p(i)\log q(i)$$

- One-hot label case
	- If the true class is $k$, then $H(p,q)=-\log q(k)$
- For target $y\in\{0,1\}$ and predicted probability $\hat y\in(0,1)$:

> [!MATH] Binary Cross Entropy
> $$\ell(y,\hat y)= -\big(y\log \hat y + (1-y)\log(1-\hat y)\big)$$

- Cross entropy is commonly used with [[202602111530 - softmax|softmax]] (multi-class) and [[202602111510 - sigmoid|sigmoid]] (binary)
- Minimising cross entropy is equivalent to maximising [[202602061424 - log-likelihood|log-likelihood]] under the model


[^1]: [A Short Introduction to Entropy, Cross-Entropy and KL-Divergence](https://www.youtube.com/watch?v=ErfnhcEV1O8)
[^2]: [Neural Networks Part 6: Cross Entropy](https://www.youtube.com/watch?v=6ArSys5qHAU)