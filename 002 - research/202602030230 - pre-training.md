---
tags:
- note
aliases:
- Pre-training
title: pre-training
description: Concise concept note on pre-training.
parent nodes:
- '[[pre-training]]'
published on: null
---

- [ ] How does it work in encoder and encoder-decoder models?
- [ ] What is AdamW?

---
- Pre-training is the initial phase of development, where the randomly initialised model is trained on a massive general dataset
- Helpful to learn foundational patterns in the data
	- It requires significantly less data and time to learn a new, specific task later on
- During this stage, a [[large language models|Large Language Model (LLM)]] learns <mark style="background: #BBFABBA6;">grammar</mark>, <mark style="background: #BBFABBA6;">facts about the world</mark>, and <mark style="background: #BBFABBA6;">reasoning skills</mark>
- Done via [[202602050153 - self-supervised learning|self-supervised learning]]
- In [[202602050141 - autoregressive|autoregressive]] models like [[decoder transformers]], the learning objective is [[202602050155 - causal language modelling|next-token prediction]], and the loss function used is [[202602030238 - cross-entropy loss|cross-entropy loss]]
	- $L = -\sum_{i=1}^{V} y_i \log(\hat{y}_i)$
- The standard [[optimizer|optimizer]] used for training Large Language Model (LLM) is AdamW (February, 2026)


