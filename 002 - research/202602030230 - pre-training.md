---
tags:
  - training
aliases:
  - Pre-training
title: pre-training
description: ""
parent nodes:
  - "[[pre-training]]"
published on:
---

- [ ] How does it work in encoder models?
- [ ] How does it work in encoder-decoder models?
- [ ] What is AdamW?

---
- Pre-training is the initial phase of development, where the randomly initialised [[202602010044 - model|model]] is [[202602111335 - training|trained]] on a massive general dataset
- Helpful to learn foundational patterns in the data
	- It requires significantly less data and time to learn a new, specific task later on
- During this stage, a [[large language models|Large Language Model (LLM)]] learns <mark style="background: #BBFABBA6;">grammar</mark>, <mark style="background: #BBFABBA6;">facts about the world</mark>, and <mark style="background: #BBFABBA6;">reasoning skills</mark>
- Done via [[202602050153 - self-supervised learning|self-supervised learning]]
- In [[202602050141 - autoregressive|autoregressive]] models like [[decoder transformers]], the learning objective is [[202602050155 - causal language modelling|next-token prediction]], and the loss function used is [[202602030238 - cross-entropy loss|cross-entropy loss]]

> [!MATH] Pre-Training Loss
> $$L = -\sum_{i=1}^{V} y_i \log(\hat{y}_i)$$

- The standard [[202602111605 - optimization algorithms|optimizer]] used for training Large Language Model (LLM) is AdamW (February, 2026)


