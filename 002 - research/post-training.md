---
tags:
- moc
description: ''
parent nodes:
- '[[training]]'
---

## Topics

	- [x] SFT
	- [x] RLHF
	- [x] PPO
	- [x] March 2022
	- [ ] <mark style="background: #FF5582A6;">IMPORTANT</mark>

## Blogs


## Papers

- [x] [Training language models to follow instructions with human feedback](https://www.alphaxiv.org/abs/2203.02155)
- [ ] [A Comprehensive Survey of LLM Alignment Techniques: RLHF, RLAIF, PPO, DPO and More](https://www.alphaxiv.org/overview/2407.16216)
- [ ] [LLM Post-Training: A Deep Dive into Reasoning Large Language Models](https://www.alphaxiv.org/abs/2502.21321)
- [ ] [RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System](https://www.alphaxiv.org/abs/2602.02488)

## Videos
