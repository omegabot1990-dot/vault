---
tags:
  - moc
description:
parent nodes:
  - "[[transformers]]"
---

- [ ] [Understanding and Coding Self-Attention, Multi-Head Attention, Causal-Attention, and Cross-Attention in LLMs](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention)
	- [ ] <mark style="background: #FFB86CA6;">IMPORTANT</mark>
- [ ] Grouped-query attention
- [ ] Sliding-window attention
- [ ] Multi-head latent attention
- [ ] Gated DeltaNet
- [ ] Mamba-2
- [ ] [Attention, please!](https://paulinamoskwa.github.io/blog/2025-11-06/attn)
	- [ ] <mark style="background: #FFB86CA6;">IMPORTANT</mark>
- [ ] [Understanding Multi-Head Latent Attention](https://planetbanatt.net/articles/mla.html)
	- [ ] <mark style="background: #FFB86CA6;">IMPORTANT</mark>
- [ ] [Kimi Linear: An Expressive, Efficient Attention Architecture](https://www.alphaxiv.org/overview/2510.26692)
- [ ] [Sequential Attention for Feature Selection](https://www.alphaxiv.org/abs/2209.14881)
	- [ ] <mark style="background: #ADCCFFA6;">LATER</mark>