---
tags:
- moc
description: ''
parent nodes:
- '[[transformers]]'
---

## Topics

	- [ ] <mark style="background: #FFB86CA6;">IMPORTANT</mark>
- [ ] Grouped-query attention
- [ ] Sliding-window attention
- [ ] Multi-head latent attention
- [ ] Gated DeltaNet
- [ ] Mamba-2
	- [ ] <mark style="background: #ADCCFFA6;">LATER</mark>

## Blogs

- [ ] [Understanding and Coding Self-Attention, Multi-Head Attention, Causal-Attention, and Cross-Attention in LLMs](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention)
- [ ] [Attention, please!](https://paulinamoskwa.github.io/blog/2025-11-06/attn)
- [ ] [Understanding Multi-Head Latent Attention](https://planetbanatt.net/articles/mla.html)

## Papers

- [ ] [Kimi Linear: An Expressive, Efficient Attention Architecture](https://www.alphaxiv.org/overview/2510.26692)
- [ ] [Sequential Attention for Feature Selection](https://www.alphaxiv.org/abs/2209.14881)

## Videos
