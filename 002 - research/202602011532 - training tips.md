---
tags:
  - research
aliases:
  - Training tips
title: training tips
description:
parent nodes:
  - "[[training]]"
annotation-target:
published on:
---

- [ ] What is catastrophic forgetting?

---
> [YouTube - 8 Timeless tips for training LLMs | Become a better ML engineer](https://www.youtube.com/watch?v=iCwvGys_iM4)

1. Start from the ==evaluation== step
	1. How do we measure progress?
	2. Have a great evaluation set, not just an okay one
	3. Delegate if possible
		1. Use a pre-existing and validated benchmark 
2. Never start from scratch
	1. Start from a Colab notebook or GitHub repo from a reputable source
	2. Don't reinvent the wheel (hyperparameters)
3. Move away from notebooks as soon as possible
4. Understand the training loss
	1. Don't let it get too close to zero, as it most likely causes overfitting
		1. Causes catastrophic forgetting
5. Use a validation set
6. Make one change at a time
7. Check boundary conditions
	1. Is it a deterministic bug or a hyperparameter issue?

