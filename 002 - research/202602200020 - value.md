---
tags:
  - reinforcement_learning
aliases:
  - Value
title: value
description: ""
bot: false
parent nodes:
  - "[[202602192221 - reinforcement learning|RL]]"
published on:
---

- Value denotes the [[202602061333 - expectation|expected]] [[202602192355 - return|return]] associated with [[202602192351 - state|states]] or state-[[202602192350 - action|action]] pairs
- <mark style="background: #BBFABBA6;">State value</mark> $V^\pi(s)$ is the expected return when starting at the state $s$ and following the [[202602192245 - policy|policy]] $\pi$

> [!MATH] State Value $V^\pi(s)$
> $$V^\pi(s) = E_\pi [G_t | s_t = s]$$

- <mark style="background: #FF5582A6;">Action value</mark> $Q^\pi(s,a)$ is the expected return after taking action $a$ in state $s$ then following $\pi$

> [!MATH] Action Value $Q^\pi(s,a)$
> $$Q^\pi(s, a) = E_\pi [G_t | s_t = s, a_t = a]$$

- The goal in [[202602192221 - reinforcement learning|RL]] is that we want to find the <mark style="background: #FFF3A3A6;">Optimal Value Function</mark> $V^*(s)$, which is the maximum possible value achievable from state $s$ by any policy
- <mark style="background: #ABF7F7A6;">V vs. Q</mark>: 
    - $V(s)$ is the value of the <mark style="background: #D2B3FFA6;">State</mark> alone
    - $Q(s, a)$ is the value of taking a specific <mark style="background: #BBFABBA6;">Action</mark> in that state
    - Relationship
        - $V(s) = \max_a Q(s, a)$ (if following an optimal policy)
- Value functions guide policy improvement by scoring long-term consequences of decisions
- [[202602211420 - bellman equation|Bellman equations]] relate values recursively through immediate [[202602192352 - reward|rewards]] and next-state values
- Value approximation is central in [[202602211440 - dynamic programming|dynamic programming]], [[202602201305 - temporal-difference algorithm|temporal-difference learning]], and [[202602201254 - actor-critic methods|actor-critic methods]] 


