---
tags:
  - reinforcement_learning
aliases:
  - Value
title: value
description: ""
bot: false
parent nodes:
  - "[[202602192221 - reinforcement learning|RL]]"
published on:
---

- [ ] What are Bellman equations?
- [ ] What is dynamic programming?
- [ ] What is temporal-difference learning?
- [ ] What is the actor-critic method?

---
- Value denotes the [[202602061333 - expectation|expected]] [[202602192355 - return|return]] associated with [[202602192351 - state|states]] or state-[[202602192350 - action|action]] pairs
- State value $V^\pi(s)$ is the expected return when starting at the state $s$ and following the [[202602192245 - policy|policy]] $\pi$
- Action value $Q^\pi(s,a)$ is the expected return after taking action $a$ in state $s$ then following $\pi$
- Value functions guide policy improvement by scoring long-term consequences of decisions
- Bellman equations relate values recursively through immediate [[202602192352 - reward|rewards]] and next-state values
- Value approximation is central in dynamic programming, temporal-difference learning, and actor-critic methods