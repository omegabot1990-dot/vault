---
tags:
  - reinforcement_learning
aliases:
  - Value
title: value
description: ""
bot: false
parent nodes:
  - "[[202602192221 - reinforcement learning|RL]]"
published on:
---

- [ ] What are Bellman equations?
- [ ] What is dynamic programming?

---
- Value denotes the [[202602061333 - expectation|expected]] [[202602192355 - return|return]] associated with [[202602192351 - state|states]] or state-[[202602192350 - action|action]] pairs
- <mark style="background: #BBFABBA6;">State value</mark> $V^\pi(s)$ is the expected return when starting at the state $s$ and following the [[202602192245 - policy|policy]] $\pi$

> [!MATH] State Value $V^\pi(s)$
> $$V^\pi(s) = E_\pi [G_t | s_t = s]$$

- <mark style="background: #FF5582A6;">Action value</mark> $Q^\pi(s,a)$ is the expected return after taking action $a$ in state $s$ then following $\pi$
- Value functions guide policy improvement by scoring long-term consequences of decisions
- Bellman equations relate values recursively through immediate [[202602192352 - reward|rewards]] and next-state values
- Value approximation is central in dynamic programming, [[202602201305 - temporal-difference algorithm|temporal-difference learning]], and [[202602201254 - actor-critic methods|actor-critic methods]] 

### State-Value Function $V^\pi(s)$
The value of a state $s$ under a policy $\pi$ is the **Expected Return** when starting in $s$ and following $\pi$ thereafter.



---

> [!MATH] ð‘“(ð‘¥) The Bellman Equation for $V(s)$
> We can decompose the value into the immediate reward plus the discounted value of the next state:
> $$V^\pi(s) = \sum_{a} \pi(a|s) \sum_{s', r} p(s', r | s, a) \left[ r + \gamma V^\pi(s') \right]$$
> 
> | Symbol | Name | Description |
> | :--- | :--- | :--- |
> | $V(s)$ | **State-Value** | The "score" or desirability of being in state $s$. |
> | $G_t$ | **Return** | The sum of all future discounted rewards. |
> | $\gamma$ | **Discount Factor** | Determines how much we care about future vs. immediate rewards. |
> | $\pi(a|s)$ | **Policy** | The probability of taking action $a$ in state $s$. |

---

### [!ABSTRACT] ðŸ’¡ Value vs. Reward
- **Reward ($r$)**: Given by the environment immediately after an action (e.g., "I found a coin!").
- **Value ($V$)**: The total future rewards you expect from that point on (e.g., "I'm in a room full of coins, so my current state has high value").

---

### Key Properties
- **The Goal**: In RL, we want to find the **Optimal Value Function** $V^*(s)$, which is the maximum possible value achievable from state $s$ by any policy.
- **V vs. Q**: 
    - $V(s)$ is the value of the **State** alone.
    - $Q(s, a)$ is the value of taking a specific **Action** in that state.
- **Relationship**: $V(s) = \max_a Q(s, a)$ (if following an optimal policy).
