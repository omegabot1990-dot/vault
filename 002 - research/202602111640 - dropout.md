---
tags:
  - deep_learning
aliases:
  - Dropout
title: dropout
description: ""
bot: true
parent nodes:
  - "[[202602111256 - hyperparameters|Hyperparameters]]"
published on:
---

- Dropout is a [[202602111636 - regularization|regularization]] method that randomly zeros a subset of activations during [[202602111335 - training|training]]
- This prevents units from relying too strongly on specific co-adapted features
- A dropout rate $p$ means each activation is dropped with [[202602062040 - probability|probability]] $p$ at training time
- During [[202602111350 - inference|inference]], dropout is disabled, and activations are used deterministically
- In inverted dropout, surviving activations are scaled by $1/(1-p)$ during training so expected activation stays consistent
- Dropout can improve generalization, especially when models are large relative to dataset size