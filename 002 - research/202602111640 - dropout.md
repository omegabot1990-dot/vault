---
tags:
  - deep_learning
aliases:
  - Dropout
title: dropout
description: ""
bot: false
parent nodes:
  - "[[202602111256 - hyperparameters|Hyperparameters]]"
published on:
---

- Dropout is a [[202602111636 - regularization|regularization]] method that <mark style="background: #BBFABBA6;">randomly zeros a subset of activations</mark> during [[202602111335 - training|training]]
- This prevents units from relying too strongly on specific co-adapted features
- A dropout rate $p$ means each activation is dropped with [[202602062040 - probability|probability]] $p$ at training time
- During [[202602111350 - inference|inference]], dropout is disabled, and activations are used deterministically
- In inverted dropout, surviving activations are scaled by $1/(1-p)$ during training, so expected activation stays consistent
- <mark style="background: #FF5582A6;">Dropout can improve generalisation</mark>, especially when models are large relative to the dataset size