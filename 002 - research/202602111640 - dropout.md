---
tags:
- note
aliases:
- Dropout
title: dropout
description: ''
bot: true
parent nodes:
- '[[training]]'
published on: null
---

- Dropout is a regularization method that randomly zeros a subset of activations during training
- This prevents units from relying too strongly on specific co-adapted features
- A dropout rate $p$ means each activation is dropped with probability $p$ at training time
- During inference, dropout is disabled and activations are used deterministically
- In inverted dropout, surviving activations are scaled by $1/(1-p)$ during training so expected activation stays consistent
- Dropout can improve generalization, especially when models are large relative to dataset size