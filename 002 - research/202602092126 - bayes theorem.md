---
tags:
  - math
aliases:
  - Bayes' Theorem
title: bayes theorem
description: ""
parent nodes:
  - "[[202602062040 - probability|Probability]]"
published on:
---

- [ ] [Naive Bayes' Classifier](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/naive-bayes.html)

---
- Bayes' Theorem is a mathematical formula used to determine the [[202602062040 - probability|probability]] of an [[202602092352 - event|event]] based on prior knowledge of [[202602080132 - conditional probability|conditions]] that might be related to the event [^6][^7][^8][^9]
- In simple terms, it is a <mark style="background: #BBFABBA6;">way to update your beliefs when you see new evidence</mark> [^3]
	- <mark style="background: #FFF3A3A6;">Posterior equals prior times likelihood, divided by the evidence</mark> [^4]

> [!MATH] Bayes' Theorem
> $$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$
> $$P(H|E) = \frac{P(E|H) \cdot P(H)}{P(E)}$$

1. <mark style="background: #BBFABBA6;">Posterior</mark> $P(A|B)$ 
	1. What do we want to know? 
	2. Probability that it's a dog ($H$), given we heard a bark ($E$)
2. <mark style="background: #FF5582A6;">Likelihood</mark> $P(B|A)$
	1. How well does the evidence fit the theory?
	2. A [[202602061241 - likelihood|likelihood]] function says how likely one is to observe any value of the collected evidence for each of the hypotheses in the class $P(E|H)$[^2]
	3. How likely is a bark ($E$) if it IS a dog ($H$)?
3. <mark style="background: #FFB86CA6;">Prior</mark> $P(A)$ 
	1. What did we know before?
	2. An observer possesses some (subjective) prior beliefs about the plausibility of the available <mark style="background: #ADCCFFA6;">hypotheses</mark> encoded in the _prior_ $P(H)$ [^1]
	3. How common are dogs in this building?
4. <mark style="background: #FFF3A3A6;">Evidence</mark> $P(B)$
	1. The total probability under any condition
	2. Probability of a bark happening

- Calculating the Evidence $P(B)$
	- To find $P(B)$, we [[202602092150 - marginalisation|marginalise]] out $A$ from the joint distribution $P(A, B)$: [^5]
5. Marginalisation

> [!MATH] The Marginalisation Step
> $$P(B) = \sum_{n} P(B \cap A_n)$$

6. Substitution, Since $P(B \cap A) = P(B|A)P(A)$, we get:

> [!MATH] The Conditional Step
> $$P(B) = \sum_{n} P(B | A_n)P(A_n)$$
> 

- Meaning
	- The total probability of the evidence ($B$) is the weighted average of how likely $B$ is under every possible hypothesis $A$


[^1]: https://d2l.ai/chapter_preliminaries/probability.html#a-more-formal-treatment:~:text=In%20Bayesian%20statistics%2C%20we,%2C
[^2]: https://d2l.ai/chapter_preliminaries/probability.html#a-more-formal-treatment:~:text=a%20likelihood%20function,.
[^3]: https://d2l.ai/chapter_preliminaries/probability.html#a-more-formal-treatment:~:text=Bayes%E2%80%99%20theorem%20is,produce%20posterior%20beliefs
[^4]: https://d2l.ai/chapter_preliminaries/probability.html#a-more-formal-treatment:~:text=posterior%20equals%20prior%20times%20likelihood%2C%20divided%20by%20the%20evidence
[^5]: https://d2l.ai/chapter_preliminaries/probability.html#a-more-formal-treatment:~:text=over%20the%20hypotheses.-,Note%20that,(2.6.6),-%E2%88%91
[^6]: [Bayes' Theorem, Clearly Explained!!!!](https://www.youtube.com/watch?v=9wCnvr7Xw4E&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=20)
[^7]: [3Brown1Blue - Bayes theorem, the geometry of changing beliefs](https://www.youtube.com/watch?v=HZGCoVF3YvM&list=PLiAulSm0XXgvCGe63mrAkda9UQ9478YQv&index=1&t=6s)
[^8]: [3Brown1Blue - The quick proof of Bayes' theorem](https://www.youtube.com/watch?v=U_85TaXbeIo&list=PLiAulSm0XXgvCGe63mrAkda9UQ9478YQv&index=2)
[^9]: [3Brown1Blue - The medical test paradox, and redesigning Bayes' rule](https://www.youtube.com/watch?v=lG4VkPoG3ko&list=PLiAulSm0XXgvCGe63mrAkda9UQ9478YQv&index=3)