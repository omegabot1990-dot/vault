---
tags:
  - transformers
aliases:
  - Distillation
  - Knowledge Distillation
title: distillation
description: ""
bot: false
parent nodes:
  - "[[202602191524 - large language model|LLM]]"
published on:
---

- Distillation transfers behaviour from a stronger teacher model to a smaller student model
- <mark style="background: #BBFABBA6;">The student is trained to match teacher outputs, logits, or preference-ranked responses on curated data</mark>
- In [[202602191524 - large language model|LLMs]], distillation improves latency and cost while preserving much of the [[202602231757 - reasoning|reasoning]] and instruction-following quality
- It can be supervised from teacher generations, RL-improved traces, or synthetic datasets
- Performance depends on teacher quality, data diversity, and objective design
- Distillation often outperforms [[202602111335 - training|training]] small models from scratch at the same compute budget


[^1]: [Knowledge Distillation: How LLMs train each other](https://www.youtube.com/watch?v=jrJKRYAdh7I)