---
tags:
- note
aliases:
- Distillation
- Knowledge Distillation
title: distillation
description: ''
bot: true
parent nodes:
- '[[large language models]]'
published on: null
---

- Distillation transfers behavior from a stronger teacher model to a smaller student model
- The student is trained to match teacher outputs, logits, or preference-ranked responses on curated data
- In LLMs, distillation improves latency and cost while preserving much of reasoning and instruction-following quality
- It can be supervised from teacher generations, RL-improved traces, or synthetic datasets
- Performance depends on teacher quality, data diversity, and objective design
- Distillation often outperforms training small models from scratch at the same compute budget