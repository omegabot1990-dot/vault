---
tags:
- note
aliases:
- Weight Decay
title: weight decay
description: ''
bot: true
parent nodes:
- '[[hyperparameters]]'
published on: null
---

- Weight decay is a regularization method that penalizes large parameter magnitudes during training
- It is commonly implemented as $L_2$ regularization added to the objective
- This penalty discourages overly complex solutions and can improve generalization
- In update terms, weight decay shrinks parameters toward zero at each step
- The decay coefficient is a hyperparameter that controls regularization strength
- In AdamW, weight decay is decoupled from gradient-based moment updates