---
tags:
  - deep_learning
aliases:
  - Weight Decay
title: weight decay
description: ""
bot: false
parent nodes:
  - "[[202602111256 - hyperparameters|Hyperparameters]]"
published on:
---

- Weight decay is a [[202602111636 - regularization|regularization]] method that penalises large [[202602111314 - model parameters|parameter]] magnitudes during [[202602111335 - training|training]]
- It is commonly implemented as [[202602111711 - l2 regularization|L2 regularization]] added to the [[202602010047 - objective function|objective]]
	- Weight decay modifies the original cost function $J_0$ by adding a penalty term proportional to the square of the weights
- The new cost function $J$ is defined as:

> [!MATH] Regularized Objective Function
> $$J(\mathbf{w}) = J_0(\mathbf{w}) + \frac{\lambda}{2} \|\mathbf{w}\|^2$$
> Where:
> - $J_0$: The original loss 
> - $\lambda$: The <mark style="background: #ABF7F7A6;">weight decay hyperparameter</mark> (regularization strength)
> - $\|\mathbf{w}\|^2$: The squared $L_2$ norm of the weight vector, $\sum w_i^2$

- This penalty discourages overly complex solutions and can improve generalisation
- In update terms, weight decay shrinks parameters toward zero at each step
- When we take the derivative for [[202602111558 - gradient descent|Gradient Descent]], the update rule becomes:

> [!MATH] The Gradient Update
> $$\mathbf{w}_{t+1} = \mathbf{w}_t - \eta \left( \nabla J_0(\mathbf{w}_t) + \lambda \mathbf{w}_t \right)$$
Which can be rewritten to show why it is called "decay":
$$\mathbf{w}_{t+1} = (1 - \eta\lambda) \mathbf{w}_t - \eta \nabla J_0(\mathbf{w}_t)$$
> Where, $(1 - \eta\lambda)$ is the decay factor

- In every step, the weight is first shrunk by a small fraction before the standard gradient update is applied
- The decay coefficient is a [[202602111256 - hyperparameters|hyperparameter]] that controls the regularization strength
