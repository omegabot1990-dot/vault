---
tags:
  - deep_learning
aliases:
  - Weight Decay
title: weight decay
description: ""
bot: true
parent nodes:
  - "[[202602111256 - hyperparameters|Hyperparameters]]"
published on:
---

- Weight decay is a regularisation method that penalises large [[202602111314 - model parameters|parameter]] magnitudes during [[202602111335 - training|training]]
- It is commonly implemented as $L_2$ regularization added to the objective
- This penalty discourages overly complex solutions and can improve generalization
- In update terms, weight decay shrinks parameters toward zero at each step
- The decay coefficient is a hyperparameter that controls regularization strength
- In AdamW, weight decay is decoupled from gradient-based moment updates