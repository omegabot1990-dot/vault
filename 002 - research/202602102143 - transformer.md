---
tags:
  - transformers
aliases:
  - Transformer
title: transformer
description: ""
bot: false
parent nodes:
  - "[[202602061153 - neural network|Neural Network]]"
published on:
---

<<<<<<< HEAD
- [ ] What is token embedding?
- [ ] What is positional embedding?
- [ ] What is an encoder-decoder?
- [ ] [StatQuest - Transformer Neural Networks, ChatGPT's foundation, Clearly Explained!!!](https://www.youtube.com/watch?v=zxQyTK8quyY&list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&index=20)
=======
- [ ] What is MHSA?
- [ ] What are residual connections?
- [ ] What is layer normalisation?
>>>>>>> db14ed6fea0c654ca337628fccb43d39d77d0c5c

---
- Transformer is a [[202602061153 - neural network|neural network]] architecture for sequence modelling built around [[202602191531 - attention|attention]]
- Introduced in the paper [[202602191533 - attention is all you need|Attention Is All You Need]]
- Uses self-attention to mix information across positions in parallel
- Common backbone for modern language models

- Typical components
	- Token embeddings plus positional information
	- [[202602191619 - multi-head self attention|Multi-head self-attention]]
	- Position-wise [[202602191537 - feed-forward network|Feedforward Network]]
	- [[202602191621 - residual connection|Residual connections]] and [[202602191623 - layer normalization|layer normalisation]]

- Common variants
	- Encoder-decoder for sequence-to-sequence tasks
	- Decoder only for next-token prediction

- Practical properties
	- Training parallelises well compared to recurrent models
<<<<<<< HEAD
	- <mark style="background: #FF5582A6;">Attention cost grows with sequence length</mark>

![](https://paper-assets.alphaxiv.org/figures/1706.03762v7/ModalNet-21.png)


[^1]: [Transformers, the tech behind LLMs | Deep Learning Chapter 5](https://www.youtube.com/watch?v=wjZofJX0v4M&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
=======
	- Attention cost grows with sequence length

![](https://paper-assets.alphaxiv.org/figures/1706.03762v7/ModalNet-21.png)
>>>>>>> db14ed6fea0c654ca337628fccb43d39d77d0c5c
