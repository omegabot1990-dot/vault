---
tags:
  - transformers
aliases:
  - Transformer
title: transformer
description: ""
bot: true
parent nodes:
  - "[[transformers]]"
published on:
---

- Transformer is a neural network architecture for sequence modelling built around attention
- Uses self-attention to mix information across positions in parallel
- Common backbone for modern language models

- Typical components
	- Token embeddings plus positional information
	- Multi-head self-attention
	- Position-wise feed-forward network
	- Residual connections and layer normalisation

- Common variants
	- Encoder-decoder for sequence-to-sequence tasks
	- Decoder only for next-token prediction

- Practical properties
	- Training parallelises well compared to recurrent models
	- Attention cost grows with sequence length
