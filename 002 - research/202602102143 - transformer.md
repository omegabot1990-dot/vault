---
tags:
  - transformers
aliases:
  - Transformer
title: transformer
description: ""
bot: true
parent nodes:
  - "[[202602061153 - neural network|Neural Network]]"
published on:
---

- [ ] What is MHSA?
- [ ] What are residual connections?
- [ ] What is layer normalisation?

---
- Transformer is a [[202602061153 - neural network|neural network]] architecture for sequence modelling built around [[202602191531 - attention|attention]]
- Introduced in the paper [[202602191533 - attention is all you need|Attention Is All You Need]]
- Uses self-attention to mix information across positions in parallel
- Common backbone for modern language models

- Typical components
	- Token embeddings plus positional information
	- Multi-head self-attention
	- Position-wise feed-forward network
	- Residual connections and layer normalisation

- Common variants
	- Encoder-decoder for sequence-to-sequence tasks
	- Decoder only for next-token prediction

- Practical properties
	- Training parallelises well compared to recurrent models
	- Attention cost grows with sequence length

![](https://paper-assets.alphaxiv.org/figures/1706.03762v7/ModalNet-21.png)