---
tags:
- note
aliases:
- Momentum
title: momentum
description: ''
bot: true
parent nodes:
- '[[optimizer]]'
published on: null
---

- Momentum is an optimization technique that accumulates an exponential moving average of past gradients
- It accelerates updates in consistent directions and damps oscillations in noisy or high-curvature directions
- Momentum is commonly combined with SGD to improve convergence speed and stability
- The momentum coefficient $\beta$ controls how much past velocity is retained
- Typical values are around $0.9$ in deep learning practice
- Momentum helps optimization traverse flat valleys more efficiently than plain SGD