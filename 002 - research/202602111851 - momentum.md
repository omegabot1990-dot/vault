---
tags:
  - deep_learning
aliases:
  - Momentum
title: momentum
description: ""
bot: false
parent nodes:
  - "[[202602111845 - stochastic gradient descent|Stochastic Gradient Descent]]"
published on:
---

- [ ] What is an exponential moving average?

---
- Momentum is an [[202602111605 - optimization algorithms|optimization]] technique that accumulates an exponential moving average of past gradients
- It <mark style="background: #FFF3A3A6;">accelerates updates in consistent directions and damps oscillations in noisy or high-curvature directions</mark>
- Momentum is commonly combined with [[202602111845 - stochastic gradient descent|Stochastic Gradient Descent (SGD)]] to improve convergence speed and stability
	- To accelerate convergence and reduce oscillations, we add a momentum term $\gamma$:

> [!MATH] SGD with Momentum
> 
$$v_t = \gamma v_{t-1} + \eta \nabla_{\theta} J(\theta)$$
$$\theta = \theta - v_t$$
> Where:
> -   $v_t$: The velocity (moving average of gradients)
> - $\gamma$: The momentum hyperparameter (usually set to $0.9$)

- The momentum coefficient $\gamma$ controls how much past velocity is retained
- Typical values are around $0.9$ in deep learning practice
- Momentum helps optimization traverse flat valleys more efficiently than plain SGD