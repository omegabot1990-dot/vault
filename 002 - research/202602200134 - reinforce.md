---
tags:
  - reinforcement_learning
aliases:
  - REINFORCE
  - Monte Carlo Policy Gradient
title: reinforce
description: ""
bot: false
parent nodes:
  - "[[202602200129 - policy gradient methods|Policy Gradient Methods]]"
published on:
---

- REINFORCE is a Monte Carlo [[202602200129 - policy gradient methods|policy-gradient]] algorithm for optimizing [[202512271205 - stochastic|stochastic]] [[202602192245 - policy|policies]]
- It updates policy [[202602111314 - model parameters|parameters]] using sampled trajectory returns without a learned [[202602200020 - value|value]] model
- The goal is to find $\theta$ that maximises the expected [[202602192355 - return|return]]:

> [!MATH] Performance Measure
> $$J(\theta) = E_{\pi_\theta} [G_t]$$

- Since we want to find the <mark style="background: #FF5582A6;">maximum</mark>, we use <mark style="background: #BBFABBA6;">Gradient Ascent</mark>:

> [!MATH] Gradient Ascent
> $$\theta_{t+1} = \theta_t + \alpha \nabla_\theta J(\theta_t)$$


- For each [[202602192356 - episode|episode]], we update the policy parameters $\theta$ using the gradient of the log-probability scaled by the return:

> [!MATH] REINFORCE Update Rule
> $$\Delta\theta = \alpha \cdot G_t \cdot \nabla_\theta \log \pi_\theta(a_t | s_t)$$
> 
> **The Components:**
> 1.  **$\pi_\theta(a_t | s_t)$**: The probability of taking action $a$ in state $s$ given parameters $\theta$
> 2.  **$\log \pi_\theta$**: The "Log-Trick" (makes the math easier and keeps gradients stable)
> 3.  **$G_t$**: The total accumulated return (reward) from time $t$ until the end of the episode
> 4.  **$\nabla_\theta$**: The gradient (the direction we move $\theta$ to increase the probability)

- We can subtract any baseline $b(s)$ that doesn't depend on the action $a$:

> [!MATH] Advantage-like Update
> $$\nabla_\theta J(\theta) = E [ (G_t - b(s_t)) \nabla_\theta \log \pi_\theta(a_t | s_t) ]$$

- State-Value Baseline
	- Usually, we use $b(s) = V(s)$, the "expected reward for this state"
- Intuition
	- If $(G_t - V(s))$ is positive, the action was "better than average"
	- if negative, it was "worse than average"

- The core estimator multiplies log-policy gradients by return, giving an unbiased gradient estimate
- [[202602062206 - variance|Variance]] can be high, so baselines are often used to stabilise updates
- REINFORCE is conceptually simple and foundational for modern policy-gradient and actor-critic methods
- It is typically less sample-efficient than bootstrapped actor-critic approaches