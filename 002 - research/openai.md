---
tags:
- moc
description: MOC linking key concepts and zettels for openai.
parent nodes:
- '[[large language models]]'
---

- [ ] [Language Models are Unsupervised Multitask Learners](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)
	- [ ] February 14, 2019
	- [ ] GPT2
- [ ] [Language Models are Few-Shot Learners](https://www.alphaxiv.org/abs/2005.14165)
	- [ ] July 22, 2020
	- [ ] GPT3
- [ ] [From GPT-2 to gpt-oss: Analyzing the Architectural Advances](https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the)
- [ ] [O1 Replication Journey: A Strategic Progress Report -- Part 1](https://www.alphaxiv.org/abs/2410.18982)
	- [ ] By exposing the model to incorrect reasoning paths and their corrections, journey learning may also reinforce self-correction abilities, potentially making reasoning models more reliable this way
	- [ ] Shortcut learning refers to the traditional approach in instruction fine-tuning, where models are trained using only correct solution paths
	- [ ] Journey learning, on the other hand, also includes incorrect solution paths, allowing the model to learn from mistakes
	- [ ] The paper has a lot of background work done, and can be used as a reference for:
		- [ ] PRM
		- [ ] Inference Scaling
		- [ ] CoT Theory
		- [ ] Internal Thought
		- [ ] Self-improvement
- [ ] [GPTOSS: From-Scratch Implementation](https://www.projektjoe.com/blog/gptoss)
	- [ ] Without Pytorch