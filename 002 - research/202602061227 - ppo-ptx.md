---
tags:
  - reinforcement_learning
aliases:
  - PPO-ptx
title: ppo-ptx
description: ""
parent nodes:
  - "[[202602201241 - proximal policy optimization|Proximal Policy Optimization]]"
published on:
---

- The core mechanism of PPO-ptx involves combining standard [[202602201241 - proximal policy optimization|Proximal Policy Optimization (PPO)]] updates with updates that increase the [[202602061424 - log-likelihood|log-likelihood]] of the original [[202602030230 - pre-training|pre-training]] [[202602072325 - distribution|distribution]]
- This is achieved by adding a term to the [[202602010047 - objective function|objective function]] that calculates the [[202602061443 - expected log-probability|expected log-probability]] of the [[202602010044 - model|model's]] outputs over its original [[202602111335 - training|training]] data
- Mathematically, the combined objective function for PPO-ptx includes the [[202602192352 - reward|reward]] from the [[202602201334 - reward model|Reward Model (RM)]], a [[202602181827 - kl divergence|KL]] penalty to prevent the model from drifting too far from the initial [[202601282201 - supervised fine-tuning|supervised fine-tuned (SFT)]] model, and a pre-training loss term
- The strength of these components is controlled by coefficients: $\beta$ for the KL penalty and $\gamma$ for the pre-training gradients