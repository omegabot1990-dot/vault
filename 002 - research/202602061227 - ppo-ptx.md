---
tags:
- note
aliases:
- PPO-ptx
title: ppo-ptx
description: ''
parent nodes:
- '[[proximal policy optimisation]]'
published on: null
---

- The core mechanism of PPO-ptx involves combining standard PPO updates with updates that increase the [[202602061424 - log-likelihood|log-likelihood]] of the original pre-training distribution
	- This is achieved by adding a term to the [[202602010047 - objective function|objective function]] that calculates the [[202602061443 - expected log-probability|expected log-probability]] of the model's outputs over its original training data
- Mathematically, the combined objective function for PPO-ptx includes the reward from the Reward Model (RM), a KL penalty to prevent the model from drifting too far from the initial supervised fine-tuned (SFT) model, and a pre-training loss term
- The strength of these components is controlled by coefficients: $\beta$ for the KL penalty and $\gamma$ for the pre-training gradients