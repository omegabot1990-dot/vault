---
tags:
  - math
aliases:
  - Softmax
title: softmax
description: ""
bot: false
parent nodes:
  - "[[202602111400 - activation|Activation]]"
published on:
---

- [ ] What is multi-class classification?
- [ ] What is cross-entropy?

---
- Softmax maps a real-valued score vector to a [[202602072325 - distribution|probability distribution]] over classes
- It makes each output positive and normalises outputs to sum to 1
- For logits $z\in\mathbb{R}^K$, the $i^{th}$ component is:

> [!MATH] Softmax function
> $$\operatorname{softmax}(z)_i=\frac{e^{z_i}}{\sum_{j=1}^{K}e^{z_j}}$$

- Properties
    - $\operatorname{softmax}(z)_i\in(0,1)$ for each $i$
    - $\sum_{i=1}^{K}\operatorname{softmax}(z)_i=1$
    - Adding the same constant to all logits does not change the result
- Use shifted logits to avoid overflow:

> [!MATH] Numerically stable softmax
> $$\operatorname{softmax}(z)_i=\frac{e^{z_i-m}}{\sum_{j=1}^{K}e^{z_j-m}},\quad m=\max_j z_j$$

- Softmax is commonly used with cross-entropy in multi-class classification