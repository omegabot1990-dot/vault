---
tags:
- math
aliases:
- Softmax
title: softmax
description: ''
bot: true
parent nodes:
- '[[math]]'
published on: null
---

- Softmax maps a real-valued score vector to a probability distribution over classes
- It makes each output positive and normalizes outputs to sum to 1

> [!MATH] Softmax function
> For logits $z\in\mathbb{R}^K$, the $i$th component is
> $$\operatorname{softmax}(z)_i=\frac{e^{z_i}}{\sum_{j=1}^{K}e^{z_j}}$$

- Properties
  - $\operatorname{softmax}(z)_i\in(0,1)$ for each $i$
  - $\sum_{i=1}^{K}\operatorname{softmax}(z)_i=1$
  - Adding the same constant to all logits does not change the result

> [!MATH] Numerically stable softmax
> Use shifted logits to avoid overflow
> $$\operatorname{softmax}(z)_i=\frac{e^{z_i-m}}{\sum_{j=1}^{K}e^{z_j-m}},\quad m=\max_j z_j$$

- Softmax is commonly used with cross-entropy in multiclass classification