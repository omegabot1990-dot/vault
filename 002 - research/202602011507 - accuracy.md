---
tags:
  - deep_learning
  - evaluation
aliases:
  - Accuracy
title: accuracy
description: ""
parent nodes:
  - "[[evaluation]]"
published on:
---

- The <mark style="background: #FF5582A6;">fraction of total predictions that are correct</mark>

> [!MATH] Precision
$$\text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}}$$

- Best Used For 
	- <mark style="background: #BBFABBA6;">Balanced datasets where all classes have similar frequencies</mark>
- Limitation
	- It can be misleading on imbalanced datasets
	- For example, if 99% of the data is Class A, a model predicting "Class A" every time achieves 99% accuracy but fails to detect minority classes
- Best Scenario
	- A high accuracy score means the model is generally reliable at making correct predictions
- Alternatives
	- When dealing with skewed data, metrics such as [[202602011511 - precision|Precision]], [[202602011513 - recall|Recall]], and [[202602011551 - f1|F1 Score]] are often more informative

| Metric        | Focus Question                                   |                 Formula                 | Best Used When...              |
| :------------ | :----------------------------------------------- | :-------------------------------------: | :----------------------------- |
| **Accuracy**  | How often is the model correct overall?          | $\frac{(TP + TN)}{(TP + TN + FP + FN)}$ | Classes are balanced (50/50)   |
| **Precision** | Of all predicted positives, how many were right? |         $\frac{TP}{(TP + FP)}$          | You want to avoid false alarms |
| **Recall**    | Of all actual positives, how many were caught?   |         $\frac{TP}{(TP + FN)}$          | You want to avoid missed cases |
