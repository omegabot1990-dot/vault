---
tags:
- note
aliases:
- Accuracy
title: accuracy
description: Concise concept note on accuracy.
parent nodes:
- '[[evaluation]]'
published on: null
---

- The fraction of total predictions that are correct
- $\text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}}$
- **Best Used For** 
	- Balanced datasets where all classes have similar frequencies
- **Limitation** 
	- It can be misleading on imbalanced datasets
	- For example, if 99% of the data is Class A, a model predicting "Class A" every time achieves 99% accuracy but fails to detect minority classes
- **Best Scenario** 
	- A high accuracy score means the model is generally reliable at making correct predictions
- **Alternatives** 
	- When dealing with skewed data, metrics such as [[202602011511 - precision|Precision]], [[202602011513 - recall|Recall]], and [[202602011551 - f1|F1 Score]] are often more informative

| Metric        | Focus Question                                   |                 Formula                 | Best Used When...              |
| :------------ | :----------------------------------------------- | :-------------------------------------: | :----------------------------- |
| **Accuracy**  | How often is the model correct overall?          | $\frac{(TP + TN)}{(TP + TN + FP + FN)}$ | Classes are balanced (50/50)   |
| **Precision** | Of all predicted positives, how many were right? |         $\frac{TP}{(TP + FP)}$          | You want to avoid false alarms |
| **Recall**    | Of all actual positives, how many were caught?   |         $\frac{TP}{(TP + FN)}$          | You want to avoid missed cases |
