---
tags:
- note
aliases:
- Learning Rate
title: learning rate
description: ''
bot: true
parent nodes:
- '[[hyperparameters]]'
published on: null
---

- Learning rate is a hyperparameter that controls step size in parameter updates
- In gradient-based optimization, it scales how far parameters move along the negative gradient
- Too high can cause instability, oscillation, or divergence
- Too low can make training very slow or stall progress
- Learning rate schedules change this value over training to improve convergence
- Common schedules include step decay, cosine decay, warmup, and one-cycle policies