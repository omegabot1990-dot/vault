---
tags:
  - deep_learning
aliases:
  - Learning Rate
title: learning rate
description: ""
bot: true
parent nodes:
  - "[[hyperparameters]]"
published on:
---

- [ ] What is step decay?
- [ ] What is cosine decay?
- [ ] One-cycle policies?

---
- Learning rate is a [[202602111256 - hyperparameters|hyperparameter]] that controls the step size in [[202602111314 - model parameters|parameter]] updates
- In [[202602111605 - optimization algorithms|gradient-based optimisation]], it scales how far parameters move along the negative gradient
- Too high can cause instability, oscillation, or divergence
- Too low can make training very slow or stall progress
- Learning rate schedules change this value over training to improve convergence
- Common schedules include:
	- Step decay
	- Cosine decay
	- [[202602011426 - warmup|Warmup]]
	- One-cycle policies