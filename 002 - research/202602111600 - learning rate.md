---
tags:
- note
aliases:
- Learning Rate
title: learning rate
description: ''
bot: true
parent nodes:
- '[[hyperparameters]]'
published on: null
---

- Learning rate is a [[202602111256 - hyperparameters|hyperparameter]] that controls the step size in [[202602111314 - model parameters|parameter]] updates
- In gradient-based optimisation, it scales how far parameters move along the negative gradient
- Too high can cause instability, oscillation, or divergence
- Too low can make training very slow or stall progress
- Learning rate schedules change this value over training to improve convergence
- Common schedules include step decay, cosine decay, warmup, and one-cycle policies