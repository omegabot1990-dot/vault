---
tags:
- note
aliases:
- Reward Model
title: reward model
description: ''
bot: true
parent nodes:
- '[[reinforcement learning]]'
published on: null
---

- A reward model predicts scalar preference or quality scores used as rewards for policy optimization
- In RLHF pipelines, reward models are typically trained from human preference comparisons
- The learned reward function enables optimization when explicit ground-truth rewards are unavailable
- Reward misspecification can induce reward hacking and policy behaviors that exploit model flaws
- Calibration, regularization, and adversarial evaluations are used to improve robustness
- Reward models are central components in modern post-training of large language models