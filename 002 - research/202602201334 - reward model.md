---
tags:
  - reinforcement_learning
aliases:
  - Reward Model
title: reward model
description: ""
bot: false
parent nodes:
  - "[[202602192352 - reward|Reward]]"
published on:
---

- A reward model predicts scalar preference or quality scores used as rewards for [[202602192245 - policy|policy]] optimization
- In [[202602192217 - reinforcement learning human feedback|RLHF]] pipelines, reward models are typically [[202602111335 - training|trained]] from human preference comparisons
- <mark style="background: #BBFABBA6;">The learned reward function enables optimization when explicit ground-truth rewards are unavailable</mark>
- <mark style="background: #FF5582A6;">Reward misspecification can induce reward hacking and policy behaviours that exploit model flaws</mark>
- Calibration, [[202602111636 - regularization|regularization]], and adversarial evaluations are used to improve robustness
- Reward models are central components in modern [[202602191644 - post-training|post-training]] of [[202602191524 - large language model|large language models]]