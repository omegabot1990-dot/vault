---
tags:
- math
aliases:
- GELU
- Gaussian Error Linear Unit
title: gelu
description: ''
bot: true
parent nodes:
- '[[math]]'
published on: null
---

- GELU is a smooth activation that scales inputs by their Gaussian CDF probability
- It keeps large positive values, suppresses large negative values, and softly weights values near zero

> [!MATH] GELU function
> The Gaussian Error Linear Unit is
> $$\operatorname{GELU}(x)=x\,\Phi(x)$$
> where $\Phi(x)$ is the standard normal CDF

- Common approximation used in practice
  - $$\operatorname{GELU}(x)\approx 0.5x\left(1+\tanh\left(\sqrt{\frac{2}{\pi}}\left(x+0.044715x^3\right)\right)\right)$$

- Compared with ReLU, GELU is non-piecewise and fully smooth
- GELU is widely used in transformer architectures