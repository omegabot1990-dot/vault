---
tags:
  - math
aliases:
  - GELU
  - Gaussian Error Linear Unit
title: gelu
description: ""
bot: false
parent nodes:
  - "[[202602111400 - activation|Activation]]"
published on:
---

- [ ] What is CDF?

---
- Gaussian Error Linear Unit or GELU is a smooth activation that scales inputs by their Gaussian cumulative density function (CDF) probability
- It keeps large positive values, suppresses large negative values, and softly weights values near zero
- The Gaussian Error Linear Unit is:

> [!MATH] GELU Function
> $$\operatorname{GELU}(x)=x\,\Phi(x)$$
> where $\Phi(x)$ is the standard normal CDF

![GELU|700](https://docs.pytorch.org/docs/stable/_images/GELU.png)

- Common approximation used in practice:

> [!MATH] GELU Function
$$\operatorname{GELU}(x)\approx 0.5x\left(1+\tanh\left(\sqrt{\frac{2}{\pi}}\left(x+0.044715x^3\right)\right)\right)$$

- Compared with [[202602111526 - relu|ReLU]], GELU is non-piecewise and fully smooth
- GELU is widely used in [[202602102143 - transformer|transformer]] architectures