---
tags:
  - academic
  - idl
aliases:
  - deep learning - quick notes
title: deep learning - quick notes
description: exam notes for IDL
parent nodes:
  - "[[introduction to deep learning]]"
child nodes:
annotation-target:
---


## Cues

#### 1: Intro (UDL: 1)

- N/A

#### 2: Supervised Learning and Linear Models (UDL: 1, 2)

- What is supervised learning?
	- An equation relating input (age) to output (height)
	- Search through family of possible equations to find one that fits training data well
	- Deep neural networks are just a very flexible family of equations
	- Fitting deep neural networks = “Deep Learning”
- What is unsupervised learning?
	- Learning about a dataset without labels
		- Clustering
	- Generative models can create examples
		- Generative adversarial networks
	- Probabilistic Generative Models (PGMs) learn distribution over data 
		- Variational autoencoders
		- Normalising flows
		- Diffusion models
	- Latent interpolation
		- Since the latent space of a generative model is usually represented using continuous variables, we can smoothly traverse this space to create blends of generative outputs
	- Image inpainting
- What is reinforcement learning?
- What are the supervised model types?
	- Regression
		- Univariate regression: Predicting one continuous variable
		- Architecture: Multi-layer perceptron / fully connected network
	- Graph regression
		- Multivariate graph regression: given a graph, predict its characteristics (>1)
		- More examples: social network graphs, metabolic networks, a collection of stars, etc.
		- Architecture: Graph neural network
	- Depth estimation
		- Multivariate regression: many outputs, continuous predictions
		- Architecture: Convolutional Encoder-Decoder network
	- Text classification
		- Binary classification: given a sentence, predict its tone/agreeability/positivity, etc.
		- Architecture: Recurrent neural network (RNN), Transformer
	- Image classification
		- Multi-class image classification: which class does an image belong to?
		- Architecture: Convolutional neural network, Transformer
	- Music genre classification
		- Multi-class classification: given an audio wave, predict what genre of music is represented
		- Other examples: speaker identification (“Hey Siri/Alexa/Google” - binary classification)
	- Image segmentation
		- Multivariate binary pixel-level classification: assign 0/1 to each pixel to indicate whether it belongs to the background or not
			- Many outputs, two discrete classes
- What is regression?
	- Continuous numbers as output
- What is classification?
	- Discrete classes as output
- What is the difference between two-class and multi-class classification?
	- Two-class has one output with a threshold
	- Multi-class multiple outputs, treated as probabilities
- What is the difference between univariate and multivariate? 
	- Univariate has one output
	- Multivariate has more than one output
- What are the model terminology and symbols? 
	- Input: $x$
	- Output: $y$
	- Parameters: $\phi$
	- Model: $y=f[x, \phi]$
	- Notes: 
		- Variables are always in Roman letters 
		- Normal = Scalar
		- Bold = Vector
		- Capital bold = Matrix
		- Functions always have square brackets
		- Parameters are always in Greek letters
- What is loss function?
	- Training dataset of $I$ pairs of input/output examples: $\{x_{i},y_{i}\}^I_{i=1}$
	- The loss function or cost function measures how bad the model is
		- $L[\phi,f[x,\phi]],\{x_{i},y_{i}\}^I_{i=1}]$ - \[parameters, model, train data]
		- $L[\phi]$ - in short
	- Returns a scalar that is smaller when model maps inputs to outputs better
- What is training?
	- Find the parameter that minimise the loss
	- $\hat{\phi} = \underset{\phi}{\mathrm{argmin}} \, \mathcal{L}[\phi]$
- What is testing?
	- To test the model, run on a separate test dataset of input/output pairs 
	- See how well it generalises to new data
	- A validation set is used for tuning model hyperparameters
	- Data splitting strategy – important consideration based on how model is going to be deployed
		- Random, temporal, cluster splits, etc.
- Explain 1D linear regression.
	- Model
		- $y=f[x,\phi] = \phi_{0}+\phi_{1}x$
		- $\phi = \begin{bmatrix} \phi_0 \\ \phi_1 \end{bmatrix}$
	- Parameters
		- $\phi_{0}$ - y-offset
		- $\phi_{1}$ - slope
- What is mean squared loss?
	- $\mathcal{L}[\phi] = \sum_{i=1}^I \left( \mathrm{f}[x_i, \phi] - y_i \right)^2$
	- $= \sum_{i=1}^I \left( \phi_0 + \phi_1 x_i - y_i \right)^2$
- What is gradient descent?
- How to test linear regression?
	- Test with different set of paired input/output data
		- Measure performance (loss, accuracy, MSE, etc.)
		- The degree to which the metrics are same as training = generalisation
	- Might not generalise well because:
		- Model too simple
		- Model too complex
			- It fits the noise of the data too well 
			- Known as overfitting

#### 3: Shallow and Deep Neural Networks (UDL: 3, 4)

- What is a univariate regression problem?
	- One output, continuous value
- Why choose neural networks over 1D linear regression?
	- 1D linear regression is too limited
		- More complex relations than linear
		- Multiple inputs, multiple outputs
	- We need a different family of parametrised functions
- What is an example of a shallow neural network?
	- $y = \mathrm{f}[x, \phi] = \phi_0 + \phi_1 a[\theta_{10} + \theta_{11}x] + \phi_2 a[\theta_{20} + \theta_{21}x] + \phi_3 a[\theta_{30} + \theta_{31}x]$
		- $a$ is the activation function
	- $\begin{cases} h_1 = a[\theta_{10} + \theta_{11}x] \\ h_2 = a[\theta_{20} + \theta_{21}x] \\ h_3 = a[\theta_{30} + \theta_{31}x] \end{cases}$
	- $y = \phi_0 + \phi_1 h_1 + \phi_2 h_2 + \phi_3 h_3$
	- $h_d = a[\theta_{d0} + \theta_{d1}x]$
	- $y = \phi_0 + \sum_{d=1}^D \phi_d h_d$
- What is rectified linear unit?
	- $a[z] = \mathrm{ReLU}[z] = \begin{cases} 0 & z < 0 \\ z & z \geq 0 \end{cases}$
- What are the other activation functions?
	- Sigmoid
	- Tanh
	- Leaky ReLU
	- SoftPlus
	- Exponential Linear Unit
	- SiLU
	- GeLU
	- Swish
- What is universal approximation theorem?
	- With enough capacity (hidden units), a shallow network can describe any continuous 1D function defined on a compact subset of the real line to arbitrary precision
- What is the generalised representation of a shallow neural network?
	- $D_i$ inputs
	- $D$ hidden nodes
	- $D_o$ outputs
	- $h_d = a\left[ \theta_{d0} + \sum_{i=1}^{D_i} \theta_{di} x_i \right]$
	- $y_j = \phi_{j0} + \sum_{d=1}^D \phi_{jd} h_d$
- What are the terminologies used in a shallow neural network?
	- Neural network, multilayer perceptron (MLP)
	- 1 hidden layer means shallow, greater than 1 hidden layer means deep
	- All nodes connected means fully-connected
	- Values before activation are called pre-activations
	- Values after activation are called activations
	- Y-offsets are called biases
- What is the problem with a shallow neural network?
	- Can require extremely large number of hidden nodes
- What are the hyperparameters?
	- $K$ number of layers: depth of network
	- $D_{k}$ nodes per layer: width of network
- What is hyperparameter optimisation?
- What is the matrix notation for a simple deep network?
	- $\begin{bmatrix} h_1 \\ h_2 \\ h_3 \end{bmatrix} = \mathbf{a} \left( \begin{bmatrix} \theta_{10} \\ \theta_{20} \\ \theta_{30} \end{bmatrix} + \begin{bmatrix} \theta_{11} \\ \theta_{21} \\ \theta_{31} \end{bmatrix} x \right)$
		- $\mathbf{h} = \mathbf{a}\left[ \boldsymbol{\theta}_0 + \boldsymbol{\theta} x \right]$
	- $\begin{bmatrix} h_1' \\ h_2' \\ h_3' \end{bmatrix} = \mathbf{a} \left( \begin{bmatrix} \psi_{10} \\ \psi_{20} \\ \psi_{30} \end{bmatrix} + \begin{bmatrix} \psi_{11} & \psi_{12} & \psi_{13} \\ \psi_{21} & \psi_{22} & \psi_{23} \\ \psi_{32} & \psi_{32} & \psi_{33} \end{bmatrix} \begin{bmatrix} h_1 \\ h_2 \\ h_3 \end{bmatrix} \right)$
		- $\mathbf{h'} = \mathbf{a}\left[ \boldsymbol{\psi}_0 + \boldsymbol{\Psi} \mathbf{h} \right]$
	- $y' = \phi_0' + \begin{bmatrix} \phi_1' & \phi_2' & \phi_3' \end{bmatrix} \begin{bmatrix} h_1' \\ h_2' \\ h_3' \end{bmatrix}$
		- $y = \phi_0' + \boldsymbol{\phi'} \mathbf{h'}$
	- $\mathbf{h}_1 = \mathbf{a}\left[\boldsymbol{\beta}_0 + \boldsymbol{\Omega}_0 \mathbf{x} \right]$
	- $\mathbf{h}_2 = \mathbf{a}\left[\boldsymbol{\beta}_1 + \boldsymbol{\Omega}_1 \mathbf{h}_1 \right]$
	- $\mathbf{h}_3 = \mathbf{a}\left[\boldsymbol{\beta}_2 + \boldsymbol{\Omega}_2 \mathbf{h}_2 \right]$
	- $\vdots$
	- $\mathbf{h}_K = \mathbf{a}\left[\boldsymbol{\beta}_{K-1} + \boldsymbol{\Omega}_{K-1} \mathbf{h}_{K-1} \right]$
	- $\mathbf{y} = \boldsymbol{\beta}_K + \boldsymbol{\Omega}_K \mathbf{h}_K$
		- $\boldsymbol{\beta}$ - Bias vector
		- $\boldsymbol{\Omega}$ - Weight vector
- Does deep networks obey universal approximation theorem?
	- Yes, deep network is extension of shallow network
- What are the differences between shallow and deep networks?
	- Deep networks create many more regions per parameter
		- There are dependencies between regions
		- Apparently not a problem for many applications
	- Deep networks exhibit depth efficiency
		- Some functions require shallow networks with exponentially more hidden units than deep networks for similar accuracy
	- Deep networks are large structured networks
		- Modern networks often use other types of layers
			- Convolutional, pooling, attention, etc. 
		- Having many layers allows for structuring these layers, allowing exploiting some knowledge about the problem
	- Training and generalisation
		- Deep networks seem easier to train than shallow networks
		- Deep networks seem to generalise better than shallow networks

#### 4: Loss Function and Gradient Descent (UDL: 5, 6)

- What do neural networks do?
	- The network predicts a conditional probability distribution over outputs $y$ given inputs $x$
		- $Pr(y|x)$
	- The loss function aims to give training examples with a high probability
- How do we model this?
	- Pick a distribution with parameters $\boldsymbol{\theta}$
	- $\hat{\phi} = \underset{\phi}{\mathrm{argmax}} \left[ \prod_{i=1}^I Pr(y_i | \mathbf{x}_i) \right]$
	- $= \underset{\phi}{\mathrm{argmax}} \left[ \prod_{i=1}^I Pr(y_i | \theta_i) \right]$
	- $= \underset{\phi}{\mathrm{argmax}} \left[ \prod_{i=1}^I Pr(y_i | \mathrm{f}[\mathbf{x}_i, \phi]) \right]$
	- $\hat{\phi} = \underset{\phi}{\mathrm{argmax}} \left[ \prod_{i=1}^I Pr(y_i | \mathrm{f}[\mathbf{x}_i, \phi]) \right]$
- What is maximum log likelihood?
	- $\hat{\phi} = \underset{\phi}{\mathrm{argmax}} \left[ \prod_{i=1}^I Pr(y_i | \mathrm{f}[\mathbf{x}_i, \phi]) \right]$
	- $= \underset{\phi}{\mathrm{argmax}} \left[ \log \left( \prod_{i=1}^I Pr(y_i | \mathrm{f}[\mathbf{x}_i, \phi]) \right) \right]$
	- $= \underset{\phi}{\mathrm{argmax}} \left[ \sum_{i=1}^I \log \left( Pr(y_i | \mathrm{f}[\mathbf{x}_i, \phi]) \right) \right]$
- What is minimum negative log likelihood?
	- $\hat{\phi} = \underset{\phi}{\mathrm{argmax}} \left[ \sum_{i=1}^I \log \left( Pr(y_i | \mathrm{f}[\mathbf{x}_i, \phi]) \right) \right]$
	- $= \underset{\phi}{\mathrm{argmin}} \left[ - \sum_{i=1}^I \log \left( Pr(y_i | \mathrm{f}[\mathbf{x}_i, \phi]) \right) \right]$
	- $= \underset{\phi}{\mathrm{argmin}} \left[ \mathcal{L}[\phi] \right]$
- What is inference?
	- Network outputs distribution
	- In practice, we often need values so we use maximum of the output distribution
	- $\hat{\mathbf{y}} = \underset{\mathbf{y}}{\mathrm{argmax}} \left[ Pr(\mathbf{y} | \mathrm{f}[\mathbf{x}, \boldsymbol{\phi}]) \right]$
- What is the recipe of a loss function?
	- Choose a suitable probability distribution $Pr(\mathbf{y} | \boldsymbol{\theta})$ that is defined over the domain of the predictions $\mathbf{y}$ and has distribution parameters $\boldsymbol{\theta}$.
	- Set the machine learning model $\mathrm{f}[\mathbf{x}, \boldsymbol{\phi}]$ to predict one or more of these parameters so $\boldsymbol{\theta} = \mathrm{f}[\mathbf{x}, \boldsymbol{\phi}]$ and $Pr(\mathbf{y} | \boldsymbol{\theta}) = Pr(\mathbf{y} | \mathrm{f}[\mathbf{x}, \boldsymbol{\phi}])$.
	- To train the model, find the network parameters $\hat{\boldsymbol{\phi}}$ that minimise the negative log-likelihood loss function over the training dataset pairs $\{ \mathbf{x}_i, \mathbf{y}_i \}$:
	- $\hat{\boldsymbol{\phi}} = \underset{\boldsymbol{\phi}}{\mathrm{argmin}} \left[ L[\boldsymbol{\phi}] \right] = \underset{\boldsymbol{\phi}}{\mathrm{argmin}} \left[ -\sum_{i=1}^I \log \left( Pr(y_i | \mathrm{f}[\mathbf{x}_i, \boldsymbol{\phi}]) \right) \right]$
	- To perform inference for a new test example $\mathbf{x}$, return either the full distribution $Pr(\mathbf{y} | \mathrm{f}[\mathbf{x}, \hat{\boldsymbol{\phi}}])$ or the maximum of this distribution
- Explain the univariate regression problem?
	- Step 1
		- Normal distribution
		- $Pr(y | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left[ -\frac{(y - \mu)^2}{2\sigma^2} \right]$
	- Step 2
		- $Pr(y | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left[ -\frac{(y - \mu)^2}{2\sigma^2} \right]$
		- $Pr(y | \mathrm{f}[\mathbf{x}, \boldsymbol{\phi}], \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left[ -\frac{(y - \mathrm{f}[\mathbf{x}, \boldsymbol{\phi}])^2}{2\sigma^2} \right]$
	- Step 3
		- $L[\boldsymbol{\phi}] = -\sum_{i=1}^I \log \left[ Pr(y_i | \mathrm{f}[\mathbf{x}_i, \boldsymbol{\phi}], \sigma^2) \right]$
		- $= -\sum_{i=1}^I \log \left[ \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{\left( y_i - \mathrm{f}[\mathbf{x}_i, \boldsymbol{\phi}] \right)^2}{2\sigma^2} \right) \right]$
		- $\hat{\boldsymbol{\phi}} = \underset{\boldsymbol{\phi}}{\mathrm{argmin}} \left[ -\sum_{i=1}^I \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{\left( y_i - \mathrm{f}[\mathbf{x}_i, \boldsymbol{\phi}] \right)^2}{2\sigma^2} \right) \right) \right]$
		- $= \underset{\boldsymbol{\phi}}{\mathrm{argmin}} \left[ -\sum_{i=1}^I \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right) + \log \left( \exp \left( -\frac{\left( y_i - \mathrm{f}[\mathbf{x}_i, \boldsymbol{\phi}] \right)^2}{2\sigma^2} \right) \right) \right]$
		- $= \underset{\boldsymbol{\phi}}{\mathrm{argmin}} \left[ -\sum_{i=1}^I \log \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right) - \frac{\left( y_i - \mathrm{f}[\mathbf{x}_i, \boldsymbol{\phi}] \right)^2}{2\sigma^2} \right]$
		- $= \underset{\boldsymbol{\phi}}{\mathrm{argmin}} \left[ -\sum_{i=1}^I \frac{\left( y_i - \mathrm{f}[\mathbf{x}_i, \boldsymbol{\phi}] \right)^2}{2\sigma^2} \right]$
		- $= \underset{\boldsymbol{\phi}}{\mathrm{argmin}} \left[ \sum_{i=1}^I \left( y_i - \mathrm{f}[\mathbf{x}_i, \boldsymbol{\phi}] \right)^2 \right]$
			- Least square error
	- Step 4
		- $Pr(y | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left[ -\frac{(y - \mu)^2}{2\sigma^2} \right]$
		- $Pr(y | \mathrm{f}[\mathbf{x}, \boldsymbol{\phi}], \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left[ -\frac{\left( y - \mathrm{f}[\mathbf{x}, \boldsymbol{\phi}] \right)^2}{2\sigma^2} \right]$
	- Here we assume the noise is constant everywhere
	- We can also make it functions of the input by using two outputs
		- $\mu = \mathrm{f}_1[\mathbf{x}, \boldsymbol{\phi}]$
		- $\sigma^2 = \mathrm{f}_2[\mathbf{x}, \boldsymbol{\phi}]^2$
		- $\hat{\boldsymbol{\phi}} = \underset{\boldsymbol{\phi}}{\mathrm{argmin}} \left[ -\sum_{i=1}^I \log \left( \frac{1}{\sqrt{2\pi \mathrm{f}_2[\mathbf{x}_i, \boldsymbol{\phi}]^2}} \right) - \frac{\left( y_i - \mathrm{f}_1[\mathbf{x}_i, \boldsymbol{\phi}] \right)^2}{2\mathrm{f}_2[\mathbf{x}_i, \boldsymbol{\phi}]^2} \right]$
- Explain binary classification problem?
	- Step 1
		- Bernoulli distribution, domain $y \in \{0, 1\}$
		- One parameter $\lambda \in [0, 1]$
		- $Pr(y | \lambda) = \begin{cases} 1 - \lambda & \text{if } y = 0 \\ \lambda & \text{if } y = 1 \end{cases}$
		- $Pr(y | \lambda) = (1 - \lambda)^{1 - y} \cdot \lambda^y$
	- Step 2
		- Here the problem is the neural network output can be anything so we pass it through a logistic sigmoid function to map it to $[0,1]$
			- $\mathrm{sig}[z] = \frac{1}{1 + \exp(-z)}$
		- $Pr(y | \lambda) = (1 - \lambda)^{1 - y} \cdot \lambda^y$
		- $Pr(y | \mathbf{x}) = (1 - \mathrm{sig}[\mathrm{f}[\mathbf{x} | \boldsymbol{\phi}]])^{1 - y} \cdot \mathrm{sig}[\mathrm{f}[\mathbf{x} | \boldsymbol{\phi}]]^y$
	- Step 3
		- $Pr(y | \mathbf{x}) = (1 - \mathrm{sig}[\mathrm{f}[\mathbf{x} | \boldsymbol{\phi}]])^{1 - y} \cdot \mathrm{sig}[\mathrm{f}[\mathbf{x} | \boldsymbol{\phi}]]^y$
		- $L[\boldsymbol{\phi}] = \sum_{i=1}^I \left[ -(1 - y_i) \log \left( 1 - \mathrm{sig}[\mathrm{f}[\mathbf{x}_i | \boldsymbol{\phi}]] \right) - y_i \log \left( \mathrm{sig}[\mathrm{f}[\mathbf{x}_i | \boldsymbol{\phi}]] \right) \right]$
			- Binary cross entropy loss
	- Step 4
		- Return full or maximum of distribution
- Explain multi-class classification problem?
	- Step 1
		- Categorical distribution, domain $y\in\{1,2,\dots,K\}$
		- $K$ parameters $\lambda_{k} \in [0,1]$
		- Sum of params = 1
		- $Pr(y=k) = \lambda_{k}$
	- Step 2
		- Here the problem is the neural network output can be anything but we need $\lambda_{k} \in [0,1]$ that sums to one so we pass it through a $softmax$ function
			- $\mathrm{softmax}_k[\mathbf{z}] = \frac{\exp[z_k]}{\sum_{k'=1}^K \exp[z_{k'}]}$
		- $Pr(y = k | \mathbf{x}) = \mathrm{softmax}_k[\mathrm{f}[\mathbf{x}, \boldsymbol{\phi}]]$
	- Step 3
		- $L[\boldsymbol{\phi}] = -\sum_{i=1}^I \log \left[ \text{softmax}_{y_i} \left[ f \left[ \boldsymbol{x}_i, \boldsymbol{\phi} \right] \right] \right]$
		- $L[\boldsymbol{\phi}] = -\sum_{i=1}^I f_{y_i} \left[ \boldsymbol{x}_i, \boldsymbol{\phi} \right] - \log \left[ \sum_{k=1}^K \exp \left[ f_k \left[ \boldsymbol{x}_i, \boldsymbol{\phi} \right] \right] \right]$
			- Multi-class cross-entropy loss
	- Step 4
		- Return full or maximum of distribution
- How can we handle multiple outputs?
	- Treat all outputs as independent
		- $Pr(\mathbf{y} \mid \mathbf{f}[\mathbf{x}_i, \boldsymbol{\phi}]) = \prod_d Pr(y_d \mid \mathbf{f}_d[\mathbf{x}_i, \boldsymbol{\phi}])$
	- Negative log likelihood becomes,
		- $L[\boldsymbol{\phi}] = -\sum_{i=1}^I \log \left[ Pr(\mathbf{y} \mid \mathbf{f}[\mathbf{x}_i, \boldsymbol{\phi}]) \right] = -\sum_{i=1}^I \sum_d \log \left[ Pr(y_{id} \mid \mathbf{f}_d[\mathbf{x}_i, \boldsymbol{\phi}]) \right]$
	- Outputs can vary in magnitude
		- Example, predict weight in kilos and height in meters
		- One dimension has much bigger numbers than others
	- Solutions
		- So we learn separate variance for each output
		- Rescale before training
- What is gradient descent?
	- Step 1
		- Compute the derivatives of the loss with respect to parameters
		- $\frac{\partial L}{\partial \boldsymbol{\phi}} = \begin{bmatrix} \frac{\partial L}{\partial \phi_0} \\ \frac{\partial L}{\partial \phi_1} \\ \vdots \\ \frac{\partial L}{\partial \phi_N} \end{bmatrix}$
	- Step 2
		- Update the params according to the rule
		- $\boldsymbol{\phi} \gets \boldsymbol{\phi} - \alpha \frac{\partial L}{\partial \boldsymbol{\phi}},$
		- Here, $\alpha$ is the step size or learning rate
- How to do gradient descent for linear regression?
	- Step 1
		- $\frac{\partial L}{\partial \boldsymbol{\phi}} = \frac{\partial}{\partial \boldsymbol{\phi}} \sum_{i=1}^I \ell_i = \sum_{i=1}^I \frac{\partial \ell_i}{\partial \boldsymbol{\phi}}$
		- $\frac{\partial \ell_i}{\partial \boldsymbol{\phi}} = \begin{bmatrix} \frac{\partial \ell_i}{\partial \phi_0} \\ \frac{\partial \ell_i}{\partial \phi_1} \end{bmatrix} = \begin{bmatrix} 2(\phi_0 + \phi_1 x_i - y_i) \\ 2x_i (\phi_0 + \phi_1 x_i - y_i) \end{bmatrix}$
	- Step 2
		- $\boldsymbol{\phi} \gets \boldsymbol{\phi} - \alpha \frac{\partial L}{\partial \boldsymbol{\phi}}$
- What is convexity?
	- Linear regression problem is convex
	- Only global minimum, gradient descent always finds it
	- Loss functions for non-linear neural networks typically non-convex
- What is Gabor model?
	- Global minimum can be found if starting at the right place
	- But, can also end up in local minima
	- Or, get stuck near saddle points
- What is stochastic gradient descent?
	- Idea: Add noise to gradient
	- Use only subset of data for each iteration 
		- Mini-batch
	- Can escape from local minima
	- Adds noise, but still sensible updates as based on part of data
	- Uses all data equally
	- Less computationally expensive
	- Seems to find better solutions
	- Doesn’t converge in traditional sense
	- Learning rate schedule – decrease learning rate over time Stochastic gradient descent
- What is an epoch?
	- Single pass through all data (without replacement)
- Compare gradient descent vs stochastic gradient descent?
	- GD
		- $\boldsymbol{\phi}_{t+1} \gets \boldsymbol{\phi}_t - \alpha \sum_{i=1}^I \frac{\partial \ell_i[\boldsymbol{\phi}_t]}{\partial \boldsymbol{\phi}}$
	- SGD
		- $\boldsymbol{\phi}_{t+1} \gets \boldsymbol{\phi}_t - \alpha \sum_{i \in \mathcal{B}_t} \frac{\partial \ell_i[\boldsymbol{\phi}_t]}{\partial \boldsymbol{\phi}}$
- What is momentum?
	- Weighted sum of this gradient and previous gradient
		- $\mathbf{m}_{t+1} \gets \beta \cdot \mathbf{m}_t + (1 - \beta) \sum_{i \in \mathcal{B}_t} \frac{\partial \ell_i[\boldsymbol{\phi}_t]}{\partial \boldsymbol{\phi}}$
		- $\boldsymbol{\phi}_{t+1} \gets \boldsymbol{\phi}_t - \alpha \cdot \mathbf{m}_{t+1}$
	- Smoother trajectory, less oscillations
- What is Nesterov accelerated momentum?
	- Momentum is like a prediction
	- Alternatively we can first predict then measure gradient
		- $\mathbf{m}_{t+1} \gets \beta \cdot \mathbf{m}_t + (1 - \beta) \sum_{i \in \mathcal{B}_t} \frac{\partial \ell_i \left[ \boldsymbol{\phi}_t - \alpha \beta \cdot \mathbf{m}_t \right]}{\partial \boldsymbol{\phi}}$
- How do we normalising gradients?
	- Measure gradient and squared gradient
		- $\mathbf{m}_{t+1} \gets \frac{\partial L [\boldsymbol{\phi}_t]}{\partial \boldsymbol{\phi}} \quad \quad \mathbf{v}_{t+1} \gets \left( \frac{\partial L [\boldsymbol{\phi}_t]}{\partial \boldsymbol{\phi}} \right)^2$
		- $\mathbf{m}_{t+1} = \begin{bmatrix} 3.0 \\ -2.0 \\ 5.0 \end{bmatrix}, \quad \mathbf{v}_{t+1} = \begin{bmatrix} 9.0 \\ 4.0 \\ 25.0 \end{bmatrix}, \quad \frac{\mathbf{m}_{t+1}}{\sqrt{\mathbf{v}_{t+1}} + \epsilon} = \begin{bmatrix} 1.0 \\ -1.0 \\ 1.0 \end{bmatrix}$
	- Update parameters using normalised gradient
		- $\boldsymbol{\phi}_{t+1} \gets \boldsymbol{\phi}_t - \alpha \cdot \frac{\mathbf{m}_{t+1}}{\sqrt{\mathbf{v}_{t+1}} + \epsilon}$
- What is Adam (Adaptive Moment Estimation)
	- Use momentum for estimate of gradient and squared gradient
		- $\mathbf{m}_{t+1} \gets \beta \cdot \mathbf{m}_t + (1 - \beta) \frac{\partial L[\boldsymbol{\phi}_t]}{\partial \boldsymbol{\phi}}$
		- $\mathbf{v}_{t+1} \gets \gamma \cdot \mathbf{v}_t + (1 - \gamma) \left( \frac{\partial L[\boldsymbol{\phi}_t]}{\partial \boldsymbol{\phi}} \right)^2$
	- Adjust for bias in initial steps
		- $\tilde{\mathbf{m}}_{t+1} \gets \frac{\mathbf{m}_{t+1}}{1 - \beta^{t+1}}$
		- $\tilde{\mathbf{v}}_{t+1} \gets \frac{\mathbf{v}_{t+1}}{1 - \gamma^{t+1}}$
	- Update parameters
		- $\boldsymbol{\phi}_{t+1} \gets \boldsymbol{\phi}_t - \alpha \cdot \frac{\tilde{\mathbf{m}}_{t+1}}{\sqrt{\tilde{\mathbf{v}}_{t+1}} + \epsilon}$

#### 5: Computing Gradients (UDL: 7)

- What is the loss?
	- $L[\boldsymbol{\phi}] = \sum_{i=1}^I \ell_i = \sum_{i=1}^I \ell[f(\mathbf{x}_i, \boldsymbol{\phi}), y_i]$
- What is the SGD algorithm and its parameters?
	- $\boldsymbol{\phi}_{t+1} \gets \boldsymbol{\phi}_t - \alpha \sum_{i \in \mathcal{B}_t} \frac{\partial \ell_i[\boldsymbol{\phi}_t]}{\partial \boldsymbol{\phi}}$
	- $\boldsymbol{\phi} = \{\beta_0, \Omega_0, \beta_1, \Omega_1, \beta_2, \Omega_2, \beta_3, \Omega_3\}$
- Why do we need to compute gradients?
	- In principle, a neural network is just an equation
	- But it is huge, and we need to compute the derivative for every parameter, for every point in the batch, for every iteration of SGD
	- $\frac{\partial \ell_i}{\partial \beta_k} \quad \text{and} \quad \frac{\partial \ell_i}{\partial \Omega_k}$
- What is the intuition behind backpropagation, forward pass?
	- We need to know the activations at each layer
- What is the intuition behind backpropagation, backward pass?
	- To calculate how a small change of weight or bias feeding into $\boldsymbol{h_{i}}$ changes to loss
	- Need to know how a change in $\boldsymbol{h_{1}}$ changes $\boldsymbol{h_{2}}$
	- Need to know how a change in $\boldsymbol{h_{2}}$ changes $\boldsymbol{h_{3}}$
	- Need to know how a change in $\boldsymbol{h_{3}}$ changes the model output $\boldsymbol{f}$
	- Need to know how a change in model output $\boldsymbol{f}$ changes the loss $l$
	- Compute how model output $\boldsymbol{f}$ changes the loss $l$, determine gradients of $\boldsymbol{\Omega_{3}}$
	- Compute how $\boldsymbol{h_{3}}$ changes the model output $\boldsymbol{f}$, determine gradients of $\boldsymbol{\Omega_{2}}$
	- Compute how a change in $\boldsymbol{h_{2}}$ changes $\boldsymbol{h_{3}}$, determine gradients of $\boldsymbol{\Omega_{1}}$
	- Compute how a change in $\boldsymbol{h_{1}}$ changes $\boldsymbol{h_{2}}$, determine gradients of $\boldsymbol{\Omega_{0}}$
- What is backpropagation?
	- Step 1 (forward pass)
		- Write the equation as a series of intermediate calculations
	- Step 2 (forward pass)
		- Compute the intermediate quantities
		- Example
			- $f_0 = \beta_0 + \omega_0 \cdot x_i$
			- $h_1 = \sin[f_0]$
			- $f_1 = \beta_1 + \omega_1 \cdot h_1$
			- $h_2 = \exp[f_1]$
			- $f_2 = \beta_2 + \omega_2 \cdot h_2$
			- $h_3 = \cos[f_2]$
			- $f_3 = \beta_3 + \omega_3 \cdot h_3$
			- $\ell_i = (f_3 - y_i)^2$
			- $\begin{aligned} f_0 &= \beta_0 + \Omega_0 x_i \\ h_1 &= a[f_0] \\ f_1 &= \beta_1 + \Omega_1 h_1 \\ h_2 &= a[f_1] \\ f_2 &= \beta_2 + \Omega_2 h_2 \\ h_3 &= a[f_2] \\ f_3 &= \beta_3 + \Omega_3 h_3 \\ \ell_i &= l[f_3, y_i] \end{aligned}$
	- Step 3 (backward pass)
		- Compute the derivates of the loss with respect to these intermediate quantities but in reverse order
		- Example
			- $\frac{\partial \ell_i}{\partial f_3}, \quad \frac{\partial \ell_i}{\partial h_3}, \quad \frac{\partial \ell_i}{\partial f_2}, \quad \frac{\partial \ell_i}{\partial h_2}, \quad \frac{\partial \ell_i}{\partial f_1}, \quad \frac{\partial \ell_i}{\partial h_1}, \quad \text{and} \quad \frac{\partial \ell_i}{\partial f_0}$
	- Step 4 (backward pass)
		- Use chain rule to get the derivatives
		- Example
			- $\frac{\partial \ell_i}{\partial f_2} = \frac{\partial h_3}{\partial f_2} \left( \frac{\partial f_3}{\partial h_3} \frac{\partial \ell_i}{\partial f_3} \right)$
			- $\frac{\partial \ell_i}{\partial h_2} = \frac{\partial f_2}{\partial h_2} \left( \frac{\partial h_3}{\partial f_2} \frac{\partial f_3}{\partial h_3} \frac{\partial \ell_i}{\partial f_3} \right)$
			- $\frac{\partial \ell_i}{\partial f_1} = \frac{\partial h_2}{\partial f_1} \left( \frac{\partial f_2}{\partial h_2} \frac{\partial h_3}{\partial f_2} \frac{\partial f_3}{\partial h_3} \frac{\partial \ell_i}{\partial f_3} \right)$
			- $\frac{\partial \ell_i}{\partial h_1} = \frac{\partial f_1}{\partial h_1} \left( \frac{\partial h_2}{\partial f_1} \frac{\partial f_2}{\partial h_2} \frac{\partial h_3}{\partial f_2} \frac{\partial f_3}{\partial h_3} \frac{\partial \ell_i}{\partial f_3} \right)$
			- $\frac{\partial \ell_i}{\partial f_0} = \frac{\partial h_1}{\partial f_0} \left( \frac{\partial f_1}{\partial h_1} \frac{\partial h_2}{\partial f_1} \frac{\partial f_2}{\partial h_2} \frac{\partial h_3}{\partial f_2} \frac{\partial f_3}{\partial h_3} \frac{\partial \ell_i}{\partial f_3} \right)$
			- $\frac{\partial f_3}{\partial h_3} = \frac{\partial}{\partial h_3} \left( \beta_3 + \Omega_3 h_3 \right) = \Omega_3^T$
	- Step 5 (backward pass)
		- Take derivatives w.r.t params
- How does loss change as a function of parameters $\beta$ and $\omega$?
	- Use chain rule
		- $\frac{\partial \ell_i}{\partial \omega_k} = \frac{\partial f_k}{\partial \omega_k} \frac{\partial \ell_i}{\partial f_k}$
			- $\frac{\partial \ell_i}{\partial \beta_k} = \frac{\partial f_k}{\partial \beta_k} \frac{\partial \ell_i}{\partial f_k} = \frac{\partial}{\partial \beta_k} \left( \beta_k + \mathbf{\Omega}_k \mathbf{h}_k \right) \frac{\partial \ell_i}{\partial f_k} = \frac{\partial \ell_i}{\partial f_k}$
		- $\frac{\partial \ell_i}{\partial \beta_k} = \frac{\partial f_k}{\partial \beta_k} \frac{\partial \ell_i}{\partial f_k}$
			- $\frac{\partial \ell_i}{\partial \mathbf{\Omega}_k} = \frac{\partial f_k}{\partial \mathbf{\Omega}_k} \frac{\partial \ell_i}{\partial f_k} = \frac{\partial}{\partial \mathbf{\Omega}_k} \left( \beta_k + \mathbf{\Omega}_k \mathbf{h}_k \right) \frac{\partial \ell_i}{\partial f_k} = \frac{\partial \ell_i}{\partial f_k} \mathbf{h}_k^\top$
- What is the derivative of ReLU?
	- Indicator function
		- 1 if $z>0$
		- 0 if $z\leq 0$
	- $\mathbb{I}[z>0]$
	- $\frac{\partial h_3}{\partial f_2} = \mathbb{I}[f_{2}>0]$
- How to summarise backpropagation?
	- Forward pass: We compute and store the following quantities
		- $\begin{aligned} f_0 &= \beta_0 + \Omega_0 \mathbf{x}_i, \\ h_k &= \mathbf{a}[f_{k-1}], \quad k \in \{1, 2, \dots, K\}, \\ f_k &= \beta_k + \Omega_k h_k, \quad k \in \{1, 2, \dots, K\} \end{aligned}$
	- Backward pass: We start with the derivatives of the loss function $\ell_{i}$ with respect to the network output and work backward through the network
		- $\begin{aligned} \frac{\partial \ell_i}{\partial \beta_k} &= \frac{\partial \ell_i}{\partial f_k}, \quad k \in \{K, K-1, \dots, 1\}, \\ \frac{\partial \ell_i}{\partial \Omega_k} &= \frac{\partial \ell_i}{\partial f_k} h_k^T, \quad k \in \{K, K-1, \dots, 1\}, \\ \frac{\partial \ell_i}{\partial f_{k-1}} &= \mathbb{I}[f_{k-1} > 0] \odot \left( \Omega_k^T \frac{\partial \ell_i}{\partial f_k} \right), \quad k \in \{K, K-1, \dots, 1\}. \end{aligned}$
		- Where $\odot$ denotes pointwise multiplication and $\mathbb{I}[f_{k-1} > 0]$ is a vector containing ones where $f_{k-1}$ is greater than zero and zeros elsewhere
	- $\begin{aligned} \frac{\partial \ell_i}{\partial \beta_0} &= \frac{\partial \ell_i}{\partial \mathbf{f}_0}, \\ \frac{\partial \ell_i}{\partial \Omega_0} &= \frac{\partial \ell_i}{\partial \mathbf{f}_0} \mathbf{x}_i^T \end{aligned}$
- What are the advantages and disadvantages of backpropagation?
	- Advantage
		- Extremely efficient, even for large neural networks
		- Very structured approach, applicable to large networks 
	- Disadvantages
		- Memory hungry – must store all intermediate quantities
			- Some pragmatic solutions exist
		- Sequential – difficult to parallelise very large networks
			- Batches can be processed in parallel
- What is automatic differentiation?
	- Modern deep learning frameworks compute derivatives automatically
		- You just have to specify the model and the loss
	- How? 
		- Each component knows how to compute its own derivative
			- ReLU knows how to compute derivative of output w.r.t input
		- You specify how the order of the components are
		- It can compute the chain of derivatives
		- Works with branches as long as it’s still an acyclic graph
- How to initialise params?
	- Typically, we use random initialisation
	- Normal distribution with mean 0, variance $\sigma$
- What should the variance be?
	- If variance small
		- $f_{k}$ will become progressively smaller
	- If variance large
		- $f_{k}$ will become progressively larger
	- Similar problem for backward pass
		- Vanishing and exploding gradients
	- We want variance of $f'$ to be of similar magnitude of $f$ 
		- So we look at mean and variance of $f ’$ as a function of the variance of $\Omega$
- How does expectation work?
	- $\begin{aligned} \mathbb{E}[k] &= k, \\ \mathbb{E}[k \cdot g[x]] &= k \cdot \mathbb{E}[g[x]], \\ \mathbb{E}[f[x] + g[x]] &= \mathbb{E}[f[x]] + \mathbb{E}[g[x]], \\ \mathbb{E}[f[x]g[y]] &= \mathbb{E}[f[x]] \mathbb{E}[g[y]] \end{aligned}$
- How to make variance of $f'$ similar in magnitude to $f$?
	- $\begin{aligned} \mathbb{E}[f_i'] &= \mathbb{E} \left[ \beta_i + \sum_{j=1}^{D_h} \Omega_{ij} h_j \right] \\ &= \mathbb{E}[\beta_i] + \sum_{j=1}^{D_h} \mathbb{E}[\Omega_{ij} h_j] \\ &= \mathbb{E}[\beta_i] + \sum_{j=1}^{D_h} \mathbb{E}[\Omega_{ij}] \mathbb{E}[h_j] \\ &= 0 + \sum_{j=1}^{D_h} 0 \cdot \mathbb{E}[h_j] \\ &= 0 \end{aligned}$
	- $\begin{aligned} \sigma_{f_i'}^2 &= \mathbb{E}[f_i'^2] - \mathbb{E}[f_i']^2 \\ &= \mathbb{E} \left[ \left( \beta_i + \sum_{j=1}^{D_h} \Omega_{ij} h_j \right)^2 \right] - 0 \\ &= \mathbb{E} \left[ \left( \sum_{j=1}^{D_h} \Omega_{ij} h_j \right)^2 \right] \\ &= \sum_{j=1}^{D_h} \mathbb{E}[\Omega_{ij}^2] \mathbb{E}[h_j^2] \\ &= \sum_{j=1}^{D_h} \sigma_\Omega^2 \mathbb{E}[h_j^2] \\ &= \sigma_\Omega^2 \sum_{j=1}^{D_h} \mathbb{E}[h_j^2] \end{aligned}$
	- Assume half the values are clipped by ReLU
		- $\begin{aligned} \sigma_{f'_i}^2 &= \sigma_\Omega^2 \sum_{j=1}^{D_h} \frac{\sigma_f^2}{2} = \frac{1}{2} D_h \sigma_\Omega^2 \sigma_f^2, \\ \sigma_\Omega^2 &= \frac{2}{D_h} \end{aligned}$
- What is He initialisation?
	- For forward pass
		- $\sigma_\Omega^2 = \frac{2}{D_h}$
	- For backward pass
		- $\sigma_\Omega^2 = \frac{2}{D_h'}$
	- If $D_h \neq D_{h'}$
		- $\sigma_\Omega^2 = \frac{4}{D_h + D_h'}$
- 

#### 6: Errors and Regularisation (UDL: 8, 9)

- What is the main challenge we are focussing on?
	- We typically want good performance on new data, not a training set
	- Minimising training loss does not guarantee good performance on test data
- What are the test errors?
	- Noise
	- Bias
	- Variance
- What is noise?
	- Even if the network fits the ground truth perfectly, there can be noise in the test set 
		- Noise in data
		- Unobserved variables
		- Mislabeled data
- What is variance?
	- Uncertainty in the fitted network due to the limited size of the training set (and additional variance due to SGD)
- What is bias?
	- Systematic error from the ground truth due to limitations in the flexibility of the chosen model
- What is the noise, bias and variance for linear regression?
	- Data generated with, 
		- Mean 
			- $\mu[x] = \mathbb{E}_y[y[x]]$
		- Standard deviation
			- $\sigma^2 = \mathbb{E}_y\left[ (\mu[x] - y[x])^2 \right]$
		- Loss
			- $L[x] = \left( f[x, \phi] - y[x] \right)^2$
	- $\begin{aligned} &\mathbb{E}_D \left[ \mathbb{E}_y[L[x]] \right] = \mathbb{E}_D \left[ \left( f[x, \phi[\mathcal{D}]] - f_\mu[x] \right)^2 \right] + \left( f_\mu[x] - \mu[x] \right)^2 + \sigma^2 \end{aligned}$
		- $\mathbb{E}_D$
			- Expectation over training set choice
		- $\mathbb{E}_y[L[x]]$
			- Expectation over test data samples
		- $f[x, \phi[\mathcal{D}]]$
			- Trained model
		- $f_\mu[x]$
			- Best possible model (infinite data)
		- $\mu[x]$
			- Ground truth
		- $\mathbb{E}_D \left[ \left( f[x, \phi[\mathcal{D}]] - f_\mu[x] \right)^2 \right]$
			- ==Variance==
		- $\left( f_\mu[x] - \mu[x] \right)^2$
			- ==Bias==
		- $\sigma^2$
			- ==Noise==
- How to reduce variance?
	- Add more training examples
- How to reduce bias?
	- Increasing model capacity reduces bias
		- In neural networks, more layers and hidden units
- What is bias-variance tradeoff?
	- Reducing bias increases variance
- What is overfitting?
	- Higher capacity models fit training data better, but ground truth worse
		- Capacity used to fit noise
- What is double descent?
	- Test error falls, increases then falls again
	- Large models can make smoother functions
	- ‘Large gaps’ very likely in high-dimensional problems
- What is classical regime or under-parameterised regime?
- What is critical regime?
- What is modern regime or over-parameterised regime?
- What is the curse of dimensionality?
	- ‘Large gaps’ very likely in high-dimensional problems
		- 40-dimensional data
		- 10,000 data points
		- Suppose we quantise input data to 10 bins
		- $10^{40}$ bins - 1 data point per $10^{35}$ bins 
	- ==High-dimensional spaces tend to overwhelm number of data points==
- Why does trained large models end up smooth?
	- Not clear why trained large models end up smooth
		- Potential explanations
			- Initialisation might prefer smooth functions
			- SGD might prefer smooth functions
- What is hyperparameter search?
	- Don’t know bias or variance
	- Don’t know how much capacity needed
	- How do we choose?
		- Network structure
		- Training algorithm and learning rate
	- Solution is the validation set
		- Train models with different hyperparameters using training set
		- Choose best hyperparameters using validation set
		- Test once with the test set
- Why is there a generalisation gap between training and test data?
	- Overfitting (model describes noise)
	- Model unconstrained in areas with no training examples
- What is regularisation?
	- Methods to reduce generalisation gap
		- Technically means adding terms to loss function
		- But in practice means any method (hack) to reduce gap
- What explicit regularisation?
	- Standard loss function
		- $\hat{\phi} = \arg\min_{\phi} \left[ L[\phi] \right]$
		- $= \arg\min_{\phi} \left[ \sum_{i=1}^I \ell_i[\mathbf{x}_i, y_i] \right]$
	- Regularisation adds an extra term
		- $\hat{\phi} = \arg\min_{\phi} \left[ \sum_{i=1}^I \ell_i[\mathbf{x}_i, y_i] + \lambda \cdot g[\phi] \right]$
	- Favours some parameters, disfavours others
	- $\lambda>0$ controls the strength
- What is the probabilistic interpretation?
	- Maximum likelihood
		- $\hat{\phi} = \text{argmax}_{\phi} \prod_{i=1}^I P(y_i | \mathbf{x}_i, \phi)$
	- Regularisation is equivalent to adding a ==prior== over parameters, what you know about params before seeing the data
		- $\hat{\phi} = \text{argmax}_{\phi} [\prod_{i=1}^I P(y_i | \mathbf{x}_i, \phi) P(\phi)]$
	- Explicit regularisation
		- $\hat{\phi} = \text{argmin}_{\phi} \left[ \sum_{i=1}^I \ell_i[\mathbf{x}_i, y_i] + \lambda \cdot g[\phi] \right]$
	- Probabilistic interpretation
		- $\hat{\phi} = \text{argmax}_{\phi} [\prod_{i=1}^I P(y_i | \mathbf{x}_i, \phi) P(\phi)]$
	- Mapping 
		- $\lambda \cdot g[\phi] = -\log[P(\phi)]$
- What are some facts for explicit regularisation?
	- Can only use very general terms
	- Most common is ==L2 regularisation==
	- Favours small params
		- $\hat{\phi} = \text{argmin}_{\phi} \left[ L[\phi, \{\mathbf{x}_i, y_i\}] + \lambda \sum_j \phi_j^2 \right]$
	- Also called ==Tikhonov regularisation, ridge regression==
	- In neural networks, usually just for weights and called ==weight decay==
- What is implicit regularisation?
	- Gradient descent disfavours areas where gradients are steep
		- $\tilde{L}_{GD}[\phi] = L[\phi] + \frac{\alpha}{4} \left\lVert \frac{\partial L}{\partial \phi} \right\rVert^2$
	- SGD likes all batches to have similar gradients
		- $\tilde{L}_{SGD}[\phi] = \tilde{L}_{GD}[\phi] + \frac{\alpha}{4B} \sum_{b=1}^B \left\lVert \frac{\partial L_b}{\partial \phi} - \frac{\partial L}{\partial \phi} \right\rVert^2$
		- $\tilde{L}_{SGD}[\phi] = L[\phi] + \frac{\alpha}{4} \left\lVert \frac{\partial L}{\partial \phi} \right\rVert^2 + \frac{\alpha}{4B} \sum_{b=1}^B \left\lVert \frac{\partial L_b}{\partial \phi} - \frac{\partial L}{\partial \phi} \right\rVert^2$
	- Depends on learning rate, perhaps why larger learning rate regularises better
- What is early stopping?
	- If we stop training early, weights don’t have time to overfit
	- Weights start small, don’t have time to get large
	- Reduces effective model complexity
	- No need for re-training
- What is ==ensembling==?
	- Train several models, average outputs – an ensemble
	- Can take mean or median
	- Different initialisations and/or different models
	- Different subsets of the data resampled with replacement – bagging
- What is dropout?
- What is adding noise?
	- To inputs
	- To weights
	- To outputs (labels)
- What is Bayesian approach?
	- There are many parameters compatible with the data
		- $Pr(\phi \mid \{\mathbf{x}_i, y_i\}) = \frac{\prod_{i=1}^I Pr(y_i \mid \mathbf{x}_i, \phi) Pr(\phi)}{\int \prod_{i=1}^I Pr(y_i \mid \mathbf{x}_i, \phi) Pr(\phi) d\phi}$
		- $\phi$ is prior info about params
	- Can find a probability distribution over them
		- $Pr(y \mid \mathbf{x}, \{\mathbf{x}_i, y_i\}) = \int Pr(y \mid \mathbf{x}, \phi) Pr(\phi \mid \{\mathbf{x}_i, y_i\}) d\phi$
	- Take all possible parameters into account when making prediction
- What is transfer learning?
- What is multi-task learning?
- What is self-supervised learning?
- What is data augmentation?

#### 7: CNNs and Residual Networks (UDL: 10, 11)

- What is invariance? 
	- The function output is the same even after the transformation is applied.
	- $f[T[x]] = f[x]$
- What is equivariance or covariant? 
	- The output is transformed in the same way as the input.
	- $f[T[x]]=T[f[x]]$
- How do convolutional layers work?
	- CNNs only look at local image patches and share the parameters across images
- What are residual connections?
- What is batch normalisation?
- Examples of CNN architectures?
- What are the challenges with images?
	- Size
	- Nearby pixels are statistically related
	- It should be stable under transformations
- What is a kernel or filter?
- What is padding? 
- What is Zero padding?
	- Treat positions that are beyond the end of the input as zero. 
- What is stride?
	- Shift by $k$ positions for each output
	- Decreases size of output relative to input
- CNN use cases
	- Image classification 
		- Multi-class classification problem
	- Object detection 
	- Image segmentation
		- Multivariate binary classification problem
- What is a multi-class classification problem?
	- Discreet classes
	- >2 possible classes
- What is a multivariate binary classification problem?
	- Many outputs
	- Two discreet classes
- What is downsampling - Equivalent to stride = 2
	- Max Pooling - Partial invariance to translation 
	- Mean Pooling
- What is upsampling?
	- Duplicate
	- Max-upsampling
	- Bilinear Interpolation CNN use cases
	- Image classification 
		- Multi-class classification problem
	- Object detection 
	- Image segmentation
		- Multivariate binary classification problem
- What is kernel size?
	- Weight a different number of inputs for each output
	- Combine information from a larger area
	- Kernel size 5 uses five parameters
- What is dilated or atrous convolutions?
	- Intersperse kernel values with zeros
	- Combine information from a larger area
	- Fewer parameters
- What are channels or feature maps?
	- The convolutional operation averages together the inputs + ReLU function
	- Has to lose information
	- So, we apply several convolutions and stack them in channels
- What are receptive fields?
- What happens when we apply CNN on MNIST-1D?
	- Better inductive bias
	- Forced the network to process each location similarly 
	- Share information across locations 
	- Search through a smaller family of input/output mappings, all of which are plausible
- What is inductive bias?
	- Refers to the assumptions a machine learning model makes about the data structure or the target function to generalise from the training data to unseen data.
- What are sequential networks?
- Problems with sequential networks?
	- Deeper networks produce worse results
	- Potential cause: shattered gradients
- What are residual networks?
- What is batch normalisation?
	- Compute mean and standard deviation of input (over batch)
	- Rescale to mean 0, variance 1
	- Scale by $\gamma$, shift by $\delta$ delta(learned)
	- Stabilise forward propagation
		- Even for complicated networks
	- Higher learning rates
		- Smoother loss function landscape
	- Regularisation
		- Adds some noise to batch steps
- What is ResNet?
- What is DenseNet?

#### 9: Sequence Modelling and Transformers (UDL: 12 D2L: 9, 10)

- What is sequential data?
	- Modelling sequential data is very important in various diverse domains
		- Speech, sound synthesis/recognition
		- Time series, market data
		- Video processing
		- Bioinformatics/Chemistry
		- Language/text generation
- What is tokenisation?
	- It involves breaking down text into smaller units, such as words, characters, or subwords
	- It can be performed on different levels:
		- Word-level
		- Character-level
		- Subword-level
		- Sentence-level
- What is vectorisation?
	- Each token gets assigned a unique index that can then get mapped to vector space using a linear embedding layer
	- Similar transformations can be applied to any data to make them suitable for sequence processing models
	- Distances between word vectors in a well-trained model are meaningful
	- Arithmetic operations in this space make intuitive sense
- What are sequence-to-sequence models?
	- Defining property: 
		- It can handle data sequences of different lengths as input/output without adjusting parameters or the DNN architecture
	- Versatility: 
		- It can handle various training objectives 
			- (1) vector-to-sequence (vec2sec)
			- (2) sequence-to-vector (seq2vec)
			- (3) sequence-to-sequence (seq2seq)
			- (4) encoder-decoder
- What is the vec2sec model?
	- Fixed length inputs (image/word/sentence embedding) to variable length outputs 
	- Applications:
		- Image captioning
- What is the sec2sec model?
	- Sequence-to-vector models take a sequence of inputs and convert it to a fixed-length output
	- Applications: 
		- Sentence classification (e.g. sentiment analysis - good/bad review?)
		- Image generation (input: text, output: image vector that gets decoded)
		- Time series forecasting (1 step ahead)
- What is the sec2sec model?
	- Versatile model suitable for various problems (most of the chatbot models)
	- Applications: 
		- Next word/token prediction
		- Machine translation
		- Image generation -> image completion, inpainting, etc.
- What is the encoder-decoder model?
	- A sequence-to-sequence model variant that translates sequential data between different domains (or different sequence lengths)
	- Applications:
		- Language Translation
		- Speech-to-text
		- Text-to-speech
- What are recurrent neural networks?
	- They are inherently sequential and cannot be easily parallelised - need to obtain the previous state of the network before moving on to the next
	- Long-term dependencies are difficult (both for gradients and context awareness)
	- There is no explicit way to model a hierarchy
	- Models do not scale well with GPUs
	- They do still have use cases/upsides
	- Selective state space models (SSSMs) - Mamba: Recurrence making a comeback
- Can we use CNNs for sequences?
	- CNNs can also be used for sequence modelling
		- 1D convolutions
		- Example
			- Sliding a fixed-size convolution kernel along word embedding vectors
		- Often used for genomic data
		- Features: 
			- More parallelisable than RNNs
			- Limited context window
			- Need to stack many layers if input sequences are long and global relationships are important
- What is the transformer architecture?
	- Main novelty – transformer block
	- Great use of attention mechanism
	- Has positional encodings, many residual skip connections
	- Stacking a bunch of transformer blocks on top of each other with residual connections in between
	- Output – $softmax$ probabilities over your vocabulary (words/syllables)
	- It can be either encoder or decoder-based
	- Pros
		- Shallower than RNNs, easier to train
		- Very efficient on GPUs
		- The attention mechanism is key – global context awareness
		- Combines the best aspects of CNNs and RNNs
	- Cons
		- Costly for long sequences
		- Very data-hungry
- What are the key components of a transformer block?
	- Positional encodings/embeddings
	- Self-attention
	- Layer normalisation – scaling outputs (attention + residuals) to have a mean of 0 and a standard deviation of 1
		- Instead of normalising across the whole batch, normalise per sample (along feature dimension)
	- Residual connections - initial embedding added to the output of the self-attention layer
	- Simple feed-forward network – outputs of self-attention are processed individually
- What is the attention mechanism?
	- It was first used within recurrent nets
	- Combines the best of both worlds
		- Global context awareness
		- No vanishing/exploding gradients
		- Very parallelisable
- How to use self-attention as routing?
- What is similarity-based routing?
	- Vector routing depends on their similarity
	- The dot product between vectors - a measure of similarity
- What is the main idea behind self-attention?
	- Introduce model parameters that decompose the input vector embeddings into 3 separate vectors:
		- Queries (Q)
		- Keys (K)
		- Values (V)
	- Terminology comes from information retrieval systems
		- `d = {'a': 1, 'b': 2 }`
		- `d['a']` is the query
		- `a` is the key
		- `1` is the value
	- Queries (Q), Keys (K) and Values (V) are obtained by multiplying with Q, K, and V weight matrices
	- During training, Q and V are mapped to a space where the dot product of these vectors represents co-dependence and similarity between words. This gives the model a lot more flexibility
	- Attention can be viewed as a ’soft dictionary’ or a ‘continuous dictionary’
	- Routing is based on the dot product of query (Q) and key (K) vectors (similarity)
- Explain self-attention step by step.
	- Obtain query $(q)$, key $(k)$ and value $(v)$ vectors
	- The dot product between query and key vectors (using own query vector and all other key vectors)
	- Scale by dividing by $\sqrt{d}$ and apply $softmax$ operation (determines the relative importance of each element)
	- Use these coefficients to scale value vectors: $A(Q, K, V) = \text{softmax}(QK^T)V$
	- Sum up the scaled value vectors to obtain the final output $z$
- What is multi-headed attention?
	- There might be more than one important token-token relationships to attend to
	- Having multiple heads solves this problem
	- A matter of splitting tensors and increasing their rank
	- The end result of all individual heads is concatenated
- Why use positional encodings/embeddings?
	- By default transformers do not have any sense of the token's positioning within a sequence
	- Complete positional equivariance – order of inputs does not matter, unless positional encoding
- What are positional encodings/embeddings?
	- Relative input token positions are encoded by adding some periodical signal to the initial input embeddings:
		- Sum of multiple sine/cosine signals (easily learned by the network)
		- Other complex periodical signals
		- As learnable parameters of the model (embeddings) - most models nowadays 
		- Because of residual connections, this positional signal is transmitted through layers/blocks
- Explain the growth of DL?
	- 2 waves:
		- 1st - CNNs, LSTMs, MLPs (supervised, RL training)
			- Specialised, domain-specific architectures
		- 2nd - Transformers (semi/self-supervised training)
			- A single architecture that does everything
- What is the neural scaling law?
	- Key takeaways from the paper:
		- Larger models need fewer samples to learn
		- Scale is more important than architecture* 
		- Three factors need to be scaled in tandem
		- Smooth power laws (6 orders of magnitude)
		- Loss values are predictable
		- The trend is expected to continue as datasets/GPU technology improves
- What is the main objective of most models?
	- Next token prediction (autoregression)
	- Bidirectional (no masking) - great for labelling/classification (e.g. sentiment), filling in sequences
- What is Reinforcement Learning from Human Feedback (RLHF)?
	- Use human feedback to improve output of the models
	- Iterative loop:
		- Create a dataset of prompts
		- Generate a bunch of model outputs using those prompts (multiple per prompt)
		- Use a team of human evaluators to rank the generated outputs
		- Train a reward model that tries to reproduce the human rankings
		- Use that reward model to auto-rank prompts and fine-tune the model via PPO 
		- Repeat 2-5

#### 10: Unsupervised Learning and GAN (UDL: 14, 15)

- What is the difference between supervised and unsupervised learning?
	- Supervised learning maps data $x$ to labels $y$ based on the example training pairs
	- Unsupervised learning only has the data $x$
		- Examples:
			- Generate new examples
			- Denoise examples
			- Find outliers
			- Understand the structure of data
			- Compress data
			- Manipulate data
			- Interpolate between data points
- What are generative models?
	- Can generate new examples
- What are probabilistic models?
	- Can assign a probability to examples
	- Assign a probability $Pr(x,ϕ)$ to each data point $x$
	- During training, maximise the probability of training examples
		- $L[\phi] = -\sum_{i=1}^{I} \log \left[ Pr(\mathbf{x}_i | \phi) \right]$
- What are latent variable models?
	- Define a mapping between data examples $x$ and latent variables $z$
	- $z$ typically lower dimension than $x$, capture essential information
	- Possibility: map $x$ to $z$
		- E.g., k-means algorithm
	- Possibility: map $z$ to $x$
		- Define a distribution $Pr(z)$ over $z$, e.g. normal
		- Sample from $Pr(z)$ and map to $x$ -> generative models
	- Map a random “latent” variable to create a new data sample
- What are the types of unsupervised generative models?
	- Generative adversarial networks (GANs) (LV)
	- Variational auto-encoders (VAEs) (P, LV)
	- Diffusion models (P, LV)
	- Normalising flows (P, LV)
- What makes a good generative model?
	- Efficient sampling
		- Generating samples should be computationally inexpensive
	- High-quality sampling
		- The samples should be indistinguishable from the real data
	- Coverage
		- Samples should represent the entire training distribution
	- Well-behaved latent space
		- Every latent variable $z$ should correspond to a plausible data example $x$ and smooth changes in $z$ should correspond to smooth changes in $x$
	- Disentangled latent space
		- Manipulating each dimension of $z$ should correspond to changing an interpretable property of the data
	- Efficient likelihood computation
		- If the model is probabilistic, we would like to be able to calculate the probability of new examples efficiently and accurately
- How do we measure the performance of trained models?
	- Test likelihood
		- Measure likelihood $Pr(x,ϕ)$ of holdout test set
		- Can be impossible (GANs)
		- It can be computationally costly (VAEs and diffusion models)
	- Inception score
		- Apply trained classification network to generated samples
		- Output should be
			- Highly peaked for the correct class
			- All classes should be generated
	- Fréchet inception distance
		- Apply trained classification network to generated samples and real examples
		- Measure the difference between distributions of activations in deep layers
	- Manifold precision/recall 
		- Measure the overlap between the data manifold and model manifold 
		- Precision: fraction of generated samples that fall into the data manifold 
		- Recall: fraction of real examples that fall into model manifold 
		- Manifolds estimated by hyperspheres (possibly on activations)
- What are generative adversarial networks?
	- Map random latent variable to new sample 
	- In GANs, this is called the generator
		- $\mathbf{x}_* = \mathbf{g}[\mathbf{z}_i, \boldsymbol{\theta}]$
	- Aim
		- Generate samples ${\mathbf{x}_{j}^*}$ that are from the same distribution as a set of real training data $\mathbf{x}_{i}$
	- Approach
		- Generate sample $\mathbf{x}_{j}$
			- Choose latent variable $\mathbf{z}_{j}$ from a simple distribution (e.g. normal)
			- Pass through generator network $\mathbf{x}_* = \mathbf{g}[\mathbf{z}_i, \boldsymbol{\theta}]$
	- To train, use second network $\mathbf{f}[\bullet, \boldsymbol{\phi}]$, the ==discriminator==, that tries to classify whether input is real or generated
- What is discriminator loss function?
	- Standard cross entropy loss
		- $\hat{\phi} = \arg\min_{\phi} \left[ \sum_j -\log \left( 1 - \text{sig}\left[\mathbf{f}[\mathbf{x}_j^*, \phi]\right] \right) - \sum_i \log \left( \text{sig}\left[\mathbf{f}[\mathbf{x}_i, \phi]\right] \right) \right]$
	- Assume real examples have label 1, generated samples label 0
- What is generator loss function?
	- Why maximise?
	- $\hat{\theta} = \arg\max_{\theta} \left[ \min_{\phi} \left( \sum_j -\log \left( 1 - \text{sig}\left[\mathbf{f}[\mathbf{g}[\mathbf{z}_j, \theta], \phi]\right] \right) - \sum_i \log \left( \text{sig}\left[\mathbf{f}[\mathbf{x}_i, \phi]\right] \right) \right) \right]$
- Why is it called an adversarial network?
	- Generator tries to fool discriminator, discriminator tries to ‘catch’ the generator
- How to train the model?
	- Split into two loss functions 
	- Generator loss
		- $L[\theta] = \sum_j \log \left( 1 - \text{sig} \left[ \mathbf{f}[\mathbf{g}[\mathbf{z}_j, \theta], \phi] \right] \right)$
		- Generated samples should be assigned high probability by discriminator
	- Discriminator loss
		- $L[\phi] = \sum_j -\log \left( 1 - \text{sig} \left[ \mathbf{f}[\mathbf{g}[\mathbf{z}_j, \theta], \phi] \right] \right) - \sum_i \log \left( \text{sig} \left[ \mathbf{f}[\mathbf{x}_i, \phi] \right] \right)$
		- Generated samples should be assigned high probability by discriminator
- What is deep convolutional (DC) GAN?
- What are the difficulties in training a DC GAN?
	- Mode dropping
		- Samples look realistic, but only generates subset (e.g., no beards)
	- Mode collapse
		- Model ignores latent variables, many generated samples look alike
	- Reason 1
		- No penalty for producing only subset of possible data
	- Reason 2
		- Generator gradients vanish if discriminator becomes ‘too good’
- What is Wasserstein GANs?
	- Uses Wasserstein distance as a loss function
		- How much work is needed to change one distribution to another
	- $L[\phi] = \sum_j \mathbf{f}[\mathbf{x}_j^*, \phi] - \sum_i \mathbf{f}[\mathbf{x}_i, \phi] \quad \text{with} \quad \left| \frac{\partial \mathbf{f}[\mathbf{x}, \phi]}{\partial \mathbf{x}} \right| < 1$
	- Discriminator gives score (no sigmoid)
	- Discriminator tries to maximise average score of real examples and minimise average score of generated samples
	- Generator tries to maximise average score of generated samples
	- Ensure small discriminator gradient by clipping weights
- What are the tricks to overcome difficulties training GANs?
	- Progressive growing
	- Mini-batch discrimination
		- Add in statistics across mini-batch as input to discriminator
		- Forces generated batch to have similar variation to real batch
	- Truncation
		- Reject unlikely latent samples
- What is conditional GAN?
	- We want to control what is generated
	- We pass in latent variable and attribute vector to generator
		- Latent variable $\mathbf{z}_j$
		- Attribute vector $\mathbf{c}_{j}$
		- $\mathbf{g}[\mathbf{z}_j, \mathbf{c}_j, \theta]$
- What is auxiliary classifier GAN?
	- We pass in latent variable and class to generator
		- Latent variable $\mathbf{z}_j$
		- Class $c_{j}$
		- $\mathbf{g}[\mathbf{z}_j, c_j, \theta]$
	- We get if probability is real and probability of class from discriminator
		- Probability is real
			- $\text{sig}[f_1[\bullet, \phi]]$
		- Probability of class
			- $\text{softmax}[f_2[\bullet, \phi]]$
- What is InfoGAN?
	- Identify attributes automatically
	- Combination of both auxiliary GAN and conditional GAN
- What is image translation?
	- Translating one data example into another
		- Grayscale image to color image
		- Blurry image to sharp image
		- Sketch to photo-realistic image
	- Pix2Pix
	- SRGAN
	- CycleGAN
		- If we don't have matched pairs
	- StyleGAN

#### 11: RNNs and VAEs (UDL: 17, D2L: 9, 10)

- What is LSTM?
- What are GRUs?
- What are auto encoders?
	- A neural network that takes input $x$ and predicts $x$
	- To make it non-trivial, add a bottleneck layer with a small number of nodes
- Why autoencoders?
	- Map high-dimensional data to two dimensions for visualisation
	- Compression (i.e. reducing the file size)
	- Learn abstract features in an unsupervised way to apply them to a supervised task
	- Learn a semantically meaningful representation where you can, e.g., interpolate between different images
	- Denoising
- What is principle component analysis (PCA)?
	- Simple autoencoder: one hidden layer, linear activations, and squared error loss
	- Network computes $x̂ = UVx$, which is a linear function
	- $V$ maps $x$ to a $K$-dimensional space, so it’s doing dimensionality reduction
- What are nonlinear latent variable models?
	- The probability of output $x$ is given by
		- $Pr(\mathbf{x}) = \int Pr(\mathbf{x} | \mathbf{z}) Pr(\mathbf{z}) d\mathbf{z}$
	- We assume a simple normal distribution for $Pr(\mathbf{z})$
		- $Pr(\mathbf{z}) = \text{Norm}_{\mathbf{z}}[0, \mathbf{I}]$
	- We also assume that the likelihood of $\mathbf{x}$ given $\mathbf{z}$ is a normal distribution
		- $Pr(\mathbf{x} | \mathbf{z}, \phi) = \text{Norm}_{\mathbf{x}} \left[ \mathbf{f}[\mathbf{z}, \phi], \sigma^2 \mathbf{I} \right]$
	- The mean is given by a neural network $\mathbf{f}[\mathbf{z}, \phi]$, capturing important aspects, while the noise $σ$ captures unmodelled aspects
	- Probability of data $\mathbf{x}$ given by
		- $Pr(\mathbf{x} | \phi) = \int Pr(\mathbf{x}, \mathbf{z} | \phi) d\mathbf{z}$
		- $= \int Pr(\mathbf{x} | \mathbf{z}, \phi) \cdot Pr(\mathbf{z}) d\mathbf{z}$
		- $= \int \text{Norm}_{\mathbf{x}} \left[ \mathbf{f}[\mathbf{z}, \phi], \sigma^2 \mathbf{I} \right] \cdot \text{Norm}_{\mathbf{z}} [0, \mathbf{I}] \, d\mathbf{z}$
- How to generate a sample from the given trained network?
	- Sample $\mathbf{z}$ from normal distribution
	- Apply $\mathbf{f}[\mathbf{z}, \phi]$
	- Add noise sampled with variance $\sigma$
- How do we train the network?
	- Assume $\sigma$ is known, and we are given examples $\mathbf{x_{i}}$
	- Maximise the probability of training examples
		- $\hat{\phi} = argmax_{\phi} \left[ \sum_{i=1}^I \log \, Pr(\mathbf{x}_i | \phi) \right]$
		- $Pr(\mathbf{x}_i | \phi) = \int \text{Norm}_{\mathbf{x}_i} \left[ \mathbf{f}[\mathbf{z}, \phi], \sigma^2 \mathbf{I} \right] \cdot \text{Norm}_{\mathbf{z}} [0, \mathbf{I}] \, d\mathbf{z}$
	- Unfortunately, it is not possible to solve this effectively so we define a lower bound on the log-likelihood, and maximise that
- What is Jensen's inequality?
	- To derive lower bound, use Jensen’s inequality
	- For concave function $g$
		- $g[\mathbb{E}[y]] \geq \mathbb{E}[g[y]]$
	- For logarithm
		- $\log[\mathbb{E}[y]] \geq \mathbb{E}[\log[y]]$
- What is evidence lower bound (ELBO)?
	- Define another probability distribution $q(z)$, and multiply and divide
		- $\log[Pr(\mathbf{x} | \phi)] = \log \left[ \int Pr(\mathbf{x}, \mathbf{z} | \phi) \, d\mathbf{z} \right]$
		- $= \log \left[ \int q(\mathbf{z}) \frac{Pr(\mathbf{x}, \mathbf{z} | \phi)}{q(\mathbf{z})} \, d\mathbf{z} \right]$
	- Use Jensen’s inequality
		- $\log \left[ \int q(\mathbf{z}) \frac{Pr(\mathbf{x}, \mathbf{z} | \phi)}{q(\mathbf{z})} \, d\mathbf{z} \right] \geq \int q(\mathbf{z}) \log \left[ \frac{Pr(\mathbf{x}, \mathbf{z} | \phi)}{q(\mathbf{z})} \right] \, d\mathbf{z}$
	- Assume $q(z)$ is parametrised by $φ$
		- $\text{ELBO}[\theta, \phi] = \int q(\mathbf{z} | \theta) \log \left[ \frac{Pr(\mathbf{x}, \mathbf{z} | \phi)}{q(\mathbf{z} | \theta)} \right] \, d\mathbf{z}$
		- $\text{ELBO}[\theta, \phi] = \int q(\mathbf{z} | \theta) \log \left[ Pr(\mathbf{x} | \mathbf{z}, \phi) \right] \, d\mathbf{z} - D_{KL} \left[ q(\mathbf{z} | \theta) \| Pr(\mathbf{z}) \right]$
			- First term: reconstruction loss
				- Data example should have high probability
			- Second term: measure difference (KL divergence) between distribution $q(\mathbf{z}|\boldsymbolθ)$ and normal distribution
				- Variational distribution should be similar to prior
	- This is called the evidence lower bound (ELBO) 
	- We will maximise this over $θ$ and $φ$ – the result is a VAE
- What is a VAE?
	- It is possible to show that lower bound is tight when $q(z|θ)$ is equal to $Pr(z|x, φ)$ – i.e. distribution over $z$ that can be responsible for $x$ 
	- $Pr(z|x, φ)$ is difficult to compute!
	- Make $q(z|θ)$ a normal distribution, and use a neural network to estimate parameters for this distribution:
		- $q(\mathbf{z} | \mathbf{x}, \theta) = \text{Norm}_\mathbf{z} \left[ \mathbf{g}_\mu[\mathbf{x}, \theta], \mathbf{g}_\Sigma[\mathbf{x}, \theta] \right]$
		- $\mathbf{g}_\mu[\mathbf{x}, \theta]$ represents the mean function
		- $\mathbf{g}_\Sigma[\mathbf{x}, \theta]$ represents the covariance function
- How do we approximate sample probability?
	- To approximate probability of sample $x$, can compute average
		- $Pr(\mathbf{x}) \approx \frac{1}{N} \sum_{n=1}^N Pr(\mathbf{x} | \mathbf{z}_n)$
	- Problem: very likely randomly drawn $z$ will have low probability
	- Solution: sample from distribution produced by encoder, and weight accordingly
- How to generate samples?
	- Sample from normal distribution, apply decoder, add noise
	- Unfortunately, often low-quality
	- Tricks are usually needed to generate high-quality samples
		- One trick: don’t add noise – often blurry
		- Another trick: sample from aggregated posterior
		- $q(\mathbf{z}|\mathbf{\theta}) = \frac{1}{I} \sum_i q(\mathbf{z}|\mathbf{x}_i, \mathbf{\theta})$
- What is resynthesis?

#### 12: Diffusion (UDL: 18)

- Why diffusion?
	- State of the art image generation technique
	- Currently the most popular generative model for images, mostly because of the high quality of the outputs
	- Math fairly similar to VAEs
	- Decoder that generates samples from latent variables
	- We also have an encoder, but this encoder is pre-specified
	- All learning is happening in the decoder
- Give an overview of diffusion.
	- Encoder
		- Take sample $x$ and map it through series of intermediate latent variables $z_{1} , …, z_{T}$
		- Pre-specified
			- $\mathbf{z}_1 = \sqrt{1 - \beta_1} \cdot \mathbf{x} + \sqrt{\beta_1} \cdot \boldsymbol{\epsilon}_1$
			- $\mathbf{z}_t = \sqrt{1 - \beta_t} \cdot \mathbf{z}_{t-1} + \sqrt{\beta_t} \cdot \boldsymbol{\epsilon}_t \quad \forall \; t \in \{2, \dots, T\}$
			- $ϵ_{t}$ is noise drawn from standard normal distribution
			- $\beta_{t} \in [0,1]$ determines how quickly noise is added, called the noise schedule, and can influence results
		- Distribution of the latent variables given sample $x$
			- $q(\mathbf{z}_1 | \mathbf{x}) = \text{Norm}_{\mathbf{z}_1} \left[ \sqrt{1 - \beta_1} \mathbf{x}, \beta_1 \mathbf{I} \right]$
			- $q(\mathbf{z}_t | \mathbf{z}_{t-1}) = \text{Norm}_{\mathbf{z}_t} \left[ \sqrt{1 - \beta_t} \mathbf{z}_{t-1}, \beta_t \mathbf{I} \right]$
			- Note: $z_{t}$ only depends on $z_{t-1}$
			- After many steps, $q(\mathbf{z}_T | \mathbf{x}) = q(\mathbf{z}_T)$ becomes a standard normal distribution
	- Decoder
		- Take latent variable $z_{T}$ and map back through $z_{T-1} , …, z_{1}$ to finally create sample $x$
		- In principle, we want to use to go from $z_T$ through $z_{T-1}$, $z_{T-2}$, $z_{T-3}$, … , until we obtain a realistic $x$
		- Impossible to compute, but assume well approximated by normal distribution
		- We approximate the distributions with normal distributions with means given by neural networks
			- $Pr(\mathbf{z}_T) = \text{Norm}_{\mathbf{z}_T} \left[ 0, \mathbf{I} \right]$
			- $Pr(\mathbf{z}_{t-1} | \mathbf{z}_t, \phi_t) = \text{Norm}_{\mathbf{z}_{t-1}} \left[ f_t[\mathbf{z}_t, \phi_t], \sigma_t^2 \mathbf{I} \right]$
			- $Pr(\mathbf{x} | \mathbf{z}_1, \phi_1) = \text{Norm}_{\mathbf{x}} \left[ f_1[\mathbf{z}_1, \phi_1], \sigma_1^2 \mathbf{I} \right]$
		- To train, maximise probability of training examples
			- $Pr(\mathbf{x}, \mathbf{z}_{1:T} | \phi_{1:T}) = Pr(\mathbf{x} | \mathbf{z}_1, \phi_1) \prod_{t=2}^T Pr(\mathbf{z}_{t-1} | \mathbf{z}_t, \phi_t) \cdot Pr(\mathbf{z}_T)$
			- $Pr(\mathbf{x} | \phi_{1:T}) = \int Pr(\mathbf{x}, \mathbf{z}_{1:T} | \phi_{1:T}) d\mathbf{z}_{1:T}$
			- $\hat{\phi}_{1:T} = \arg\max_{\phi_{1:T}} \left[ \sum_{i=1}^I \log Pr(\mathbf{x}_i | \phi_{1:T}) \right]$
			- But this is intractable
		- So use ELBO
			- $\log Pr(\mathbf{x} | \phi_{1:T}) = \log \left[ \int Pr(\mathbf{x}, \mathbf{z}_{1:T} | \phi_{1:T}) d\mathbf{z}_{1:T} \right]$
			- $= \log \left[ \int q(\mathbf{z}_{1:T} | \mathbf{x}) \frac{Pr(\mathbf{x}, \mathbf{z}_{1:T} | \phi_{1:T})}{q(\mathbf{z}_{1:T} | \mathbf{x})} d\mathbf{z}_{1:T} \right]$
			- $\geq \int q(\mathbf{z}_{1:T} | \mathbf{x}) \log \left[ \frac{Pr(\mathbf{x}, \mathbf{z}_{1:T} | \phi_{1:T})}{q(\mathbf{z}_{1:T} | \mathbf{x})} \right] d\mathbf{z}_{1:T}$
			- $\text{ELBO}[\phi_{1:T}] = \int q(\mathbf{z}_{1:T} | \mathbf{x}) \log \left[ \frac{Pr(\mathbf{x}, \mathbf{z}_{1:T} | \phi_{1:T})}{q(\mathbf{z}_{1:T} | \mathbf{x})} \right] d\mathbf{z}_{1:T}$
		- $\text{ELBO}[\phi_{1:T}] \approx \int q(\mathbf{z}_{1:T} | \mathbf{x}) \left( \log Pr(\mathbf{x} | \mathbf{z}_1, \phi_1) + \sum_{t=2}^T \log \frac{Pr(\mathbf{z}_{t-1} | \mathbf{z}_t, \phi_t)}{q(\mathbf{z}_{t-1} | \mathbf{z}_t, \mathbf{x})} \right) d\mathbf{z}_{1:T}$
		- $\text{ELBO}[\phi_{1:T}] = \mathbb{E}_{q(\mathbf{z}_{1} | \mathbf{x})} \left[ \log Pr(\mathbf{x} | \mathbf{z}_1, \phi_1) \right] - \sum_{t=2}^T \mathbb{E}_{q(\mathbf{z}_t | \mathbf{x})} \left[ D_{KL} \left[ q(\mathbf{z}_{t-1} | \mathbf{z}_t, \mathbf{x}) || Pr(\mathbf{z}_{t-1} | \mathbf{z}_t, \phi_t) \right] \right]$
			- First term: reconstruction error
				- Make training examples more likely
			- Second terms: make learned distribution look like $q(z_{t-1}|z_{t}, \mathbf{x})$
				- Both known normal distributions, so can compute!
		- Loss function based on ELBO
			- $L[\phi_{1 \dots T}] = \sum_{i=1}^I \left( -\log \text{Norm}_{\mathbf{x}_i} \left[ f_1[\mathbf{z}_{i1}, \phi_1], \sigma^2 \mathbf{I} \right] \right)+ \sum_{t=2}^T \frac{1}{2 \sigma_t^2} \left\| \frac{1 - \alpha_{t-1}}{1 - \alpha_t} \sqrt{1 - \beta_t} \mathbf{z}_{it} + \frac{\sqrt{\alpha_{t-1}} \beta_t}{1 - \alpha_t} \mathbf{x}_i - f_t[\mathbf{z}_{it}, \phi_t] \right\|^2$
			- Reconstruction term
				- $-\log \text{Norm}_{\mathbf{x}_i} \left[ f_1[\mathbf{z}_{i1}, \phi_1], \sigma^2 \mathbf{I} \right]$
			- Target mean of $q(z_{t-1}|z_{t}, x)$
				- $\frac{1 - \alpha_{t-1}}{1 - \alpha_t} \sqrt{1 - \beta_t} \mathbf{z}_{it} + \frac{\sqrt{\alpha_{t-1}} \beta_t}{1 - \alpha_t} \mathbf{x}_i$
			- Predicted $z_{t-1}$
				- $f_t[\mathbf{z}_{it}, \phi_t]$
- What is a diffusion kernel?
	- During training, will need to sample many latent variables $z_{t}$
	- Currently, will need $t$ steps to generate each $z_{t}$ – slow! 
	- Solution: derive closed-form expression for $z_{t}$
- How to derive closed-form expression for $z_t$
	- $\mathbf{z}_1 = \sqrt{1 - \beta_1} \cdot \mathbf{x} + \sqrt{\beta_1} \cdot \boldsymbol{\epsilon}_1$
	- $\mathbf{z}_2 = \sqrt{1 - \beta_2} \cdot \mathbf{z}_1 + \sqrt{\beta_2} \cdot \boldsymbol{\epsilon}_2$
	- $\mathbf{z}_2 = \sqrt{1 - \beta_2} \left( \sqrt{1 - \beta_1} \cdot \mathbf{x} + \sqrt{\beta_1} \cdot \boldsymbol{\epsilon}_1 \right) + \sqrt{\beta_2} \cdot \boldsymbol{\epsilon}_2$
		- Last two terms sum of normal distribution samples, can combine
		- $\mathbf{z}_2 = \sqrt{(1 - \beta_2)(1 - \beta_1)} \cdot \mathbf{x} + \sqrt{\left(1 - \beta_2\right)(1 -\beta_1)} \cdot \boldsymbol{\epsilon}$
	- This generalises to general $t$
		- $\mathbf{z}_t = \sqrt{\alpha_t} \cdot \mathbf{x} + \sqrt{1 - \alpha_t} \cdot \boldsymbol{\epsilon}, \quad \alpha_t = \prod_{s=1}^t (1 - \beta_s)$
		- $q(\mathbf{z}_t | \mathbf{x}) = \text{Norm}_{\mathbf{z}_t} \left[ \sqrt{\alpha_t} \cdot \mathbf{x}, \, (1 - \alpha_t) \mathbf{I} \right]$
- What is marginal distribution?
	- $q(z_t)$: probability of observing $z_{t}$ given all possible starting points $x$ and possible diffusion paths
		- $q(\mathbf{z}_t) = \int q(\mathbf{z}_t | \mathbf{x}) \cdot Pr(\mathbf{x}) \, d\mathbf{x}$
	- Can compute by sampling from $Pr(x)$ and computing $q(z_t |x)$
		- Only feasible for small/simple problems
- How to achieve the reverse process?
	- We know the forward process, going from $x$ to $z_{T}$
	- We want to reverse this process, going from $z_{T}$ samples to $x$
		- By definition, $q(z_T)$ is normal distribution, so easy to sample
	- Which distribution(s) do we need to perform this reverse process?
		- $q(\mathbf{z}_{t-1}|\mathbf{z}_t) = \frac{q(\mathbf{z}_t|\mathbf{z}_{t-1}) q(\mathbf{z}_{t-1})}{q(\mathbf{z}_t)}$
	- Unfortunately, intractable and too costly to evaluate numerically
	- But, for small $β$s and many steps, well approximated by a normal distribution
	- We will use this for training
	- If a specific $x$ is known, reverse distribution can be computed
	- $q(\mathbf{z}_{t-1}|\mathbf{z}_t, \mathbf{x}) = \text{Norm}_{\mathbf{z}_{t-1}} \left[ \left( \frac{(1 - \alpha_{t-1})}{1 - \alpha_t} \sqrt{1 - \beta_t} \mathbf{z}_t + \frac{\sqrt{\alpha_{t-1} \beta_t}}{1 - \alpha_t} \mathbf{x} \right), \frac{\beta_t (1 - \alpha_{t-1})}{1 - \alpha_t} \mathbf{I} \right]$
- What is reparameterisation?
	- This works well, but turns out to work better with reparameterisation
	- In principle, we want the model to predict the noise that was mixed
	- Instead of a network predicting the mean latent $\hat{z}_{t-1} = f_t[z_t, \phi_t]$
	- Use a network to predict mixed noise $\hat{\epsilon} = g_t[z_t, \phi_t]$
	- $f_t[z_t, \phi_t] = \frac{1}{\sqrt{1 - \beta_t}} z_t - \frac{\beta_t}{\sqrt{1 - \alpha_t} \sqrt{1 - \beta_t}} g_t[z_t, \phi_t]$
	- Substituting this in the loss function and simplifying
		- $L[\phi_{1 \dots T}] = \sum_{i=1}^I \sum_{t=1}^T \left\| g_t \left[ \sqrt{\alpha_t} \cdot x_i + \sqrt{1 - \alpha_t} \cdot \epsilon_{it}, \phi_t \right] - \epsilon_{it} \right\|^2$
- Can we do conditional generation here?
	- Similar to GANs, want to control the generation
	- One solution: classifier guidance
		- $\mathbf{z}_{t-1} = \hat{\mathbf{z}}_{t-1} + \sigma_t^2 \frac{\partial \log \left[ P(c | \mathbf{z}_t) \right]}{\partial \mathbf{z}_t} + \sigma_t \boldsymbol{\epsilon}$
	- In each reverse step, make class $c$ more likely

---
