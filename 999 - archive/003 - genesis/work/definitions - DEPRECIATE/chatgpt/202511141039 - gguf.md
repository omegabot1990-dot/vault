---
tags:
  - work
  - academic
aliases:
  - GGUF
title: gguf
description: introduction to gguf
parent nodes:
  - "[[definitions]]"
child nodes:
annotation-target:
---

# üóÇ **What is GGUF?**

**GGUF (GPT-Generated Unified Format)** is a modern model format created by **llama.cpp** for efficient loading and inference of LLMs on CPUs, GPUs, and edge devices.

It is the successor to **GGML** and **GGJT**.

### üß© Why GGUF exists

Older formats (GGML) were limited in metadata and scaling to large models.  
GGUF solves that with:

- Better metadata
    
- Better tensor layout
    
- Standardized quantisation schemes (Q2, Q3, Q4, Q5, Q8‚Ä¶)
    
- Fast loading, fast inference
    
- Cross-compatibility (Ollama, LM Studio, llama.cpp, Kobold, etc.)
    

### ‚úîÔ∏è What GGUF contains

- Model weights (quantised or FP16)
    
- Tokenizer vocabulary (BPE/SentencePiece/etc.)
    
- Special tokens (BOS, EOS, PAD)
    
- Architecture metadata (layers, dims, RoPE scaling, etc.)
    

### ‚úîÔ∏è Why GGUF is popular

- Ideal for **local LLMs**
    
- Works offline
    
- Supports ultra-small models (2‚Äì8 bits)
    
- Fast inference on laptops/CPUs (no GPU required)
    
- Easy to distribute (single file)
    

### ‚úîÔ∏è Common GGUF quantisation types

|Name|Bits|Notes|
|---|---|---|
|**Q8_0**|8-bit|Best quality|
|**Q6_K**|6-bit|Sweet spot quality/performance|
|**Q5_K**|5-bit|Strong mix of speed + accuracy|
|**Q4_K_M / Q4_K_S**|4-bit|Most used for laptops|
|**Q3_K / Q2_K**|2‚Äì3 bit|For extreme low hardware, accuracy drop|

(K types = grouped quantisation with per-channel scaling, higher quality.)

---
# üß† Summary (for notes)

- **Quantisation** reduces precision ‚Üí smaller, faster models
    
- Methods: **INT8, INT4, QAT, GPTQ, AWQ, KV-cache quantisation**
    
- **GGUF** = modern llama.cpp model format with integrated quantisation
    
- Optimal quantisations for you: **Q4_K_M**, **Q5_K_M**, **Q6_K**
    

---
