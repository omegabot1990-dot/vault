---
tags:
  - work
aliases:
  - Tensor
title: tensor
description: what is a tensor
parent nodes:
  - "[[definitions]]"
child nodes:
annotation-target:
---

# ğŸ§ƒ **A tensor = just a box of numbers. Thatâ€™s it.**

But the box can have different shapes.

Seriously â€” thatâ€™s the whole thing.

The word â€œtensorâ€ sounds like some Avengers-level math concept, but in PyTorch/NumPy it just means:

> **A container that stores numbers in multiple dimensions.**

---

# ğŸ“¦ Levels of "tensors" (the easiest way to understand)

## **1D â†’ a list**

Example:

```python
[1, 2, 3]
```

This is a **1-dimensional tensor** (`shape = (3,)`).

---

## **2D â†’ a grid** (like Excel rows Ã— columns)

Example:

```python
[[1, 2, 3],
 [4, 5, 6]]
```

This is a **2D tensor** (`shape = (2, 3)`).

---

## **3D â†’ a stack of grids**

Imagine stacking multiple spreadsheets:

```
Layer 1:
[[1 2 3]
 [4 5 6]]

Layer 2:
[[7 8 9]
 [0 1 2]]
```

Shape is `(2, 2, 3)` â†’ 2 layers, each 2Ã—3.

Thatâ€™s a **3D tensor**.

---

## **4D â†’ a video / batch of images**

Shapes like `(batch, channels, height, width)`.

Example:

```
(32, 3, 224, 224)
```

- 32 images
    
- 3 color channels (RGB)
    
- each 224Ã—224 pixels
    

---

## **5D and higher â†’ same idea, just more axes.**

Transformers often use:

```
(batch, heads, sequence_length, dimension)
```

Still: just a fancier box of numbers.

---

# ğŸ§  Why call it a â€œtensorâ€ at all?

Mostly tradition.

- Physicists used the word first.
    
- Mathematicians used it later.
    
- Deep learning inherited it.
    

But for you as a programmer, **itâ€™s just a multi-dimensional array**.

---

# ğŸ”¥ The real important part: **The shape.**

Every tensor has a `.shape` that tells you its structure.

Example:

```python
X = torch.randn(2, 3, 4)
print(X.shape)
```

Output:

```
torch.Size([2, 3, 4])
```

This means:

- 2 batches
    
- 3 tokens per batch
    
- 4 numbers (features) per token
    

Knowing how to read shapes is **90% of understanding transformers**.

---

# ğŸ§© Why shapes matter so much in attention

Because attention has steps like:

```
Q: (batch, seq_len, d_k)
K: (batch, seq_len, d_k)
scores = Q @ Káµ€   â†’ (batch, seq_len, seq_len)
```

Every operation changes or combines shapes.

You start to â€œfeelâ€ it like Lego pieces that must fit.

---

# ğŸ”¥ 2D Tensor

Example:

```
2D tensor (2x3)
+-----+-----+-----+
|  1  |  2  |  3  |
+-----+-----+-----+
|  4  |  5  |  6  |
+-----+-----+-----+
```

---
