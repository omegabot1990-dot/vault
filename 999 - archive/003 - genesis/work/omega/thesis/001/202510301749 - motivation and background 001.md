---
tags:
  - work
aliases:
  - motivation and background 001
title: motivation and background 001
description:
parent nodes:
  - "[[202510300118 - proposal 001]]"
child nodes:
annotation-target:
---

- Mental health awareness is growing, yet access to therapy is limited by stigma, cost, and availability.
    
- Current large language models (LLMs), such as GPT-4 and Gemini, show promise as companions but are too large, opaque, and data-hungry to be deployed ethically in sensitive domains like mental health.
		
	- Most mental-health chat agents rely on large black-box LLMs that are hard to deploy privately and often “sound empathetic” without **verifiable reasoning**.
    
- A **smaller, edge-deployable, privacy-preserving LLM**, fine-tuned to support journaling, self-reflection, and early emotional awareness, could bridge this gap.
    
- **Goal:** Build a compact, interpretable, and ethically aligned LLM that acts as a psychological assistant — not a therapist — to help users reflect, identify emotions, and explore coping mechanisms.
		
	- A **small reasoning model** that, 
			
		- (a) shows structured, auditable thinking via an internal scratchpad and verifier
			
		- (b) runs on edge devices
			 
		- (c) remains firmly _non-diagnostic_, focused on **psychoeducation, journaling, and emotion exploration**
	

---

==Why do you think LLMs are not being used?==