---
tags:
  - work
aliases:
  - objectives 001
title: objectives 001
description:
parent nodes:
  - "[[202510300118 - proposal 001|proposal 001]]"
child nodes:
annotation-target:
---

- Train or fine-tune a **compact LLM** architecture (e.g., Phi-2, Mistral-7B-instruct, TinyLlama, Qwen-1.5B) using emotion-focused dialogues and psychological literature.
	
	- Keep the base model small (Phi-2/TinyLlama/1–3B class) with **PEFT/LoRA** and **int8/4-bit** inference.
		
- Build a **reasoning-centric pipeline**: **Planner (task decomposition)** → **Solver (response generation)** → **Checker (factuality/safety/consistency)**.
    
- Implement a **RAG pipeline** using curated resources (psychoeducational materials, CBT/DBT worksheets, journaling prompts).
	
- Add **state/symptom classification** (excluding conditions) and a **recommendation layer** (optionally NCF) for coping strategies.
    
- Integrate **multilingual support** (English + Dutch or Malayalam) using transfer learning or mT5/mBERT embeddings.
    
- Explore **edge deployment**: quantisation, distillation, and LoRA adapters for efficient inference.
    
- Integrate **Federated Learning** to allow decentralised learning of emotion classification patterns.
	
	- **Federated** updates for classifiers and recommender signals.
	    
- Validate via small-scale user studies measuring **helpfulness, empathy, emotional alignment**, and **energy efficiency**.
	
	- A clear **human-subjects validation** and **ethics** protocol.
	    
