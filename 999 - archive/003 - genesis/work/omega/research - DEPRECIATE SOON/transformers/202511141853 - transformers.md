---
tags:
  - work
  - academic
aliases:
  - Transformers
title: transformers
description: all things transformers
parent nodes:
  - "[[003 - genesis/work/omega/research - DEPRECIATE SOON/transformers/transformers]]"
child nodes:
annotation-target:
---

## Questions

1. **What is a hypernetwork?**  
    A network that generates the **weights** of another network dynamically, enabling parameter sharing and adaptive model behaviour.
    
    - Example: Attention weights can act as dynamically generated parameters for another layer.
        
2. **What is SQuAD?**  
    The **Stanford Question Answering Dataset** — a benchmark for machine reading comprehension, where models answer questions based on Wikipedia passages.
    
3. **What is SuperGLUE?**  
    A **benchmark suite** that evaluates general language understanding across diverse NLP tasks (e.g., entailment, coreference, question answering), designed as a harder successor to GLUE.
    
4. **What is BIG-bench?**  
    **Beyond the Imitation Game Benchmark**, a large-scale collaborative evaluation suite with 200+ diverse tasks to test reasoning, understanding, and generalisation in large language models.
    
5. **What is the SentencePiece library?**  
    A **tokeniser** that performs unsupervised text segmentation (subword units) using BPE or Unigram models, independent of language or whitespace — used in models like T5 and BERT.
    
6. **What is teacher forcing?**  
    A **training strategy for sequence models** (e.g., RNNs) where the model receives the _ground truth_ previous token instead of its own predicted token, stabilising early training.
    
7. **What is beam search?**  
    A **decoding algorithm** that keeps the top _k_ most probable sequences at each step, balancing between greedy decoding and exhaustive search for better output quality.
    
8. **What is top-K sampling?**  
    A **stochastic decoding method** where only the _K_ highest-probability tokens are considered at each step, and one is sampled proportionally to its probability.
    
9. **What is nucleus sampling?**  
    Also called **top-p sampling** — dynamically selects the smallest set of tokens whose cumulative probability ≥ _p_, then samples from that subset, adapting to uncertainty in predictions.
    
10. **What are squeeze-and-excitation networks?**  
    CNN modules that **adaptively recalibrate channel weights** by “squeezing” global spatial information and “exciting” important feature maps to improve representational power.
    
11. **What is memory-compressed attention?**  
    A **scaled-down attention mechanism** that applies strided convolution to keys and values, reducing sequence length and memory cost by attending over compressed representations.
    
12. **What is local attention?**  
    A **restricted attention pattern** where tokens attend only to neighbouring blocks, improving efficiency for long sequences at the cost of global context.
    
13. **What is learning rate warm-up?**  
    A **training technique** where the learning rate starts small and gradually increases for a few steps or epochs, helping stabilise training in deep models before full-rate optimisation.
    
