---
tags:
  - work
  - academic
aliases:
  - An Introduction to Transformers
title: an introduction to transformers
description: an introduction to transformers
parent nodes:
  - "[[003 - genesis/work/omega/research - DEPRECIATE SOON/transformers/transformers]]"
child nodes:
annotation-target: an_introduction_to_transformers.pdf
---




>%%
>```annotation-json
>{"created":"2025-11-07T21:00:31.299Z","updated":"2025-11-07T21:00:31.299Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":160,"end":291},{"type":"TextQuoteSelector","exact":"The transformer is a neural network component that can be used to learn useful represen-tations of sequences or sets of data-points","prefix":"dge, UKret26@cam.ac.ukAbstract. ","suffix":" [Vaswani et al., 2017]. The tra"}]}]}
>```
>%%
>*%%PREFIX%%dge, UKret26@cam.ac.ukAbstract.%%HIGHLIGHT%% ==The transformer is a neural network component that can be used to learn useful represen-tations of sequences or sets of data-points== %%POSTFIX%%[Vaswani et al., 2017]. The tra*
>%%LINK%%[[#^mt6zq3je1p|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^mt6zq3je1p


>%%
>```annotation-json
>{"created":"2025-11-07T21:03:04.523Z","text":"A mode of behaviour or way of thought peculiar to an individual","updated":"2025-11-07T21:03:04.523Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":801,"end":814},{"type":"TextQuoteSelector","exact":"idiosyncratic","prefix":"nents of the transformer can be ","suffix":". In this note we aim for a math"}]}]}
>```
>%%
>*%%PREFIX%%nents of the transformer can be%%HIGHLIGHT%% ==idiosyncratic== %%POSTFIX%%. In this note we aim for a math*
>%%LINK%%[[#^t4dza3bwz8k|show annotation]]
>%%COMMENT%%
>A mode of behaviour or way of thought peculiar to an individual
>%%TAGS%%
>
^t4dza3bwz8k


>%%
>```annotation-json
>{"created":"2025-11-07T21:25:36.756Z","text":"Autoregressive prediction is when a model generates the next element in a sequence based on all the elements it has already produced or seen","updated":"2025-11-07T21:25:36.756Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":4283,"end":4308},{"type":"TextQuoteSelector","exact":"auto-regressiveprediction","prefix":"representations can be used for ","suffix":" of the next (n+1)th token, glob"}]}]}
>```
>%%
>*%%PREFIX%%representations can be used for%%HIGHLIGHT%% ==auto-regressiveprediction== %%POSTFIX%%of the next (n+1)th token, glob*
>%%LINK%%[[#^avx7uzfcixl|show annotation]]
>%%COMMENT%%
>Autoregressive prediction is when a model generates the next element in a sequence based on all the elements it has already produced or seen
>%%TAGS%%
>
^avx7uzfcixl


>%%
>```annotation-json
>{"created":"2025-11-07T21:55:11.382Z","text":"Global classification (by pooling) means you compress all token representations into one vector (using mean/max/CLS pooling) and use that single vector to classify the entire input sequence at once","updated":"2025-11-07T21:55:11.382Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":4337,"end":4425},{"type":"TextQuoteSelector","exact":"lobal classification of the entire sequence(by pooling across the whole representation),","prefix":"ion of the next (n+1)th token, g","suffix":" sequence-to-sequence or image-t"}]}]}
>```
>%%
>*%%PREFIX%%ion of the next (n+1)th token, g%%HIGHLIGHT%% ==lobal classification of the entire sequence(by pooling across the whole representation),== %%POSTFIX%%sequence-to-sequence or image-t*
>%%LINK%%[[#^d2a0lb0byi5|show annotation]]
>%%COMMENT%%
>Global classification (by pooling) means you compress all token representations into one vector (using mean/max/CLS pooling) and use that single vector to classify the entire input sequence at once
>%%TAGS%%
>
^d2a0lb0byi5



>%%
>```annotation-json
>{"created":"2025-11-08T11:03:04.256Z","updated":"2025-11-08T11:03:04.256Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":1271,"end":1395},{"type":"TextQuoteSelector","exact":"The input to a transformer is N vectorsx(0)n which are each D dimensional. These canbe collected together into an array X(0)","prefix":"5  [cs.LG]  8 Feb 2024Figure 1: ","suffix":".1 Strictly speaking, the collec"}]}]}
>```
>%%
>*%%PREFIX%%5  [cs.LG]  8 Feb 2024Figure 1:%%HIGHLIGHT%% ==The input to a transformer is N vectorsx(0)n which are each D dimensional. These canbe collected together into an array X(0)== %%POSTFIX%%.1 Strictly speaking, the collec*
>%%LINK%%[[#^nrctbn10tre|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^nrctbn10tre


>%%
>```annotation-json
>{"created":"2025-11-08T11:03:17.182Z","updated":"2025-11-08T11:03:17.182Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":1481,"end":1515},{"type":"TextQuoteSelector","exact":"ransformercan handle them as a set","prefix":" need to have an order and the t","suffix":" (where order does notmatter), r"}]}]}
>```
>%%
>*%%PREFIX%%need to have an order and the t%%HIGHLIGHT%% ==ransformercan handle them as a set== %%POSTFIX%%(where order does notmatter), r*
>%%LINK%%[[#^emr2bcrfdgm|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^emr2bcrfdgm


>%%
>```annotation-json
>{"created":"2025-11-08T11:03:38.057Z","updated":"2025-11-08T11:03:38.057Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":3242,"end":3334},{"type":"TextQuoteSelector","exact":"embeddings can be fixed or they can be learned with the rest of the pa-rameters of the model","prefix":" can bemapped into a vector.The ","suffix":" e.g. the vectors representing w"}]}]}
>```
>%%
>*%%PREFIX%%can bemapped into a vector.The%%HIGHLIGHT%% ==embeddings can be fixed or they can be learned with the rest of the pa-rameters of the model== %%POSTFIX%%e.g. the vectors representing w*
>%%LINK%%[[#^ubbqfusz7l9|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ubbqfusz7l9


>%%
>```annotation-json
>{"created":"2025-11-08T11:17:23.366Z","text":"Made for a particular customer or user","updated":"2025-11-08T11:17:23.366Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":3656,"end":3663},{"type":"TextQuoteSelector","exact":"bespoke","prefix":"licable rather than requiring a ","suffix":" architectures for each modality"}]}]}
>```
>%%
>*%%PREFIX%%licable rather than requiring a%%HIGHLIGHT%% ==bespoke== %%POSTFIX%%architectures for each modality*
>%%LINK%%[[#^93p7tls0ve|show annotation]]
>%%COMMENT%%
>Made for a particular customer or user
>%%TAGS%%
>
^93p7tls0ve


>%%
>```annotation-json
>{"created":"2025-11-08T11:17:42.293Z","updated":"2025-11-08T11:17:42.293Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":3994,"end":4139},{"type":"TextQuoteSelector","exact":"transformer will ingest the input data X(0) and return a representation ofthe sequence in terms of another matrix X(M) which is also of size D ×N","prefix":"representations of sequencesThe ","suffix":".The slice xn = X(M):,n will be "}]}]}
>```
>%%
>*%%PREFIX%%representations of sequencesThe%%HIGHLIGHT%% ==transformer will ingest the input data X(0) and return a representation ofthe sequence in terms of another matrix X(M) which is also of size D ×N== %%POSTFIX%%.The slice xn = X(M):,n will be*
>%%LINK%%[[#^p25ggxtb6q|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^p25ggxtb6q


>%%
>```annotation-json
>{"created":"2025-11-08T11:17:52.033Z","updated":"2025-11-08T11:17:52.033Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":4140,"end":4243},{"type":"TextQuoteSelector","exact":"The slice xn = X(M):,n will be a vector of features representing the sequence atthe location of token n","prefix":"X(M) which is also of size D ×N.","suffix":". These representations can be u"}]}]}
>```
>%%
>*%%PREFIX%%X(M) which is also of size D ×N.%%HIGHLIGHT%% ==The slice xn = X(M):,n will be a vector of features representing the sequence atthe location of token n== %%POSTFIX%%. These representations can be u*
>%%LINK%%[[#^mu0e9kn51m|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^mu0e9kn51m


>%%
>```annotation-json
>{"created":"2025-11-08T11:19:36.725Z","updated":"2025-11-08T11:19:36.725Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":4668,"end":4700},{"type":"TextQuoteSelector","exact":"X(m) = transformer-block(X(m−1))","prefix":"vely applyinga transformer block","suffix":".The block itself comprises two "}]}]}
>```
>%%
>*%%PREFIX%%vely applyinga transformer block%%HIGHLIGHT%% ==X(m) = transformer-block(X(m−1))== %%POSTFIX%%.The block itself comprises two*
>%%LINK%%[[#^l7ltzehyk8s|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^l7ltzehyk8s


>%%
>```annotation-json
>{"created":"2025-11-08T11:24:36.232Z","updated":"2025-11-08T11:24:36.232Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":4741,"end":4810},{"type":"TextQuoteSelector","exact":"ne operating across the sequence and oneoperating across the features","prefix":"k itself comprises two stages: o","suffix":". The first stage refines each f"}]}]}
>```
>%%
>*%%PREFIX%%k itself comprises two stages: o%%HIGHLIGHT%% ==ne operating across the sequence and oneoperating across the features== %%POSTFIX%%. The first stage refines each f*
>%%LINK%%[[#^ljr6dr9duw7|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ljr6dr9duw7


>%%
>```annotation-json
>{"created":"2025-11-08T11:24:50.505Z","updated":"2025-11-08T11:24:50.505Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":4817,"end":4923},{"type":"TextQuoteSelector","exact":"irst stage refines each feature independentlyaccording to relationships between tokens across the sequence","prefix":"ating across the features. The f","suffix":" e.g. how mucha word in a sequen"}]}]}
>```
>%%
>*%%PREFIX%%ating across the features. The f%%HIGHLIGHT%% ==irst stage refines each feature independentlyaccording to relationships between tokens across the sequence== %%POSTFIX%%e.g. how mucha word in a sequen*
>%%LINK%%[[#^r38ihxjs7qb|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^r38ihxjs7qb


>%%
>```annotation-json
>{"created":"2025-11-08T11:25:08.044Z","updated":"2025-11-08T11:25:08.044Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":5088,"end":5138},{"type":"TextQuoteSelector","exact":"This stage acts horizontally across rows of X(m−1)","prefix":"mage are related to one another.","suffix":". The second stage refinesthe fe"}]}]}
>```
>%%
>*%%PREFIX%%mage are related to one another.%%HIGHLIGHT%% ==This stage acts horizontally across rows of X(m−1)== %%POSTFIX%%. The second stage refinesthe fe*
>%%LINK%%[[#^bsg61ce3974|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^bsg61ce3974


>%%
>```annotation-json
>{"created":"2025-11-08T11:25:15.095Z","updated":"2025-11-08T11:25:15.095Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":5145,"end":5200},{"type":"TextQuoteSelector","exact":"econd stage refinesthe features representing each token","prefix":"lly across rows of X(m−1). The s","suffix":". This stage acts vertically acr"}]}]}
>```
>%%
>*%%PREFIX%%lly across rows of X(m−1). The s%%HIGHLIGHT%% ==econd stage refinesthe features representing each token== %%POSTFIX%%. This stage acts vertically acr*
>%%LINK%%[[#^2bs8z1rdafk|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^2bs8z1rdafk


>%%
>```annotation-json
>{"created":"2025-11-08T11:25:21.673Z","updated":"2025-11-08T11:25:21.673Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":5202,"end":5252},{"type":"TextQuoteSelector","exact":"This stage acts vertically across a columnof X(m−1","prefix":"atures representing each token. ","suffix":"). By repeatedly applying the tr"}]}]}
>```
>%%
>*%%PREFIX%%atures representing each token.%%HIGHLIGHT%% ==This stage acts vertically across a columnof X(m−1== %%POSTFIX%%). By repeatedly applying the tr*
>%%LINK%%[[#^3xnz1o7m74a|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^3xnz1o7m74a


>%%
>```annotation-json
>{"created":"2025-11-08T11:25:42.146Z","updated":"2025-11-08T11:25:42.146Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":6984,"end":7066},{"type":"TextQuoteSelector","exact":"The output of the first stage of the transformer block is another D ×N array,Y (m)","prefix":"lf-attention across the sequence","suffix":". The output is produced by aggr"}]}]}
>```
>%%
>*%%PREFIX%%lf-attention across the sequence%%HIGHLIGHT%% ==The output of the first stage of the transformer block is another D ×N array,Y (m)== %%POSTFIX%%. The output is produced by aggr*
>%%LINK%%[[#^osx04qth47g|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^osx04qth47g


>%%
>```annotation-json
>{"created":"2025-11-08T11:26:06.357Z","updated":"2025-11-08T11:26:06.357Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":7069,"end":7203},{"type":"TextQuoteSelector","exact":"he output is produced by aggregating information across the sequenceindependently for each feature using an operation called attention","prefix":"k is another D ×N array,Y (m). T","suffix":".Attention. Specifically, the ou"}]}]}
>```
>%%
>*%%PREFIX%%k is another D ×N array,Y (m). T%%HIGHLIGHT%% ==he output is produced by aggregating information across the sequenceindependently for each feature using an operation called attention== %%POSTFIX%%.Attention. Specifically, the ou*
>%%LINK%%[[#^efpb0w8ib2e|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^efpb0w8ib2e


>%%
>```annotation-json
>{"created":"2025-11-08T11:26:59.692Z","updated":"2025-11-08T11:26:59.692Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":7229,"end":7391},{"type":"TextQuoteSelector","exact":"the output vector at location n, denoted y(m)n , is pro-duced by a simple weighted average of the input features at location n′ =1 ...N, denoted x(m−1)n′, that is","prefix":"ention.Attention. Specifically, ","suffix":"4y(m)n =N∑n′=1x(m−1)n′A(m)n′,n. "}]}]}
>```
>%%
>*%%PREFIX%%ention.Attention. Specifically,%%HIGHLIGHT%% ==the output vector at location n, denoted y(m)n , is pro-duced by a simple weighted average of the input features at location n′ =1 ...N, denoted x(m−1)n′, that is== %%POSTFIX%%4y(m)n =N∑n′=1x(m−1)n′A(m)n′,n.*
>%%LINK%%[[#^tcgl9hx89rj|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^tcgl9hx89rj


>%%
>```annotation-json
>{"created":"2025-11-08T11:27:03.829Z","updated":"2025-11-08T11:27:03.829Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":7392,"end":7421},{"type":"TextQuoteSelector","exact":"y(m)n =N∑n′=1x(m−1)n′A(m)n′,n","prefix":"...N, denoted x(m−1)n′, that is4","suffix":". (1)Here the weighting is given"}]}]}
>```
>%%
>*%%PREFIX%%...N, denoted x(m−1)n′, that is4%%HIGHLIGHT%% ==y(m)n =N∑n′=1x(m−1)n′A(m)n′,n== %%POSTFIX%%. (1)Here the weighting is given*
>%%LINK%%[[#^gtcu0ncexb|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^gtcu0ncexb


>%%
>```annotation-json
>{"created":"2025-11-08T11:29:22.406Z","updated":"2025-11-08T11:29:22.406Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":7469,"end":7494},{"type":"TextQuoteSelector","exact":"attention matrix A(m)n′,n","prefix":"ighting is given by a so-called ","suffix":" which is ofsize5 N × N and norm"}]}]}
>```
>%%
>*%%PREFIX%%ighting is given by a so-called%%HIGHLIGHT%% ==attention matrix A(m)n′,n== %%POSTFIX%%which is ofsize5 N × N and norm*
>%%LINK%%[[#^u8ylupuohwr|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^u8ylupuohwr


>%%
>```annotation-json
>{"created":"2025-11-08T11:29:49.641Z","updated":"2025-11-08T11:29:49.641Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":7512,"end":7517},{"type":"TextQuoteSelector","exact":"N × N","prefix":"atrix A(m)n′,n which is ofsize5 ","suffix":" and normalises over its columns"}]}]}
>```
>%%
>*%%PREFIX%%atrix A(m)n′,n which is ofsize5%%HIGHLIGHT%% ==N × N== %%POSTFIX%%and normalises over its columns*
>%%LINK%%[[#^b6yblfcyr5c|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^b6yblfcyr5c


>%%
>```annotation-json
>{"created":"2025-11-08T11:29:57.740Z","text":"This normalization guarantees stability and helps the Transformer learn appropriate weightings for sharing information across the sequence, ensuring that the process is focused on aggregation rather than simply inflating or deflating the magnitude of the resulting features","updated":"2025-11-08T11:29:57.740Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":7522,"end":7569},{"type":"TextQuoteSelector","exact":"normalises over its columns ∑Nn′=1 A(m)n′,n = 1","prefix":"n′,n which is ofsize5 N × N and ","suffix":". Intuitivelyspeaking A(m)n′,n w"}]}]}
>```
>%%
>*%%PREFIX%%n′,n which is ofsize5 N × N and%%HIGHLIGHT%% ==normalises over its columns ∑Nn′=1 A(m)n′,n = 1== %%POSTFIX%%. Intuitivelyspeaking A(m)n′,n w*
>%%LINK%%[[#^q0lu39dduo|show annotation]]
>%%COMMENT%%
>This normalization guarantees stability and helps the Transformer learn appropriate weightings for sharing information across the sequence, ensuring that the process is focused on aggregation rather than simply inflating or deflating the magnitude of the resulting features
>%%TAGS%%
>
^q0lu39dduo


>%%
>```annotation-json
>{"created":"2025-11-08T11:41:58.914Z","updated":"2025-11-08T11:41:58.914Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":7591,"end":7697},{"type":"TextQuoteSelector","exact":"A(m)n′,n will take a high value for locations in the sequence n′ which areof high relevance for location n","prefix":"m)n′,n = 1. Intuitivelyspeaking ","suffix":". For irrelevant locations, it w"}]}]}
>```
>%%
>*%%PREFIX%%m)n′,n = 1. Intuitivelyspeaking%%HIGHLIGHT%% ==A(m)n′,n will take a high value for locations in the sequence n′ which areof high relevance for location n== %%POSTFIX%%. For irrelevant locations, it w*
>%%LINK%%[[#^v1bjia160e|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^v1bjia160e


>%%
>```annotation-json
>{"created":"2025-11-08T11:42:20.456Z","updated":"2025-11-08T11:42:20.456Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":7934,"end":7952},{"type":"TextQuoteSelector","exact":"Y (m) = X(m−1)A(m)","prefix":"ship as a matrix multiplication,","suffix":", (2)and we illustrate it below "}]}]}
>```
>%%
>*%%PREFIX%%ship as a matrix multiplication,%%HIGHLIGHT%% ==Y (m) = X(m−1)A(m)== %%POSTFIX%%, (2)and we illustrate it below*
>%%LINK%%[[#^mhhfzr0iqlb|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^mhhfzr0iqlb


>%%
>```annotation-json
>{"created":"2025-11-08T11:43:17.563Z","updated":"2025-11-08T11:43:17.563Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":6473,"end":6675},{"type":"TextQuoteSelector","exact":"applying the transformer to the whole sequence,and using masking in the attention mechanism(A(m) becomes an upper triangular matrix) toprevent future tokens affecting the representationat earlier tokens","prefix":"ing and inference. This involves","suffix":". Causal predictions can then be"}]}]}
>```
>%%
>*%%PREFIX%%ing and inference. This involves%%HIGHLIGHT%% ==applying the transformer to the whole sequence,and using masking in the attention mechanism(A(m) becomes an upper triangular matrix) toprevent future tokens affecting the representationat earlier tokens== %%POSTFIX%%. Causal predictions can then be*
>%%LINK%%[[#^074u2vwrwvja|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^074u2vwrwvja


>%%
>```annotation-json
>{"created":"2025-11-08T11:43:26.107Z","updated":"2025-11-08T11:43:26.107Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":6678,"end":6778},{"type":"TextQuoteSelector","exact":"ausal predictions can then bemade for the entire sequence in one forward passthrough the transformer","prefix":"presentationat earlier tokens. C","suffix":". See section 4 for moreinformat"}]}]}
>```
>%%
>*%%PREFIX%%presentationat earlier tokens. C%%HIGHLIGHT%% ==ausal predictions can then bemade for the entire sequence in one forward passthrough the transformer== %%POSTFIX%%. See section 4 for moreinformat*
>%%LINK%%[[#^ps0nqrdze7g|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ps0nqrdze7g


>%%
>```annotation-json
>{"created":"2025-11-08T11:45:04.779Z","updated":"2025-11-08T11:45:04.779Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":8494,"end":8580},{"type":"TextQuoteSelector","exact":"attentionmatrix is generated from the input sequence itself – so-called self-attention","prefix":" of the transformer is that the ","suffix":".A simple way of generating the "}]}]}
>```
>%%
>*%%PREFIX%%of the transformer is that the%%HIGHLIGHT%% ==attentionmatrix is generated from the input sequence itself – so-called self-attention== %%POSTFIX%%.A simple way of generating the*
>%%LINK%%[[#^adjc0cifprh|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^adjc0cifprh


>%%
>```annotation-json
>{"created":"2025-11-08T11:45:56.078Z","updated":"2025-11-08T11:45:56.078Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":8612,"end":8628},{"type":"TextQuoteSelector","exact":"attention matrix","prefix":".A simple way of generating the ","suffix":" from the input would be tomeasu"}]}]}
>```
>%%
>*%%PREFIX%%.A simple way of generating the%%HIGHLIGHT%% ==attention matrix== %%POSTFIX%%from the input would be tomeasu*
>%%LINK%%[[#^fvzzcuufd6u|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^fvzzcuufd6u


>%%
>```annotation-json
>{"created":"2025-11-08T11:46:10.904Z","updated":"2025-11-08T11:46:10.904Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":8655,"end":8761},{"type":"TextQuoteSelector","exact":"measure the similarity between two locations by the dot product between thefeatures at those two locations","prefix":"atrix from the input would be to","suffix":" and then use a softmax function"}]}]}
>```
>%%
>*%%PREFIX%%atrix from the input would be to%%HIGHLIGHT%% ==measure the similarity between two locations by the dot product between thefeatures at those two locations== %%POSTFIX%%and then use a softmax function*
>%%LINK%%[[#^3qayemcog0g|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^3qayemcog0g


>%%
>```annotation-json
>{"created":"2025-11-08T11:46:14.911Z","updated":"2025-11-08T11:46:14.911Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":8776,"end":8793},{"type":"TextQuoteSelector","exact":" softmax function","prefix":"ose two locations and then use a","suffix":" to handle thenormalisation i.e."}]}]}
>```
>%%
>*%%PREFIX%%ose two locations and then use a%%HIGHLIGHT%% ==softmax function== %%POSTFIX%%to handle thenormalisation i.e.*
>%%LINK%%[[#^nu42hbqwufi|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^nu42hbqwufi


>%%
>```annotation-json
>{"created":"2025-11-08T11:46:18.537Z","updated":"2025-11-08T11:46:18.537Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":8807,"end":8820},{"type":"TextQuoteSelector","exact":"normalisation","prefix":"a softmax function to handle the","suffix":" i.e.7An,n′= exp(x⊤n xn′)∑Nn′′=1"}]}]}
>```
>%%
>*%%PREFIX%%a softmax function to handle the%%HIGHLIGHT%% ==normalisation== %%POSTFIX%%i.e.7An,n′= exp(x⊤n xn′)∑Nn′′=1*
>%%LINK%%[[#^ym6s1jkm9oh|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ym6s1jkm9oh


>%%
>```annotation-json
>{"created":"2025-11-08T11:47:03.167Z","updated":"2025-11-08T11:47:03.167Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":8826,"end":8866},{"type":"TextQuoteSelector","exact":"An,n′= exp(x⊤n xn′)∑Nn′′=1 exp(x⊤n′′xn′)","prefix":"to handle thenormalisation i.e.7","suffix":" .28 Often you will see attentio"}]}]}
>```
>%%
>*%%PREFIX%%to handle thenormalisation i.e.7%%HIGHLIGHT%% ==An,n′= exp(x⊤n xn′)∑Nn′′=1 exp(x⊤n′′xn′)== %%POSTFIX%%.28 Often you will see attentio*
>%%LINK%%[[#^jrb63v63nap|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^jrb63v63nap


>%%
>```annotation-json
>{"created":"2025-11-08T11:51:59.477Z","text":"The decision of where to look (location/relevance) is inseparable from what the information is (content/features)","updated":"2025-11-08T11:51:59.477Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":11269,"end":11399},{"type":"TextQuoteSelector","exact":"naïve approach entangles information about the similarity betweenlocations in the sequence with the content of the sequence itself","prefix":"-tational demands.However, this ","suffix":".An alternative is to perform th"}]}]}
>```
>%%
>*%%PREFIX%%-tational demands.However, this%%HIGHLIGHT%% ==naïve approach entangles information about the similarity betweenlocations in the sequence with the content of the sequence itself== %%POSTFIX%%.An alternative is to perform th*
>%%LINK%%[[#^3gdgda2gna9|show annotation]]
>%%COMMENT%%
>The decision of where to look (location/relevance) is inseparable from what the information is (content/features)
>%%TAGS%%
>
^3gdgda2gna9


>%%
>```annotation-json
>{"created":"2025-11-08T11:54:45.021Z","updated":"2025-11-08T11:54:45.021Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":11504,"end":11550},{"type":"TextQuoteSelector","exact":"An,n′= exp(x⊤n U⊤Uxn′)∑Nn′′=1 exp(x⊤n′′U⊤Uxn′)","prefix":"on ofthe sequence, Uxn, so that8","suffix":"Typically, U will project to a l"}]}]}
>```
>%%
>*%%PREFIX%%on ofthe sequence, Uxn, so that8%%HIGHLIGHT%% ==An,n′= exp(x⊤n U⊤Uxn′)∑Nn′′=1 exp(x⊤n′′U⊤Uxn′)== %%POSTFIX%%Typically, U will project to a l*
>%%LINK%%[[#^hl6pbegqmy9|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^hl6pbegqmy9


>%%
>```annotation-json
>{"created":"2025-11-08T11:54:59.516Z","updated":"2025-11-08T11:54:59.516Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":11561,"end":11640},{"type":"TextQuoteSelector","exact":"U will project to a lower dimensional space i.e. U is K×D dimensionalwith K < D","prefix":"′′=1 exp(x⊤n′′U⊤Uxn′)Typically, ","suffix":". In this way only some of the f"}]}]}
>```
>%%
>*%%PREFIX%%′′=1 exp(x⊤n′′U⊤Uxn′)Typically,%%HIGHLIGHT%% ==U will project to a lower dimensional space i.e. U is K×D dimensionalwith K < D== %%POSTFIX%%. In this way only some of the f*
>%%LINK%%[[#^kak39paqck9|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^kak39paqck9


>%%
>```annotation-json
>{"created":"2025-11-08T11:55:21.920Z","updated":"2025-11-08T11:55:21.920Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":11655,"end":11739},{"type":"TextQuoteSelector","exact":"nly some of the features in the input sequence needbe used to compute the similarity","prefix":"nsionalwith K < D. In this way o","suffix":", the others being projected out"}]}]}
>```
>%%
>*%%PREFIX%%nsionalwith K < D. In this way o%%HIGHLIGHT%% ==nly some of the features in the input sequence needbe used to compute the similarity== %%POSTFIX%%, the others being projected out*
>%%LINK%%[[#^zuuzfgj253|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^zuuzfgj253


>%%
>```annotation-json
>{"created":"2025-11-08T11:55:28.622Z","updated":"2025-11-08T11:55:28.622Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":11781,"end":11835},{"type":"TextQuoteSelector","exact":"de-coupling the attention computation from the content","prefix":"rs being projected out, thereby ","suffix":". However, the numeratorin this "}]}]}
>```
>%%
>*%%PREFIX%%rs being projected out, thereby%%HIGHLIGHT%% ==de-coupling the attention computation from the content== %%POSTFIX%%. However, the numeratorin this*
>%%LINK%%[[#^p31nh60r8z|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^p31nh60r8z


>%%
>```annotation-json
>{"created":"2025-11-08T11:55:54.935Z","updated":"2025-11-08T11:55:54.935Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":11851,"end":11892},{"type":"TextQuoteSelector","exact":"umeratorin this construction is symmetric","prefix":"from the content. However, the n","suffix":". This could be a disadvantage. "}]}]}
>```
>%%
>*%%PREFIX%%from the content. However, the n%%HIGHLIGHT%% ==umeratorin this construction is symmetric== %%POSTFIX%%. This could be a disadvantage.*
>%%LINK%%[[#^fl2ecqk9pkq|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^fl2ecqk9pkq


>%%
>```annotation-json
>{"created":"2025-11-08T11:56:30.499Z","updated":"2025-11-08T11:56:30.499Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":12329,"end":12381},{"type":"TextQuoteSelector","exact":"An,n′= exp (x⊤n U⊤kUqxn′)∑Nn′′=1 exp (x⊤n′′U⊤kUqxn′)","prefix":"ations to the original sequence,","suffix":". (3)The two quantities that are"}]}]}
>```
>%%
>*%%PREFIX%%ations to the original sequence,%%HIGHLIGHT%% ==An,n′= exp (x⊤n U⊤kUqxn′)∑Nn′′=1 exp (x⊤n′′U⊤kUqxn′)== %%POSTFIX%%. (3)The two quantities that are*
>%%LINK%%[[#^0vdv1p3xj8x9|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^0vdv1p3xj8x9


>%%
>```annotation-json
>{"created":"2025-11-08T11:56:40.428Z","updated":"2025-11-08T11:56:40.428Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":12442,"end":12512},{"type":"TextQuoteSelector","exact":"qn = Uqxn and kn =Ukxn are typically known as the queries and the keys","prefix":"are dot-producted together here ","suffix":", respectively.Together equation"}]}]}
>```
>%%
>*%%PREFIX%%are dot-producted together here%%HIGHLIGHT%% ==qn = Uqxn and kn =Ukxn are typically known as the queries and the keys== %%POSTFIX%%, respectively.Together equation*
>%%LINK%%[[#^vyok3dpx7b|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^vyok3dpx7b


>%%
>```annotation-json
>{"created":"2025-11-08T11:57:46.976Z","updated":"2025-11-08T11:57:46.976Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":9313,"end":9358},{"type":"TextQuoteSelector","exact":"xperimental evidence to supportusing Uq̸= Uk.","prefix":"lity. However, Ido not know of e","suffix":"10 Relationship to Recurrent Neu"}]}]}
>```
>%%
>*%%PREFIX%%lity. However, Ido not know of e%%HIGHLIGHT%% ==xperimental evidence to supportusing Uq̸= Uk.== %%POSTFIX%%10 Relationship to Recurrent Neu*
>%%LINK%%[[#^p47armoj1ec|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^p47armoj1ec


>%%
>```annotation-json
>{"created":"2025-11-08T11:57:55.252Z","updated":"2025-11-08T11:57:55.252Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":9296,"end":9312},{"type":"TextQuoteSelector","exact":"Ido not know of ","prefix":"lows more flexibility. However, ","suffix":"experimental evidence to support"}]}]}
>```
>%%
>*%%PREFIX%%lows more flexibility. However,%%HIGHLIGHT%% ==Ido not know of== %%POSTFIX%%experimental evidence to support*
>%%LINK%%[[#^au8hfvlpc2|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^au8hfvlpc2


>%%
>```annotation-json
>{"created":"2025-11-08T12:01:05.264Z","text":"The Query represents the current token's demand for information. You can think of it as asking: \"What information am I looking for?\"","updated":"2025-11-08T12:01:05.264Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":12491,"end":12500},{"type":"TextQuoteSelector","exact":" queries ","prefix":"=Ukxn are typically known as the","suffix":"and the keys, respectively.Toget"}]}]}
>```
>%%
>*%%PREFIX%%=Ukxn are typically known as the%%HIGHLIGHT%% ==queries== %%POSTFIX%%and the keys, respectively.Toget*
>%%LINK%%[[#^z7mpjo8uhy|show annotation]]
>%%COMMENT%%
>The Query represents the current token's demand for information. You can think of it as asking: "What information am I looking for?"
>%%TAGS%%
>
^z7mpjo8uhy


>%%
>```annotation-json
>{"created":"2025-11-08T12:01:31.969Z","text":"The Key represents the content or identifier of the current token. You can think of it as saying: \"Here is the information I have to offer.\"","updated":"2025-11-08T12:01:31.969Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":12507,"end":12512},{"type":"TextQuoteSelector","exact":" keys","prefix":"lly known as the queries and the","suffix":", respectively.Together equation"}]}]}
>```
>%%
>*%%PREFIX%%lly known as the queries and the%%HIGHLIGHT%% ==keys== %%POSTFIX%%, respectively.Together equation*
>%%LINK%%[[#^tem3tpr1zb|show annotation]]
>%%COMMENT%%
>The Key represents the content or identifier of the current token. You can think of it as saying: "Here is the information I have to offer."
>%%TAGS%%
>
^tem3tpr1zb


>%%
>```annotation-json
>{"created":"2025-11-08T12:07:01.933Z","updated":"2025-11-08T12:07:01.933Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":12855,"end":13003},{"type":"TextQuoteSelector","exact":"This can act as a bottleneck in the architec-ture – it would be useful for pairs of points to be similar in some ‘dimensions’and different in others","prefix":" locations within the sequence. ","suffix":".11In order to increase capacity"}]}]}
>```
>%%
>*%%PREFIX%%locations within the sequence.%%HIGHLIGHT%% ==This can act as a bottleneck in the architec-ture – it would be useful for pairs of points to be similar in some ‘dimensions’and different in others== %%POSTFIX%%.11In order to increase capacity*
>%%LINK%%[[#^nudhoy97u4|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^nudhoy97u4


>%%
>```annotation-json
>{"created":"2025-11-08T12:08:03.188Z","updated":"2025-11-08T12:08:03.188Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":10434,"end":10589},{"type":"TextQuoteSelector","exact":"he computational cost of multi-head self-attention is usually dominated by the matrix mul-tiplication involving the attention matrix and istherefore O(HDN2","prefix":"umbers of channels in a CNN.12 T","suffix":").13 The product of the matrices"}]}]}
>```
>%%
>*%%PREFIX%%umbers of channels in a CNN.12 T%%HIGHLIGHT%% ==he computational cost of multi-head self-attention is usually dominated by the matrix mul-tiplication involving the attention matrix and istherefore O(HDN2== %%POSTFIX%%).13 The product of the matrices*
>%%LINK%%[[#^l7rquc410yk|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^l7rquc410yk


>%%
>```annotation-json
>{"created":"2025-11-08T12:08:35.644Z","updated":"2025-11-08T12:08:35.644Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":13006,"end":13249},{"type":"TextQuoteSelector","exact":"In order to increase capacity of the first self-attention stage, the transformerblock applies H sets of self-attention in parallel12 (termed H heads) and thenlinearly projects the results down to the D ×N array required for further pro-cessing","prefix":"sions’and different in others.11","suffix":". This slight generalisation is "}]}]}
>```
>%%
>*%%PREFIX%%sions’and different in others.11%%HIGHLIGHT%% ==In order to increase capacity of the first self-attention stage, the transformerblock applies H sets of self-attention in parallel12 (termed H heads) and thenlinearly projects the results down to the D ×N array required for further pro-cessing== %%POSTFIX%%. This slight generalisation is*
>%%LINK%%[[#^lk33ygn4pt7|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^lk33ygn4pt7


>%%
>```annotation-json
>{"created":"2025-11-08T12:08:39.369Z","updated":"2025-11-08T12:08:39.369Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":13288,"end":13313},{"type":"TextQuoteSelector","exact":"multi-head self-attention","prefix":"slight generalisation is called ","suffix":".Y (m) = MHSAθ(X(m−1)) =H∑h=1V ("}]}]}
>```
>%%
>*%%PREFIX%%slight generalisation is called%%HIGHLIGHT%% ==multi-head self-attention== %%POSTFIX%%.Y (m) = MHSAθ(X(m−1)) =H∑h=1V (*
>%%LINK%%[[#^rh4u07gb6u|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^rh4u07gb6u


>%%
>```annotation-json
>{"created":"2025-11-08T12:09:13.272Z","updated":"2025-11-08T12:09:13.272Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":13314,"end":13361},{"type":"TextQuoteSelector","exact":"Y (m) = MHSAθ(X(m−1)) =H∑h=1V (m)h X(m−1)A(m)h ","prefix":"alled multi-head self-attention.","suffix":", where (4)[A(m)h ]n,n′=exp((k(m"}]}]}
>```
>%%
>*%%PREFIX%%alled multi-head self-attention.%%HIGHLIGHT%% ==Y (m) = MHSAθ(X(m−1)) =H∑h=1V (m)h X(m−1)A(m)h== %%POSTFIX%%, where (4)[A(m)h ]n,n′=exp((k(m*
>%%LINK%%[[#^zd1gkd04y0j|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^zd1gkd04y0j


>%%
>```annotation-json
>{"created":"2025-11-08T12:10:29.971Z","updated":"2025-11-08T12:10:29.971Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":11113,"end":11255},{"type":"TextQuoteSelector","exact":"ypically K is set to K = D/H so thatchanging the number of heads leads to modelswith similar numbers of parameters and compu-tational demands.","prefix":"here Uh is DxK and Uv,h isKxD. T","suffix":"However, this naïve approach ent"}]}]}
>```
>%%
>*%%PREFIX%%here Uh is DxK and Uv,h isKxD. T%%HIGHLIGHT%% ==ypically K is set to K = D/H so thatchanging the number of heads leads to modelswith similar numbers of parameters and compu-tational demands.== %%POSTFIX%%However, this naïve approach ent*
>%%LINK%%[[#^f13wfw6q76e|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^f13wfw6q76e


>%%
>```annotation-json
>{"created":"2025-11-08T12:12:41.163Z","updated":"2025-11-08T12:12:41.163Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":13506,"end":13626},{"type":"TextQuoteSelector","exact":"Here the H matrices V (m)h which are D×D project the H self-attention stagesdown to the required output dimensionality D","prefix":" k(m)h,n = U(m)k,h x(m−1)n . (6)","suffix":".13The addition of the matrices "}]}]}
>```
>%%
>*%%PREFIX%%k(m)h,n = U(m)k,h x(m−1)n . (6)%%HIGHLIGHT%% ==Here the H matrices V (m)h which are D×D project the H self-attention stagesdown to the required output dimensionality D== %%POSTFIX%%.13The addition of the matrices*
>%%LINK%%[[#^hrwjej50vun|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^hrwjej50vun


>%%
>```annotation-json
>{"created":"2025-11-08T12:19:59.435Z","updated":"2025-11-08T12:19:59.435Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":16205,"end":16225},{"type":"TextQuoteSelector","exact":"x(m)n = MLPθ(y(m)n )","prefix":" each locationn in the sequence,","suffix":".Notice that the parameters of t"}]}]}
>```
>%%
>*%%PREFIX%%each locationn in the sequence,%%HIGHLIGHT%% ==x(m)n = MLPθ(y(m)n )== %%POSTFIX%%.Notice that the parameters of t*
>%%LINK%%[[#^ytlbhpu1ala|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ytlbhpu1ala


>%%
>```annotation-json
>{"created":"2025-11-08T12:20:07.684Z","updated":"2025-11-08T12:20:07.684Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":16242,"end":16300},{"type":"TextQuoteSelector","exact":"parameters of the MLP, θ, are the same for each location n","prefix":" = MLPθ(y(m)n ).Notice that the ","suffix":".14152.3 The transformer block: "}]}]}
>```
>%%
>*%%PREFIX%%= MLPθ(y(m)n ).Notice that the%%HIGHLIGHT%% ==parameters of the MLP, θ, are the same for each location n== %%POSTFIX%%.14152.3 The transformer block:*
>%%LINK%%[[#^jo1rs3o7wkd|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^jo1rs3o7wkd


>%%
>```annotation-json
>{"created":"2025-11-08T12:20:21.599Z","updated":"2025-11-08T12:20:21.599Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":14071,"end":14184},{"type":"TextQuoteSelector","exact":"he MLPs used typically have one or twohidden-layers with dimension equal to the num-ber of features D (or larger)","prefix":"cond stage to address this.314 T","suffix":". The computationalcost of this "}]}]}
>```
>%%
>*%%PREFIX%%cond stage to address this.314 T%%HIGHLIGHT%% ==he MLPs used typically have one or twohidden-layers with dimension equal to the num-ber of features D (or larger)== %%POSTFIX%%. The computationalcost of this*
>%%LINK%%[[#^skhjil84syg|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^skhjil84syg


>%%
>```annotation-json
>{"created":"2025-11-08T12:21:24.430Z","updated":"2025-11-08T12:21:24.430Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":14186,"end":14382},{"type":"TextQuoteSelector","exact":"The computationalcost of this step is therefore roughly N ×D×D. Ifthe feature embedding size approaches the lengthof the sequence D ≈ N, the MLPs can start todominate the computational complexity ","prefix":"-ber of features D (or larger). ","suffix":"(e.g. thiscan be the case for vi"}]}]}
>```
>%%
>*%%PREFIX%%-ber of features D (or larger).%%HIGHLIGHT%% ==The computationalcost of this step is therefore roughly N ×D×D. Ifthe feature embedding size approaches the lengthof the sequence D ≈ N, the MLPs can start todominate the computational complexity== %%POSTFIX%%(e.g. thiscan be the case for vi*
>%%LINK%%[[#^e2syeer1tff|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^e2syeer1tff


>%%
>```annotation-json
>{"created":"2025-11-08T12:21:51.717Z","updated":"2025-11-08T12:21:51.717Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":16648,"end":16668},{"type":"TextQuoteSelector","exact":"Residual connections","prefix":"al connectionsand normalisation.","suffix":". The use of residual connection"}]}]}
>```
>%%
>*%%PREFIX%%al connectionsand normalisation.%%HIGHLIGHT%% ==Residual connections== %%POSTFIX%%. The use of residual connection*
>%%LINK%%[[#^ay9uhxg7a9j|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ay9uhxg7a9j


>%%
>```annotation-json
>{"created":"2025-11-08T12:22:09.311Z","updated":"2025-11-08T12:22:09.311Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":16752,"end":16853},{"type":"TextQuoteSelector","exact":"initialisation simple, have a sensible inductive biastowards simple functions, and stabilise learning","prefix":"ssmachine learning as they make ","suffix":" [Szegedy et al., 2017]. Instead"}]}]}
>```
>%%
>*%%PREFIX%%ssmachine learning as they make%%HIGHLIGHT%% ==initialisation simple, have a sensible inductive biastowards simple functions, and stabilise learning== %%POSTFIX%%[Szegedy et al., 2017]. Instead*
>%%LINK%%[[#^p1b9e9oorj|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^p1b9e9oorj


>%%
>```annotation-json
>{"created":"2025-11-08T12:22:27.809Z","updated":"2025-11-08T12:22:27.809Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":17019,"end":17047},{"type":"TextQuoteSelector","exact":"x(m) = x(m−1) + resθ(x(m−1))","prefix":"tity mapping and a residual term","suffix":".Equivalently, this can be viewe"}]}]}
>```
>%%
>*%%PREFIX%%tity mapping and a residual term%%HIGHLIGHT%% ==x(m) = x(m−1) + resθ(x(m−1))== %%POSTFIX%%.Equivalently, this can be viewe*
>%%LINK%%[[#^cgw9htjk74h|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^cgw9htjk74h


>%%
>```annotation-json
>{"created":"2025-11-08T12:26:59.017Z","updated":"2025-11-08T12:26:59.017Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":17863,"end":17954},{"type":"TextQuoteSelector","exact":"ormalises each token separately, re-moving the mean and dividing by the standard deviation,","prefix":"erNorm [Ba et al., 2016] which n","suffix":"16 ̄xd,n = 1√var(xn) (xd,n −mean"}]}]}
>```
>%%
>*%%PREFIX%%erNorm [Ba et al., 2016] which n%%HIGHLIGHT%% ==ormalises each token separately, re-moving the mean and dividing by the standard deviation,== %%POSTFIX%%16 ̄xd,n = 1√var(xn) (xd,n −mean*
>%%LINK%%[[#^17i1kj9mwb3|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^17i1kj9mwb3


>%%
>```annotation-json
>{"created":"2025-11-08T12:27:08.578Z","updated":"2025-11-08T12:27:08.578Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":17958,"end":18017},{"type":"TextQuoteSelector","exact":"xd,n = 1√var(xn) (xd,n −mean(xn)) γd + βd = LayerNorm(X)d,n","prefix":"g by the standard deviation,16 ̄","suffix":"where mean(xn) = 1D∑Dd=1 xd,n an"}]}]}
>```
>%%
>*%%PREFIX%%g by the standard deviation,16 ̄%%HIGHLIGHT%% ==xd,n = 1√var(xn) (xd,n −mean(xn)) γd + βd = LayerNorm(X)d,n== %%POSTFIX%%where mean(xn) = 1D∑Dd=1 xd,n an*
>%%LINK%%[[#^fy54ty3v7fo|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^fy54ty3v7fo


>%%
>```annotation-json
>{"created":"2025-11-08T12:32:02.330Z","updated":"2025-11-08T12:32:02.330Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":20588,"end":20626},{"type":"TextQuoteSelector","exact":"his block can then be repeated M times","prefix":"theMLP. They are then stacked. T","suffix":".3 Position encodingThe transfor"}]}]}
>```
>%%
>*%%PREFIX%%theMLP. They are then stacked. T%%HIGHLIGHT%% ==his block can then be repeated M times== %%POSTFIX%%.3 Position encodingThe transfor*
>%%LINK%%[[#^b8cljg6di56|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^b8cljg6di56


>%%
>```annotation-json
>{"created":"2025-11-08T12:32:12.986Z","updated":"2025-11-08T12:32:12.986Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":20646,"end":20852},{"type":"TextQuoteSelector","exact":"The transformer treats the data as a set — if you permute the columns of X(0)(i.e. re-order the tokens in the input sequence) you permute all the represen-tations throughout the network X(m) in the same way","prefix":"ated M times.3 Position encoding","suffix":". This is key for manyapplicatio"}]}]}
>```
>%%
>*%%PREFIX%%ated M times.3 Position encoding%%HIGHLIGHT%% ==The transformer treats the data as a set — if you permute the columns of X(0)(i.e. re-order the tokens in the input sequence) you permute all the represen-tations throughout the network X(m) in the same way== %%POSTFIX%%. This is key for manyapplicatio*
>%%LINK%%[[#^rahiprbm2i|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^rahiprbm2i


>%%
>```annotation-json
>{"created":"2025-11-08T12:32:48.055Z","updated":"2025-11-08T12:32:48.055Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":22379,"end":22444},{"type":"TextQuoteSelector","exact":"one is to include this information directly into theembedding X(0","prefix":"several options how to do this, ","suffix":"). E.g. by simply adding the pos"}]}]}
>```
>%%
>*%%PREFIX%%several options how to do this,%%HIGHLIGHT%% ==one is to include this information directly into theembedding X(0== %%POSTFIX%%). E.g. by simply adding the pos*
>%%LINK%%[[#^6qse0mykjgu|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^6qse0mykjgu


>%%
>```annotation-json
>{"created":"2025-11-08T12:33:00.020Z","updated":"2025-11-08T12:33:00.020Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":22537,"end":22575},{"type":"TextQuoteSelector","exact":"The position information can be fixed ","prefix":" thisworks19) or concatenating. ","suffix":"e.g. adding avector of sinusoids"}]}]}
>```
>%%
>*%%PREFIX%%thisworks19) or concatenating.%%HIGHLIGHT%% ==The position information can be fixed== %%POSTFIX%%e.g. adding avector of sinusoids*
>%%LINK%%[[#^4vcwvzsuhyd|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^4vcwvzsuhyd


>%%
>```annotation-json
>{"created":"2025-11-08T12:33:29.033Z","updated":"2025-11-08T12:33:29.033Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":22713,"end":22755},{"type":"TextQuoteSelector","exact":"it can be a free parameter which islearned","prefix":"ence [Vaswani et al., 2017], or ","suffix":" [Devlin et al., 2019], as it of"}]}]}
>```
>%%
>*%%PREFIX%%ence [Vaswani et al., 2017], or%%HIGHLIGHT%% ==it can be a free parameter which islearned== %%POSTFIX%%[Devlin et al., 2019], as it of*
>%%LINK%%[[#^w9kqoui4lul|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^w9kqoui4lul


>%%
>```annotation-json
>{"created":"2025-11-08T12:33:39.402Z","updated":"2025-11-08T12:33:39.402Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":22820,"end":23009},{"type":"TextQuoteSelector","exact":"here arealso approaches to include relative distance information between pairs of tokensby modifying the self-attention mechanism [Wu et al., 2021] which connects toequivariant transformers","prefix":"en done in image transformers. T","suffix":".4 Application specific transfor"}]}]}
>```
>%%
>*%%PREFIX%%en done in image transformers. T%%HIGHLIGHT%% ==here arealso approaches to include relative distance information between pairs of tokensby modifying the self-attention mechanism [Wu et al., 2021] which connects toequivariant transformers== %%POSTFIX%%.4 Application specific transfor*
>%%LINK%%[[#^phdb8gubb4l|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^phdb8gubb4l


>%%
>```annotation-json
>{"created":"2025-11-08T12:34:53.607Z","updated":"2025-11-08T12:34:53.607Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":23406,"end":23562},{"type":"TextQuoteSelector","exact":"auto-regressive language modelling the goal is to predict the next word wnin the sequence given the previous words w1:n−1, that is to return p(wn =w|w1:n−1)","prefix":"regressive language modellingIn ","suffix":". Two modifications are required"}]}]}
>```
>%%
>*%%PREFIX%%regressive language modellingIn%%HIGHLIGHT%% ==auto-regressive language modelling the goal is to predict the next word wnin the sequence given the previous words w1:n−1, that is to return p(wn =w|w1:n−1)== %%POSTFIX%%. Two modifications are required*
>%%LINK%%[[#^mp5lg49d8c|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^mp5lg49d8c


>%%
>```annotation-json
>{"created":"2025-11-08T12:37:09.368Z","updated":"2025-11-08T12:37:09.368Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":24823,"end":24859},{"type":"TextQuoteSelector","exact":"X(n) = transformer-incremental(w1:n)","prefix":"o the first n words be denoted20","suffix":".Then the output of the incremen"}]}]}
>```
>%%
>*%%PREFIX%%o the first n words be denoted20%%HIGHLIGHT%% ==X(n) = transformer-incremental(w1:n)== %%POSTFIX%%.Then the output of the incremen*
>%%LINK%%[[#^43o9uhdzyq8|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^43o9uhdzyq8


>%%
>```annotation-json
>{"created":"2025-11-08T12:37:22.878Z","updated":"2025-11-08T12:37:22.878Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":24936,"end":24976},{"type":"TextQuoteSelector","exact":"X(n+1) = transformer-incremental(w1:n+1)","prefix":"er when applied to n+ 1 words is","suffix":".In the incremental transformer "}]}]}
>```
>%%
>*%%PREFIX%%er when applied to n+ 1 words is%%HIGHLIGHT%% ==X(n+1) = transformer-incremental(w1:n+1)== %%POSTFIX%%.In the incremental transformer*
>%%LINK%%[[#^dsoqeeuq5o|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^dsoqeeuq5o


>%%
>```annotation-json
>{"created":"2025-11-08T12:37:32.638Z","updated":"2025-11-08T12:37:32.638Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":24977,"end":25108},{"type":"TextQuoteSelector","exact":"In the incremental transformer X(n) = X(n+1)1:D,1:n i.e. the representation of theold tokens has not changed by adding the new one.","prefix":"transformer-incremental(w1:n+1).","suffix":" If we have this property621 Not"}]}]}
>```
>%%
>*%%PREFIX%%transformer-incremental(w1:n+1).%%HIGHLIGHT%% ==In the incremental transformer X(n) = X(n+1)1:D,1:n i.e. the representation of theold tokens has not changed by adding the new one.== %%POSTFIX%%If we have this property621 Not*
>%%LINK%%[[#^2rc83bpndyh|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^2rc83bpndyh


>%%
>```annotation-json
>{"created":"2025-11-08T12:38:06.725Z","updated":"2025-11-08T12:38:06.725Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":25532,"end":25642},{"type":"TextQuoteSelector","exact":"t test-time auto-regressive generation can use incremental updates tocompute the new representation efficientl","prefix":"ns, i.e. in-creasing D.then 1. a","suffix":"y, 2. at training time we can ma"}]}]}
>```
>%%
>*%%PREFIX%%ns, i.e. in-creasing D.then 1. a%%HIGHLIGHT%% ==t test-time auto-regressive generation can use incremental updates tocompute the new representation efficientl== %%POSTFIX%%y, 2. at training time we can ma*
>%%LINK%%[[#^yn96953lnxh|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^yn96953lnxh


>%%
>```annotation-json
>{"created":"2025-11-08T12:38:19.092Z","updated":"2025-11-08T12:38:19.092Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":25648,"end":25814},{"type":"TextQuoteSelector","exact":"at training time we can makethe N auto-regressive predictions for the whole sequence p(w1 = w)p(w2 =w|w1)p(w2 = w|w1,w2) ...p(wN = w|w1:N−1) in a single forwards pass","prefix":" representation efficiently, 2. ","suffix":".Unfortunately, the standard tra"}]}]}
>```
>%%
>*%%PREFIX%%representation efficiently, 2.%%HIGHLIGHT%% ==at training time we can makethe N auto-regressive predictions for the whole sequence p(w1 = w)p(w2 =w|w1)p(w2 = w|w1,w2) ...p(wN = w|w1:N−1) in a single forwards pass== %%POSTFIX%%.Unfortunately, the standard tra*
>%%LINK%%[[#^7ib2h8x582o|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^7ib2h8x582o


>%%
>```annotation-json
>{"created":"2025-11-08T12:42:26.969Z","updated":"2025-11-08T12:42:26.969Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":26095,"end":26257},{"type":"TextQuoteSelector","exact":"However, if we mask theattention matrix so that it is upper-triangular An,n′= 0 when n > n′ thenthe representation of each word only depends on the previous words","prefix":"ges throughout the transformer. ","suffix":".21 Thisthen gives us the increm"}]}]}
>```
>%%
>*%%PREFIX%%ges throughout the transformer.%%HIGHLIGHT%% ==However, if we mask theattention matrix so that it is upper-triangular An,n′= 0 when n > n′ thenthe representation of each word only depends on the previous words== %%POSTFIX%%.21 Thisthen gives us the increm*
>%%LINK%%[[#^skd0lss8uio|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^skd0lss8uio


>%%
>```annotation-json
>{"created":"2025-11-08T12:42:31.870Z","updated":"2025-11-08T12:42:31.870Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":26262,"end":26382},{"type":"TextQuoteSelector","exact":"histhen gives us the incremental property as none of the other operations in thetransformer operate across the sequence.","prefix":"pends on the previous words.21 T","suffix":"22Adding a head. We’re now almos"}]}]}
>```
>%%
>*%%PREFIX%%pends on the previous words.21 T%%HIGHLIGHT%% ==histhen gives us the incremental property as none of the other operations in thetransformer operate across the sequence.== %%POSTFIX%%22Adding a head. We’re now almos*
>%%LINK%%[[#^svdb55nqwo|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^svdb55nqwo


>%%
>```annotation-json
>{"created":"2025-11-08T12:42:35.708Z","updated":"2025-11-08T12:42:35.708Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":25137,"end":25264},{"type":"TextQuoteSelector","exact":"Notice that this masking operation also en-codes position information since you can inferthe order of the tokens from the mask.","prefix":"ne. If we have this property621 ","suffix":"22 This restriction to the atten"}]}]}
>```
>%%
>*%%PREFIX%%ne. If we have this property621%%HIGHLIGHT%% ==Notice that this masking operation also en-codes position information since you can inferthe order of the tokens from the mask.== %%POSTFIX%%22 This restriction to the atten*
>%%LINK%%[[#^hsq0nnmye3p|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^hsq0nnmye3p


>%%
>```annotation-json
>{"created":"2025-11-08T12:42:53.539Z","updated":"2025-11-08T12:42:53.539Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":25267,"end":25522},{"type":"TextQuoteSelector","exact":"This restriction to the attention will cause aloss of representational power. It’s an open ques-tion as to how significant this is and whether in-creasing the capacity of the model can mitigateit e.g. by using higher dimensional tokens, i.e. in-creasing D","prefix":" of the tokens from the mask.22 ","suffix":".then 1. at test-time auto-regre"}]}]}
>```
>%%
>*%%PREFIX%%of the tokens from the mask.22%%HIGHLIGHT%% ==This restriction to the attention will cause aloss of representational power. It’s an open ques-tion as to how significant this is and whether in-creasing the capacity of the model can mitigateit e.g. by using higher dimensional tokens, i.e. in-creasing D== %%POSTFIX%%.then 1. at test-time auto-regre*
>%%LINK%%[[#^o5wbz1r3tw|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^o5wbz1r3tw


>%%
>```annotation-json
>{"created":"2025-11-08T12:46:02.869Z","updated":"2025-11-08T12:46:02.869Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":26466,"end":26743},{"type":"TextQuoteSelector","exact":"We apply the masked transformer block M times to the input se-quence of words. We then take the representation at token n−1, that is x(M)n−1which captures causal information in the sequence at this point, and generatethe probability of the next word through a softmax operation","prefix":"o-regressive languagemodelling. ","suffix":"p(wn = w|w1:n−1) = p(wn = w|x(M)"}]}]}
>```
>%%
>*%%PREFIX%%o-regressive languagemodelling.%%HIGHLIGHT%% ==We apply the masked transformer block M times to the input se-quence of words. We then take the representation at token n−1, that is x(M)n−1which captures causal information in the sequence at this point, and generatethe probability of the next word through a softmax operation== %%POSTFIX%%p(wn = w|w1:n−1) = p(wn = w|x(M)*
>%%LINK%%[[#^0jrkkwdtlbaj|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^0jrkkwdtlbaj


>%%
>```annotation-json
>{"created":"2025-11-08T12:46:13.668Z","updated":"2025-11-08T12:46:13.668Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":26743,"end":26819},{"type":"TextQuoteSelector","exact":"p(wn = w|w1:n−1) = p(wn = w|x(M)n−1) = exp(g⊤wx(M)n−1)∑Ww=1 exp(g⊤wx(M)n−1) ","prefix":"word through a softmax operation","suffix":".Here W is the vocabulary size, "}]}]}
>```
>%%
>*%%PREFIX%%word through a softmax operation%%HIGHLIGHT%% ==p(wn = w|w1:n−1) = p(wn = w|x(M)n−1) = exp(g⊤wx(M)n−1)∑Ww=1 exp(g⊤wx(M)n−1)== %%POSTFIX%%.Here W is the vocabulary size,*
>%%LINK%%[[#^qbkfdbg2vhn|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^qbkfdbg2vhn


>%%
>```annotation-json
>{"created":"2025-11-08T12:46:26.561Z","updated":"2025-11-08T12:46:26.561Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":26820,"end":26921},{"type":"TextQuoteSelector","exact":"Here W is the vocabulary size, the wth word is w and {gw}Ww=1 are softmaxweights that will be learned","prefix":"wx(M)n−1)∑Ww=1 exp(g⊤wx(M)n−1) .","suffix":".4.2 Image classificationFor ima"}]}]}
>```
>%%
>*%%PREFIX%%wx(M)n−1)∑Ww=1 exp(g⊤wx(M)n−1) .%%HIGHLIGHT%% ==Here W is the vocabulary size, the wth word is w and {gw}Ww=1 are softmaxweights that will be learned== %%POSTFIX%%.4.2 Image classificationFor ima*
>%%LINK%%[[#^qgiv4vaoj6|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^qgiv4vaoj6


>%%
>```annotation-json
>{"created":"2025-11-08T12:47:30.710Z","updated":"2025-11-08T12:47:30.710Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":27096,"end":27474},{"type":"TextQuoteSelector","exact":"ne wayof computing this distribution would be to apply the standard transformer bodyM times to the tokenised image patches before aggregating the final layer of thetransformer, X(M), across the sequence e.g. by spatial pooling h = ∑Nn=1 x(M)nin order to form a feature representation for the entire image. The representationh could then be used to perform softmax classification","prefix":"uence X(0), that is p(y|X(0)). O","suffix":". An alternative approachis foun"}]}]}
>```
>%%
>*%%PREFIX%%uence X(0), that is p(y|X(0)). O%%HIGHLIGHT%% ==ne wayof computing this distribution would be to apply the standard transformer bodyM times to the tokenised image patches before aggregating the final layer of thetransformer, X(M), across the sequence e.g. by spatial pooling h = ∑Nn=1 x(M)nin order to form a feature representation for the entire image. The representationh could then be used to perform softmax classification== %%POSTFIX%%. An alternative approachis foun*
>%%LINK%%[[#^gwda63o5495|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^gwda63o5495


>%%
>```annotation-json
>{"created":"2025-11-08T12:47:55.433Z","updated":"2025-11-08T12:47:55.433Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":27554,"end":27732},{"type":"TextQuoteSelector","exact":"Instead we introduce anew fixed (learned) token at the start n = 0 of the input sequence x(0)0 . Atthe head we use the n = 0 vector, x(M)0 , to perform the softmax classification","prefix":"ter [Dosovitskiy et al., 2021]. ","suffix":".This approach has the advantage"}]}]}
>```
>%%
>*%%PREFIX%%ter [Dosovitskiy et al., 2021].%%HIGHLIGHT%% ==Instead we introduce anew fixed (learned) token at the start n = 0 of the input sequence x(0)0 . Atthe head we use the n = 0 vector, x(M)0 , to perform the softmax classification== %%POSTFIX%%.This approach has the advantage*
>%%LINK%%[[#^q6ggwyeqh1|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^q6ggwyeqh1


>%%
>```annotation-json
>{"created":"2025-11-08T12:48:04.080Z","updated":"2025-11-08T12:48:04.080Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":27733,"end":27919},{"type":"TextQuoteSelector","exact":"This approach has the advantage that the transformer maintains and refines aglobal representation of the sequence at each layer m of the transformer that isappropriate for classification","prefix":"form the softmax classification.","suffix":".4.3 More complex usesThe transf"}]}]}
>```
>%%
>*%%PREFIX%%form the softmax classification.%%HIGHLIGHT%% ==This approach has the advantage that the transformer maintains and refines aglobal representation of the sequence at each layer m of the transformer that isappropriate for classification== %%POSTFIX%%.4.3 More complex usesThe transf*
>%%LINK%%[[#^mgx1hovbn5j|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^mgx1hovbn5j


>%%
>```annotation-json
>{"created":"2025-11-08T12:51:22.120Z","text":"This is the weight vector associated with the w-th word in the vocabulary. The entire set of these weight vectors are learned parameters of the model.","updated":"2025-11-08T12:51:22.120Z","document":{"title":"an_introduction_to_transformers.pdf","link":[{"href":"urn:x-pdf:23cbc0e9ab71112a4ba8b4259ca86862"},{"href":"vault:/500 - attachments/an_introduction_to_transformers.pdf"}],"documentFingerprint":"23cbc0e9ab71112a4ba8b4259ca86862"},"uri":"vault:/500 - attachments/an_introduction_to_transformers.pdf","target":[{"source":"vault:/500 - attachments/an_introduction_to_transformers.pdf","selector":[{"type":"TextPositionSelector","start":26873,"end":26881},{"type":"TextQuoteSelector","exact":"{gw}Ww=1","prefix":"ary size, the wth word is w and ","suffix":" are softmaxweights that will be"}]}]}
>```
>%%
>*%%PREFIX%%ary size, the wth word is w and%%HIGHLIGHT%% =={gw}Ww=1== %%POSTFIX%%are softmaxweights that will be*
>%%LINK%%[[#^f4wl7kibt0u|show annotation]]
>%%COMMENT%%
>This is the weight vector associated with the w-th word in the vocabulary. The entire set of these weight vectors are learned parameters of the model.
>%%TAGS%%
>
^f4wl7kibt0u
