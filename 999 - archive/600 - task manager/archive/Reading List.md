---
tags:
  - inbox
  - academic
  - work
description: list of all the articles I want to cover
due date:
start date:
end date:
status: Archive
importance level: important
urgency level: urgent
task type: organise
story points: 13
parent nodes:
child nodes:
recurrent: true
---

## Foundational

- [x] [[an_introduction_to_transformers.pdf|An Introduction to Transformers]]
- [ ] [[attention_is_all_you_need.pdf|Attention is All You Need]]
- [ ] [[efficient_estimation_of_word_representations_in_vector_space.pdf|Efficient Estimation of Word Representations in Vector Space]]
- [ ] TBD


## Research

- [ ] [[domain_specialization_as_the_key_to_make_large_language_models_disruptive_a_comprehensive_survey.pdf|Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey]]
	- [x] Overview
- [ ] [[a_survey_on_knowledge_distillation_of_large_language_models.pdf|A Survey on Knowledge Distillation of Large Language Models|]]
	- [x] Overview
- [ ] [[deepseek_r1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning.pdf|DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning]]
	- [x] Overview
- [ ] [[constitutional_ai_harmlessness_from_ai_feedback.pdf|Constitutional AI: Harmlessness from AI Feedback]]
- [ ] [[a_comprehensive_survey_of_llm_alignment_techniques_rlhf_rlaif_ppo_dpo_and_more.pdf|A COMPREHENSIVE SURVEY OF LLM ALIGNMENT TECHNIQUES: RLHF, RLAIF, PPO, DPO AND MORE]]
- [ ] [[llm_post_training_a_deep_dive_into_reasoning_large_language_models.pdf|LLM Post-Training: A Deep Dive into Reasoning Large Language Models]]
	- [ ] [ArXiv](https://arxiv.org/abs/2502.21321)
	- [ ] [GitHub](https://github.com/mbzuai-oryx/Awesome-LLM-Post-training)
- [ ] [[a_comprehensive_survey_of_small_language_models_in_the_era_of_large_language_models_techniques_enhancements_applications_collaboration_with_llms_and_trustworthiness.pdf|A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness]]
- [ ] [[parameter_efficient_fine_tuning_in_large_models_a_survey_of_methodologies.pdf|Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies]]
- [ ] [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) - TODO
- [ ] [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) - TODO
- [ ] [Emergent Abilities of Large Language Models](https://www.semanticscholar.org/paper/Emergent-Abilities-of-Large-Language-Models-Wei-Tay/dac3a172b504f4e33c029655e9befb3386e5f63a) - TODO
- [ ] TBD



## Maybe | Future

- [ ] [A Survey on Progress in LLM Alignment from the Perspective of Reward Design](https://arxiv.org/abs/2505.02666)


## Reference

- [ ] [Constitution or Collapse? Exploring Constitutional AI with Llama 3-8B](https://www.semanticscholar.org/paper/Constitution-or-Collapse-Exploring-Constitutional-Zhang/0360b81490987707ea6251ba388edf6aa109721f)
- [ ] [Ilya Sutskever's Top 30](https://aman.ai/primers/ai/top-30-papers/)