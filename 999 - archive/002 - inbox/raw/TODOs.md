---
tags:
  - inbox
  - work
description:
due date:
start date:
end date:
status: Backlog
importance level: not important
urgency level: urgent
task type: plan
story points: 3
parent nodes:
child nodes:
recurrent:
---
## CREATE TASKS

- Use NotebookLM + ChatGPT + Obsidian
- [Attention](https://paulinamoskwa.github.io/blog/2025-11-06/attn)
- Deep dive into Text Mining.
- Deep dive into Brain-Inspired AI.
- [Domain Specific LLMs](https://www.ibm.com/think/topics/domain-specific-llm?utm_source=chatgpt.com)
- Keep an eye on [[Papers]].
	- [Trending Papers HF](https://huggingface.co/papers/trending)
	- [ORKG](https://orkg.org/) - Papers
	- [ACM Digital Library](https://dl.acm.org/)
- [BitNet](https://github.com/microsoft/BitNet)
- [Survey Swap](https://surveyswap.io/) - Get Surveys
- [Topic Modelling](https://radimrehurek.com/gensim/models/ldamodel.html)
- Benchmark Data
- [Prolific](https://www.prolific.com/participants) - Make Money Annotating
- ==XOR Problem==
- [Hugging Face LLM Course](https://huggingface.co/learn/llm-course/chapter2/1)
	- [Transformers](https://huggingface.co/docs/transformers/main/llm_tutorial)
- [TRL](https://huggingface.co/docs/trl/en/index?utm_source=chatgpt.com)
- Model Context Protocols
- Multi-Agent Frameworks
- Orchestration
- [Unmute](https://github.com/kyutai-labs/unmute)
	- Unmute is a system that allows text LLMs to listen and speak by wrapping them in Kyutai's Text-to-speech and Speech-to-text models. The speech-to-text transcribes what the user says, the LLM generates a response in text, and the text-to-speech reads it out loud. Both the STT and TTS are optimised for low latency, and the system works with any text LLM you like.
- [What it takes to be a great leader?](https://www.ted.com/talks/roselinde_torres_what_it_takes_to_be_a_great_leader?user_email_address=e37d85c32283fbe90ef556cabda1136c&lctg=63496be85b9f5b732e2e8cda)
- [Wan2.2](https://huggingface.co/spaces/Wan-AI/Wan2.2-Animate)
- [ImageGPT](https://imagegpt.com/chat/7461b437-8620-420e-8111-25e0b378b20c)
- [Papers-in-100-Lines-of-Code](https://github.com/MaximeVandegar/Papers-in-100-Lines-of-Code)
- [RWKV](https://www.rwkv.com/)
- [Understanding Multi-Head Latent Attention](https://planetbanatt.net/articles/mla.html)
- [Attention, please!: Demystifying the math behind the attention mechanism and the transformer model](https://paulinamoskwa.github.io/blog/2025-11-06/attn)
- Research Tools
	- [SciSpace](https://scispace.com/)
	- [Research Rabbit](https://www.researchrabbit.ai/)