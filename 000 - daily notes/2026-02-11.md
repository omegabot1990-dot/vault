---
tags:
- meta
description: ''
parent nodes:
- '[[research.base]]'
aliases: null
published on: null
---

- Date: 2026-02-11

## Latest papers

- [ ] [Agent Lightning: Train ANY AI Agents with Reinforcement Learning](https://arxiv.org/abs/2508.03680)
	- We present Agent Lightning, a flexible and extensible framework that enables Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent
- [ ] [TradingAgents: Multi-Agents LLM Financial Trading Framework](https://arxiv.org/abs/2412.20138)
	- Significant progress has been made in automated problem-solving using societies of agents powered by large language models (LLMs)
- [ ] [Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory](https://arxiv.org/abs/2504.19413)
	- Large Language Models (LLMs) have demonstrated remarkable prowess in generating contextually coherent responses, yet their fixed context windows pose fundamental challenges for maintaining consistency over prolonged multi-session dialogues
- [ ] [Scaling Large Language Model-based Multi-Agent Collaboration](https://arxiv.org/abs/2406.07155)
	- Recent breakthroughs in large language model-driven autonomous agents have revealed that multi-agent collaboration often surpasses each individual through collective reasoning
- [ ] [Weak-Driven Learning: How Weak Agents make Strong Agents Stronger](https://arxiv.org/abs/2602.08222)
	- As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns
- [ ] [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)
	- High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time

## TLDR top papers

- [ ] 1) Agent Lightning: Train ANY AI Agents with Reinforcement Learning — https://arxiv.org/abs/2508.03680
- [ ] summary: We present Agent Lightning, a flexible and extensible framework that enables Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent
- [ ] 2) TradingAgents: Multi-Agents LLM Financial Trading Framework — https://arxiv.org/abs/2412.20138
- [ ] summary: Significant progress has been made in automated problem-solving using societies of agents powered by large language models (LLMs)
- [ ] 3) Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory — https://arxiv.org/abs/2504.19413
- [ ] summary: Large Language Models (LLMs) have demonstrated remarkable prowess in generating contextually coherent responses, yet their fixed context windows pose fundamental challenges for maintaining consistency over prolonged multi-session dialogues
- [ ] 4) Scaling Large Language Model-based Multi-Agent Collaboration — https://arxiv.org/abs/2406.07155
- [ ] summary: Recent breakthroughs in large language model-driven autonomous agents have revealed that multi-agent collaboration often surpasses each individual through collective reasoning
- [ ] 5) Weak-Driven Learning: How Weak Agents make Strong Agents Stronger — https://arxiv.org/abs/2602.08222
- [ ] summary: As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns
- [ ] 6) Efficient Memory Management for Large Language Model Serving with PagedAttention — https://arxiv.org/abs/2309.06180
- [ ] summary: High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time
