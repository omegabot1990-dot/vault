---
tags:
- meta
description: ''
parent nodes:
- '[[research.base]]'
aliases: null
published on: null
---

- Date: 2026-02-28

## Latest papers

- [ ] [OmniGAIA: Towards Native Omni-Modal AI Agents](https://arxiv.org/abs/2602.22897)
	- Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world
- [ ] [TradingAgents: Multi-Agents LLM Financial Trading Framework](https://arxiv.org/abs/2412.20138)
	- Significant progress has been made in automated problem-solving using societies of agents powered by large language models (LLMs)
- [ ] [MobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios](https://arxiv.org/abs/2602.22638)
	- Route-planning agents powered by large language models (LLMs) have emerged as a promising paradigm for supporting everyday human mobility through natural language interaction and tool-mediated decision making
- [ ] [Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory](https://arxiv.org/abs/2504.19413)
	- Large Language Models (LLMs) have demonstrated remarkable prowess in generating contextually coherent responses, yet their fixed context windows pose fundamental challenges for maintaining consistency over prolonged multi-session dialogues
- [ ] [OpenHands: An Open Platform for AI Software Developers as Generalist Agents](https://arxiv.org/abs/2407.16741)
	- Software is one of the most powerful tools that we humans have at our disposal; it allows a skilled programmer to interact with the world in complex and profound ways
- [ ] [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)
	- High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time

## TLDR top papers

- [ ] 1) OmniGAIA: Towards Native Omni-Modal AI Agents — https://arxiv.org/abs/2602.22897
- [ ] summary: Human intelligence naturally intertwines omni-modal perception -- spanning vision, audio, and language -- with complex reasoning and tool usage to interact with the world
- [ ] 2) TradingAgents: Multi-Agents LLM Financial Trading Framework — https://arxiv.org/abs/2412.20138
- [ ] summary: Significant progress has been made in automated problem-solving using societies of agents powered by large language models (LLMs)
- [ ] 3) MobilityBench: A Benchmark for Evaluating Route-Planning Agents in Real-World Mobility Scenarios — https://arxiv.org/abs/2602.22638
- [ ] summary: Route-planning agents powered by large language models (LLMs) have emerged as a promising paradigm for supporting everyday human mobility through natural language interaction and tool-mediated decision making
- [ ] 4) Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory — https://arxiv.org/abs/2504.19413
- [ ] summary: Large Language Models (LLMs) have demonstrated remarkable prowess in generating contextually coherent responses, yet their fixed context windows pose fundamental challenges for maintaining consistency over prolonged multi-session dialogues
- [ ] 5) OpenHands: An Open Platform for AI Software Developers as Generalist Agents — https://arxiv.org/abs/2407.16741
- [ ] summary: Software is one of the most powerful tools that we humans have at our disposal; it allows a skilled programmer to interact with the world in complex and profound ways
- [ ] 6) Efficient Memory Management for Large Language Model Serving with PagedAttention — https://arxiv.org/abs/2309.06180
- [ ] summary: High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time
